{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FullyConnectedNets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musicjae/cs231n/blob/master/FullyConnectedNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOCpRM1s5ct4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ab476b7d-0ed1-4020-abed-e302db1319f8"
      },
      "source": [
        "# this mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
        "FOLDERNAME = 'Colab Notebooks/cs231n/assignments/assignment2'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# this downloads the CIFAR-10 dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd /content"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/cs231n/assignments/assignment2/cs231n/datasets\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "1JVcymla5ct8",
        "colab_type": "text"
      },
      "source": [
        "# Fully-Connected Neural Nets\n",
        "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "X4b8DJl35ct8",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
        "\n",
        "```python \n",
        "def layer_forward(x, w):\n",
        "  \"\"\" Receive inputs x and weights w \"\"\"\n",
        "  # Do some computations ...\n",
        "  z = # ... some intermediate value\n",
        "  # Do some more computations ...\n",
        "  out = # the output\n",
        "   \n",
        "  cache = (x, w, z, out) # Values we need to compute gradients\n",
        "   \n",
        "  return out, cache\n",
        "```\n",
        "\n",
        "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
        "\n",
        "```python\n",
        "def layer_backward(dout, cache):\n",
        "  \"\"\"\n",
        "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
        "  and compute derivative with respect to inputs.\n",
        "  \"\"\"\n",
        "  # Unpack cache values\n",
        "  x, w, z, out = cache\n",
        "  \n",
        "  # Use values in cache to compute derivatives\n",
        "  dx = # Derivative of loss with respect to x\n",
        "  dw = # Derivative of loss with respect to w\n",
        "  \n",
        "  return dx, dw\n",
        "```\n",
        "\n",
        "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
        "\n",
        "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch/Layer Normalization as a tool to more efficiently optimize deep networks.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "t8LqWV1A5ct9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "851579d9-32f1-4b83-8fa6-2087c04dff04"
      },
      "source": [
        "# As usual, a bit of setup\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cs231n.classifiers.fc_net import *\n",
        "from cs231n.data_utils import get_CIFAR10_data\n",
        "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from cs231n.solver import Solver\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "  \"\"\" returns relative error \"\"\"\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== You can safely ignore the message below if you are NOT working on ConvolutionalNetworks.ipynb ===========\n",
            "\tYou will need to compile a Cython extension for a portion of this assignment.\n",
            "\tThe instructions to do this will be given in a section of the notebook below.\n",
            "\tThere will be an option for Colab users and another for Jupyter (local) users.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "ymq9smS05ct_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6fd10f39-0000-4d01-b924-f946ee59fe3f"
      },
      "source": [
        "# Load the (preprocessed) CIFAR10 data.\n",
        "\n",
        "data = get_CIFAR10_data()\n",
        "for k, v in list(data.items()):\n",
        "  print(('%s: ' % k, v.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('X_train: ', (49000, 3, 32, 32))\n",
            "('y_train: ', (49000,))\n",
            "('X_val: ', (1000, 3, 32, 32))\n",
            "('y_val: ', (1000,))\n",
            "('X_test: ', (1000, 3, 32, 32))\n",
            "('y_test: ', (1000,))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETdBDVAj5cuC",
        "colab_type": "text"
      },
      "source": [
        "# Affine layer: forward\n",
        "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
        "\n",
        "Once you are done you can test your implementaion by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c95qYEpz5cuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a63b4fd3-2b2c-4306-c104-0e6843a4fc04"
      },
      "source": [
        "from builtins import range\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for an affine (fully-connected) layer.\n",
        "\n",
        "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "    then transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
        "    - w: A numpy array of weights, of shape (D, M)\n",
        "    - b: A numpy array of biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
        "    # will need to reshape the input into rows.                               \n",
        " ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    N = x.shape[0]\n",
        "    #x = x.reshape(N, -1) \n",
        "    #위와 같이 x를 따로 reshape을 해주면, backward 함수에서 dx의 reshape이 제대로 되지 않는다.\n",
        "    out = x.reshape(N, -1).dot(w)+b\n",
        "    \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            \n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    \n",
        "    return out, cache\n",
        "\n",
        "# Test the affine_forward function\n",
        "\n",
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape) #flattening process 1\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape) # (2,4,5,6)\n",
        "print('x의 shape: ', x.shape) # x(2,4,5,6)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "print('w의 shape: ',w.shape) # w(120, 3)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim) # flattening process 2\n",
        "print('b의 shape: ',b.shape) # b(3, )\n",
        "out, _ = affine_forward(x, w, b) \n",
        "print('out의 shape: ', affine_forward(x,w,b)[0].shape) # (2,3)\n",
        "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
        "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
        "\n",
        "# Compare your output with ours. The error should be around e-9 or less.\n",
        "print('Testing affine_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x의 shape:  (2, 4, 5, 6)\n",
            "w의 shape:  (120, 3)\n",
            "b의 shape:  (3,)\n",
            "out의 shape:  (2, 3)\n",
            "Testing affine_forward function:\n",
            "difference:  9.769849468192957e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKvd_R165cuF",
        "colab_type": "text"
      },
      "source": [
        "# Affine layer: backward\n",
        "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoYTvl005cuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a6410547-9736-4c5d-a1b6-a7cfd8ab81cc"
      },
      "source": [
        "def affine_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an affine layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, d_1, ... d_k)\n",
        "      - w: Weights, of shape (D, M)\n",
        "      - b: Biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
        "    - dw: Gradient with respect to w, of shape (D, M)\n",
        "    - db: Gradient with respect to b, of shape (M,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    dx = np.dot(dout, w.T) #size 맞추기 (10,5) x (5, 6) = (10, 6)\n",
        "    dx = dx.reshape(x.shape) # (10, 6) -> (10, 2, 3)\n",
        "    \n",
        "    x = x.reshape(len(x),-1) #flattening 해주기 (10, 6)\n",
        "    \n",
        "    dw = np.dot(x.T, dout) # (6, 10) x (10, 5) = (6, 5)\n",
        "    \n",
        "    db = np.sum(dout, axis=0) #(5, ) // (6,5)인 dout의 각각의 열을 모두 더해줌\n",
        "    \n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "# Test the affine_backward function\n",
        "np.random.seed(231)\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "_, cache = affine_forward(x, w, b)\n",
        "dx, dw, db = affine_backward(dout, cache)\n",
        "print('dx의 shape: ',dx.shape)\n",
        "print('dw의 shape: ',dw.shape)\n",
        "print('db의 shape: ',db.shape) \n",
        "# The error should be around e-10 or less\n",
        "print('Testing affine_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dx의 shape:  (10, 2, 3)\n",
            "dw의 shape:  (6, 5)\n",
            "db의 shape:  (5,)\n",
            "Testing affine_backward function:\n",
            "dx error:  5.399100368651805e-11\n",
            "dw error:  9.904211865398145e-11\n",
            "db error:  2.4122867568119087e-11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-1O_fWqTZyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj5Y9iZ65cuI",
        "colab_type": "text"
      },
      "source": [
        "# ReLU activation: forward\n",
        "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmB-YyIz5cuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "c7edfc66-0530-4dd0-e7fa-f8d100ef8a14"
      },
      "source": [
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    out = np.maximum(0,x)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "# Test the relu_forward function\n",
        "\n",
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "# Compare your output with ours. The error should be on the order of e-8\n",
        "print('Testing relu_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing relu_forward function:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4adcce53f1bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Compare your output with ours. The error should be on the order of e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing relu_forward function:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'difference: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rel_error' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ZLkqLb5cuL",
        "colab_type": "text"
      },
      "source": [
        "# ReLU activation: backward\n",
        "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1FkwHQa5cuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "26abd288-77f3-4945-adfd-e8d9661c5bce"
      },
      "source": [
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = None, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    dx = dout * (x>0)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "np.random.seed(231)\n",
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "print('dout의 shape: ',dout.shape)\n",
        "print('dx의 shape: ',dx.shape)\n",
        "print('dx_num의 shape: ',dx_num.shape)\n",
        "\n",
        "# The error should be on the order of e-12\n",
        "print('Testing relu_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3d0ada071b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdx_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_numerical_gradient_array' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "lNfhNGwR5cuO",
        "colab_type": "text"
      },
      "source": [
        "## Inline Question 1: \n",
        "\n",
        "We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?\n",
        "1. Sigmoid\n",
        "2. ReLU\n",
        "3. Leaky ReLU\n",
        "\n",
        "## Answer:\n",
        "답 (1). 시그모이드 함수를 사용하면, $|x|$가 커질수록, 그것의 미분 값이 0에 수렴하기 때문에, vanishing gradient 현상이 나타난다. 나머지 ReLU 유형의 함수들은 이런 문제를 해결해준다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBecHn255cuO",
        "colab_type": "text"
      },
      "source": [
        "# \"Sandwich\" layers\n",
        "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
        "\n",
        "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvdsoNxM5cuP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "27f74e75-e4df-47a4-9509-f0ee883caec0"
      },
      "source": [
        "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
        "np.random.seed(231)\n",
        "x = np.random.randn(2, 3, 4)\n",
        "w = np.random.randn(12, 10)\n",
        "b = np.random.randn(10)\n",
        "dout = np.random.randn(2, 10)\n",
        "\n",
        "out, cache = affine_relu_forward(x, w, b)\n",
        "dx, dw, db = affine_relu_backward(dout, cache)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "print('Testing affine_relu_forward:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5f01ab7cb62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maffine_relu_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine_relu_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m231\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs231n'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgYDQZ7u5cuR",
        "colab_type": "text"
      },
      "source": [
        "# Loss layers: Softmax and SVM\n",
        "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
        "\n",
        "You can make sure that the implementations are correct by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csK7RZC95cuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "145fb69b-5039-48d7-952b-27bf94ebc6ff"
      },
      "source": [
        "np.random.seed(231)\n",
        "num_classes, num_inputs = 10, 50\n",
        "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
        "y = np.random.randint(num_classes, size=num_inputs)\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = svm_loss(x, y)\n",
        "\n",
        "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
        "print('Testing svm_loss:')\n",
        "print('loss: ', loss)\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "a = rel_error(dx_num, dx)\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = softmax_loss(x, y)\n",
        "\n",
        "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
        "print('\\nTesting softmax_loss:')\n",
        "print('loss: ', loss)\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "b = rel_error(dx_num, dx)\n",
        "\n",
        "print('>>\\n For loss, svm > softmax, but for dx_error, svm < softmax')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7b14a43affe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdx_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msvm_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_numerical_gradient' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM9OSJ73ioDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "580c93e9-5933-4d28-d135-4f1a16b4615e"
      },
      "source": [
        "a = np.array([1,2,3,4,5,6])\n",
        "a = a.reshape(2,3)\n",
        "\n",
        "b=np.array([1,1,1])\n",
        "b = b.reshape(3, )\n",
        "\n",
        "print('a 행렬: ','\\n','\\n', a,'\\n','\\n','bias: ','\\n','\\n', b,'\\n','\\n','브로드 캐스팅 결과: ','\\n','\\n',a+b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a 행렬:  \n",
            " \n",
            " [[1 2 3]\n",
            " [4 5 6]] \n",
            " \n",
            " bias:  \n",
            " \n",
            " [1 1 1] \n",
            " \n",
            " 브로드 캐스팅 결과:  \n",
            " \n",
            " [[2 3 4]\n",
            " [5 6 7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc3fJG2jdqrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "4bed854e-ee76-4431-d362-3f949549ae9c"
      },
      "source": [
        "from builtins import range\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from cs231n.layers import *\n",
        "from cs231n.fast_layers import *\n",
        "\n",
        "class test(object):\n",
        "\n",
        "    def __init__(self, affine_relu_forward):\n",
        "        self.affine_relu_forward = affine_relu_forward\n",
        "    \n",
        "\n",
        "    def affine_forward(x, w, b):\n",
        "        out = None\n",
        "        ###########################################################################\n",
        "        N = x.shape[0]\n",
        "        out = x.reshape(N, -1).dot(w)+b\n",
        "        ###########################################################################\n",
        "        cache = (x, w, b)\n",
        "\n",
        "        return out, cache\n",
        "\n",
        "    def affine_backward(dout, cache):\n",
        "   \n",
        "        x, w, b = cache\n",
        "        dx, dw, db = None, None, None\n",
        "        ###########################################################################\n",
        "   \n",
        "        dx = np.dot(dout, w.T) #size 맞추기 (10,5) x (5, 6) = (10, 6)\n",
        "        dx = dx.reshape(x.shape) # (10, 6) -> (10, 2, 3)\n",
        "    \n",
        "        x = x.reshape(len(x),-1) #flattening 해주기 (10, 6)\n",
        "    \n",
        "        dw = np.dot(x.T, dout) # (6, 10) x (10, 5) = (6, 5)\n",
        "    \n",
        "        db = np.sum(dout, axis=0) #(5, ) // (6,5)인 dout의 각각의 열을 모두 더해줌\n",
        "     \n",
        "        ###########################################################################\n",
        "        return dx, dw, db\n",
        "\n",
        "\n",
        "\n",
        "    def affine_relu_forward(x, w, b):\n",
        "   \n",
        "        a, fc_cache = affine_forward(x, w, b)\n",
        "        out, relu_cache = relu_forward(a)\n",
        "        cache = (fc_cache, relu_cache)\n",
        "\n",
        "        return out, cache\n",
        "\n",
        "    def affine_relu_backward(dout, cache):\n",
        "    \n",
        "        fc_cache, relu_cache = cache\n",
        "        da = relu_backward(dout, relu_cache)\n",
        "        dx, dw, db = affine_backward(da, fc_cache)\n",
        "    \n",
        "        return dx, dw, db\n",
        "\n",
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape) #flattening process 1\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape) # (2,4,5,6)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim) # flattening process 2\n",
        "\n",
        "print('x의 shape: ', x.shape) # x(2,4,5,6)\n",
        "print('w의 shape: ',w.shape) # w(120, 3)\n",
        "print('b의 shape: ',b.shape) # b(3, )\n",
        "\n",
        "model = test(affine_relu_forward)\n",
        "\n",
        "aa = model.affine_relu_forward(x,w,b)\n",
        "#a = model.affine_relu_backward(aa[0],aa[1])\n",
        "\n",
        "list_aa = list(aa)\n",
        "\n",
        "print(list_aa[0].shape)\n",
        "print(list_aa[1][1])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2d6e7cf090f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs231n'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpVnRelWgw3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "e10bb7e5-53db-4852-aa71-56e927e527f5"
      },
      "source": [
        "class TwoLayerNet(object):\n",
        "   \n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=3 * 32 * 32,\n",
        "        hidden_dim=100,\n",
        "        num_classes=10,\n",
        "        weight_scale=1e-3,\n",
        "        reg=0.0,\n",
        "    ):\n",
        "        self.params = {}\n",
        "        self.reg = reg\n",
        "\n",
        "        self.params['W1'] = weight_scale*np.random.randn(input_dim,hidden_dim)\n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "\n",
        "        self.params['W2'] = weight_scale*np.random.randn(hidden_dim, num_classes)\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        \n",
        "        scores = None\n",
        "        # X = 3, 5\n",
        "        W1 = self.params['W1'] # 5, 50\n",
        "        W2 = self.params['W2'] # 50, 7\n",
        "        b1 = self.params['b1'] # 50,\n",
        "        b2 = self.params['b2'] # 7,\n",
        "     \n",
        "############################# start ##########################################\n",
        "       \n",
        "        h, cache1 = affine_relu_forward(X, W1, b1)\n",
        "        scores, cache2 = affine_forward(h, W2, b2)  \n",
        "\n",
        "############################## end #############################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores # 3,7\n",
        "\n",
        "            loss, grads = 0, {}\n",
        "        ########################################################################\n",
        "        # (1) grad 딕셔너리에 그래디언트를, 손실 변수에는 손실을 저장하라. \n",
        "        # (2) 소프트맥스를 사용하여 데이터 손실을 계산하라\n",
        "        # (3) grad[k]가 self.param[k]의 grad에 성립한다는 것을 확인하라.\n",
        "        # (4) 정규화 L2 하라\n",
        "        # (5) 0.5 인자를 포함하여 그래디언트 표현을 단순화 하라.                                                            \n",
        "        ########################################################################\n",
        "\n",
        "        loss, dout = softmax_loss(scores, y)\n",
        "        loss += 0.5 * self.reg * (np.sum(W1**2)+(np.sum(W2**2)))\n",
        "        dout = softmax_loss(scores, y)[1]\n",
        "        dh, dW2, db2 = affine_relu_backward(dout, cache2)\n",
        "        dx, dW1, db1 = affine_relu_backward(dh, cache1)\n",
        "        \n",
        "        grads['W2'] = dW2 + self.reg * W2\n",
        "        grads['b2'] = db2\n",
        "\n",
        "        grads['W1'] = dW + self.reg * W1\n",
        "        grads['b1'] = db1\n",
        "\n",
        "        return loss, grads\n",
        "        ########################################################################\n",
        "      \n",
        "    N, D, H, C = 3, 5, 50, 7\n",
        "    X = np.random.randn(N, D)\n",
        "    dout= None\n",
        "    cache2 = []\n",
        "    model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
        "\n",
        "    print('W1: ',model.params['W1'].shape)\n",
        "    print('W2: ',model.params['W2'].shape)\n",
        "    print('b1: ',model.params['b1'].shape)\n",
        "    print('b2: ',model.params['b2'].shape)\n",
        "    print('X * W1 = ', np.dot(X, model.params['W1']).shape)\n",
        "    print('h: ',affine_relu_forward(X, model.params['W1'], b1)[0].shape)\n",
        "    print('scores = ', scores.shape)\n",
        "    print('손실: ', softmax_loss(scores, y)[0])\n",
        "    print('dout: ' , softmax_loss(scores, y)[1].shape)\n",
        "    print('dW2: ', affine_relu_backward(dout, cache2))\n",
        "    print('grad dW2: ', grad['W2'])\n",
        "    \n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:  (5, 50)\n",
            "W2:  (50, 7)\n",
            "b1:  (50,)\n",
            "b2:  (7,)\n",
            "X * W1 =  (3, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f535d946689d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     def __init__(\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f535d946689d>\u001b[0m in \u001b[0;36mTwoLayerNet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b2: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X * W1 = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maffine_relu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scores = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'손실: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'affine_relu_forward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqPWsFJ8orod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKT02Nwd5cuZ",
        "colab_type": "text"
      },
      "source": [
        "# Two-layer network\n",
        "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
        "\n",
        "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRtVE7Oc5cua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "d0c79919-adaa-430b-f847-c7ffac81dfe2"
      },
      "source": [
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        " \n",
        "      ReLU 비선형 함수와 모듈적 층 디자인을 사용하는 소프트맥스 손실을 가진 복층 전연결 신경망. 우리는\n",
        "    입력값 차원을 D, 은닉 차원을 H 개라고 하고, C 개의 클래스로 분류를 수행한다.\n",
        "\n",
        "    아키텍처는 affine -- 렐루 -- affine -- 소프트맥스로 구성된다.\n",
        "\n",
        "    이 클래스가 경사 하강법을 수행하지 않는 대신에 <최적화를 수행하는 별개의 solver 대상>과 상호작용\n",
        "    한다는 것에 주목하자.\n",
        "\n",
        "    이 모델의 학습 가능한 매개변수는 <모수 이름>에서 <배열>로 사상하는 self.params 딕셔너리에 저장된다\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=3 * 32 * 32,\n",
        "        hidden_dim=100,\n",
        "        num_classes=10,\n",
        "        weight_scale=1e-3,\n",
        "        reg=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        - reg: Scalar giving L2 regularization strength.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.reg = reg\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with first layer weights                         #\n",
        "        # and biases using the keys 'W1' and 'b1' and second layer                 #\n",
        "        # weights and biases using the keys 'W2' and 'b2'.                         #\n",
        "        ############################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        self.params['W1'] = weight_scale*np.random.randn(input_dim,hidden_dim)\n",
        "        self.params['W2'] = weight_scale*np.random.randn(hidden_dim, num_classes)\n",
        "        self.params['b1'] = np.zeros(hidden_dim,) \n",
        "        self.params['b2'] = np.zeros(num_classes,)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "    def loss(self, X, y=None):\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        layer1_output, layer1_cache = affine_relu_forward(X, self.params[\"W1\"], self.params[\"b1\"])\n",
        "        scores, layer2_cache = affine_forward(layer1_output, self.params[\"W2\"], self.params[\"b2\"])\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k]. Don't forget to add L2 regularization!                   #\n",
        "        #                                                                          #\n",
        "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
        "        # automated tests, make sure that your L2 regularization includes a factor #\n",
        "        # of 0.5 to simplify the expression for the gradient.                      #\n",
        "        ############################################################################\n",
        "        loss, dscores = softmax_loss(scores, y)\n",
        "        dlayer1, grads[\"W2\"], grads[\"b2\"] = affine_backward(dscores, layer2_cache)\n",
        "        dx, grads[\"W1\"], grads[\"b1\"] = affine_relu_backward(dlayer1, layer1_cache)\n",
        "\n",
        "        loss += 0.5 * self.reg * np.sum(self.params[\"W1\"] * self.params[\"W1\"]) + 0.5 * self.reg * np.sum(self.params[\"W2\"] *  self.params[\"W2\"])\n",
        "        grads[\"W1\"] += self.reg * self.params[\"W1\"]\n",
        "        grads[\"W2\"] += self.reg * self.params[\"W2\"]\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return loss, grads\n",
        "\n",
        "      ###########################  main  #########################################\n",
        "\n",
        "\n",
        "np.random.seed(231)\n",
        "N, D, H, C = 3, 5, 50, 7\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=N)\n",
        "\n",
        "std = 1e-3\n",
        "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
        "\n",
        "print('Testing initialization ... ')\n",
        "W1_std = abs(model.params['W1'].std() - std)\n",
        "b1 = model.params['b1']\n",
        "W2_std = abs(model.params['W2'].std() - std)\n",
        "b2 = model.params['b2']\n",
        "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
        "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
        "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
        "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
        "\n",
        "print('Testing test-time forward pass ... ')\n",
        "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
        "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
        "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
        "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
        "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
        "scores = model.loss(X)\n",
        "correct_scores = np.asarray(\n",
        "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
        "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
        "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
        "scores_diff = np.abs(scores - correct_scores).sum()\n",
        "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
        "\n",
        "print('Testing training loss (no regularization)')\n",
        "y = np.asarray([0, 5, 1])\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 3.4702243556\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
        "\n",
        "model.reg = 1.0\n",
        "loss, grads = model.loss(X, y)\n",
        "correct_loss = 26.5948426952\n",
        "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
        "\n",
        "for reg in [0.0, 0.7]:\n",
        "  print('Running numeric gradient check with reg = ', reg)\n",
        "  model.reg = reg\n",
        "  loss, grads = model.loss(X, y)\n",
        "\n",
        "  for name in sorted(grads):\n",
        "    f = lambda _: model.loss(X, y)[0]\n",
        "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
        "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing initialization ... \n",
            "Testing test-time forward pass ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1125e0f92b00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m correct_scores = np.asarray(\n\u001b[1;32m    127\u001b[0m   [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
            "\u001b[0;32m<ipython-input-4-1125e0f92b00>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# class scores for X and storing them in the scores variable.              #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mlayer1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer1_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_relu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer2_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'affine_relu_forward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoF2I5nj5cuc",
        "colab_type": "text"
      },
      "source": [
        "# Solver\n",
        "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
        "\n",
        "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tln_solver_accuracy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "4c65f36b-a9ec-42f0-c557-f045f18a68d7"
      },
      "source": [
        "model = TwoLayerNet()\n",
        "solver = None\n",
        "\n",
        "##############################################################################\n",
        "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
        "# 50% accuracy on the validation set.                                        #\n",
        "##############################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "solver = Solver(model, data,\n",
        "                update_rule='sgd',\n",
        "                optim_config={'learning_rate': 1e-3},\n",
        "                lr_decay=0.95,\n",
        "                num_epochs=10, batch_size=100,\n",
        "                print_every=100)\n",
        "solver.train()\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "##############################################################################\n",
        "#                             END OF YOUR CODE                               #\n",
        "##############################################################################"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3e3aafe39fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TwoLayerNet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUG9KALh5cue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "50dffdc3-9784-430d-d5fa-3a733df3df84"
      },
      "source": [
        "# Run this cell to visualize training loss and train / val accuracy\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(solver.train_acc_history, '-o', label='train')\n",
        "plt.plot(solver.val_acc_history, '-o', label='val')\n",
        "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='lower right')\n",
        "plt.gcf().set_size_inches(15, 12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAALJCAYAAADF1ND/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9f5Acx3Xn+c3uKQA9oIwGLMgm2wAB0Q7QoiFgTEiEjYu1SUcQa0Okx6REhExqbx3r0/nOsWHAvDkPHVyR4NJH7OFk8i58Xp92vbvnEM0d/to5krAXPAdwZxs2KAOcGWEhEWtTJEA1uCdYQEMUpomp6c77oycb1dWZWVnVVf1j+vuJUIiYrq7Krh9Z7+V77/uElBKEEEIIIYQQQvqHXK8HQAghhBBCCCGkFTpqhBBCCCGEENJn0FEjhBBCCCGEkD6DjhohhBBCCCGE9Bl01AghhBBCCCGkz6CjRgghhBBCCCF9Bh01QgghA4UQ4k+FEP912tvGHMPPCiG+nfZ+CSGEEMVIrwdACCFk+SOE+H7gn6MArgGoLf37v5VSPuu6Lynlz2exLSGEENJP0FEjhBCSOVLKG9R/CyHeBfCrUso/C28nhBiRUi52c2yEEEJIP8LUR0IIIT1DpRAKIX5LCPFfAPxbIcRaIcRrQoiLQojLS//9I4Hv/D9CiF9d+u9/LIT4SyHE/7K07TtCiJ9PuO1mIcSfCyE+EEL8mRDifxdCfNXxd/z40rEqQogzQoh7A5/9ghDiG0v7LQsh/oelv3906bdVhBCXhBB/IYTge5kQQggAOmqEEEJ6zw8DWAfgZgBfROPd9G+X/r0RQBXA71m+fweAswA+CuB/BvCHQgiRYNs/BvA1AD8I4HEAX3AZvBDCA/AqgNcBfAzAPwXwrBBiy9Imf4hGeudHAPwEgKNLf38YwLcBrAfwQwB+G4B0OSYhhJDlDx01QgghvaYO4DEp5TUpZVVK+V0p5UtSynkp5QcAfgfAz1i+f05K+a+klDUA/yeAG9FwfJy3FUJsBPApAF+SUi5IKf8SwCuO498J4AYAB5e+exTAawA+v/S5D+ATQogfkFJellK+Gfj7jQBullL6Usq/kFLSUSOEEAKAjhohhJDec1FK+aH6hxBiVAjxfwghzgkhvgfgzwEUhRB5w/f/i/oPKeX80n/eEHPbmwBcCvwNAN5zHP9NAN6TUtYDfzsHoLT03/cD+AUA54QQ/68Q4qeW/n4IwN8BeF0I8S0hxKTj8QghhAwBdNQIIYT0mnAU6WEAWwDcIaX8AQD/YOnvpnTGNHgfwDohxGjgbxscv3sBwIZQfdlGAGUAkFL+jZTyF9FIi5wG8PzS3z+QUj4spfw4gHsB/KYQ4uc6/B2EEEKWCXTUCCGE9BsfQaMurSKEWAfgsawPKKU8B+AkgMeFECuWol73OH79DQDzAP5HIYQnhPjZpe/++6V9PSiEWCOl9AF8D41UTwghPiOE+NGlGrkraLQrqOsPQQghZNigo0YIIaTfeAZAAcDfAzgB4D926bgPAvgpAN8F8CSAKTT6vVmRUi6g4Zj9PBpj/n0A/0hK+dbSJl8A8O5SGuevLR0HAH4MwJ8B+D6Avwbw+1LKY6n9GkIIIQONYN0yIYQQ0o4QYgrAW1LKzCN6hBBCSBhG1AghhBAAQohPCSFuEULkhBD/EMAvolFTRgghhHSdkV4PgBBCCOkTfhjAy2j0Ufs2gP9OSjnT2yERQggZVpj6SAghhBBCCCF9BlMfCSGEEEIIIaTP6Fnq40c/+lG5adOmXh2eEEIIIYQQQnrKqVOn/l5KuV73Wc8ctU2bNuHkyZO9OjwhhBBCCCGE9BQhxDnTZ0x9JIQQQgghhJA+g44aIYQQQgghhPQZdNQIIYQQQgghpM+go0YIIYQQQgghfQYdNUIIIYQQQgjpM+ioEUIIIYQQQkifQUeNEEIIIYQQQvoMOmqEEEIIIYQQ0mfQUSOEEEIIIYSQPmOk1wPoF6Znyjh05CwuVKq4qVjAxO4tGB8r9XpYhBBCCCGEkCEkMqImhNgghDgmhPiGEOKMEOI3LNt+SgixKIT4bLrDzJbpmTImXphDuVKFBFCuVLFvahaPTp/u9dAIIYQQQgghQ4hL6uMigIellJ8AsBPArwshPhHeSAiRB/AvALye7hCz5/FXzsCvy7a/f/XEeUzPlHswIkIIIYQQQsgwE+moSSnfl1K+ufTfHwD4JgBdTuA/BfASgO+kOsIuUKn6xs8OHTnbxZEQQgghhBBCSEwxESHEJgBjAN4I/b0E4JcA/MuI739RCHFSCHHy4sWL8UbaI8qVaq+HQAghhBBCCBkynB01IcQNaETM9kkpvxf6+BkAvyWlrNv2IaX8ipRyh5Ryx/r16+OPNiPWjnq9HgIhhBBCCCGENHFy1IQQHhpO2rNSypc1m+wA8O+FEO8C+CyA3xdCjKc2yox57J7bej0EQgghhBBCCGniovooAPwhgG9KKX9Xt42UcrOUcpOUchOAFwH891LK6VRHmiGU4SeEEEIIIYT0Ey591HYB+AKA00KI2aW//TaAjQAgpfyDjMZGCCGEEEIIIUNJpKMmpfxLAMJ1h1LKf9zJgHrFqJfDvK8vsZueKTPqRgghhBBCCOkasVQflzP33f4jxs8o0U8IIYQQQgjpJnTUljj2lrldACX6CSGEEEIIId2EjtoSFyzOWF44Z34SQgghhBBCSMfQUVvipmLB+FlNyi6OhBBCCCGEEDLs0FFbYmL3FqNiSsnixBFCCCGEEEJI2tBRW2J8rIQf/dhq7WebfpCOGiGEEEIIIaR70FEL8K2L89q/n/jW5S6PhBBCCCGEEDLM0FELYKpFY40aIYQQQgghpJvQUSOEEEIIIYSQPoOOmiPTM+VeD4EQQgghhBAyJNBRC2Drl3boyNkujoQQQgghhBAyzNBRC2CrRbM1xCaEEEIIIYSQNKGjFsDWL83WEJsQQgghhBBC0oSOWoCJ3Vvg5drTH728wMTuLT0YESGEEEIIIWQYGen1APqJ8bESAODxV86gUvUBAGtHPTx2z23NzwghhBBCCCEka+iohRgfK9EpI4QQQgghhPQUpj4SQgghhBBCSJ9BR40QQgghhBBC+gymPmqYninj0JGzuFCp4qZiARO7tzAdkhBCCCGEENI1hLT0DsuSHTt2yJMnT/bk2DamZ8p45OXTqPq15t8EAImGfD+dNkIIIYQQQkgaCCFOSSl36D5j6mOIQ0fOtjhpQMNJA4BypYpHXj6N6Zly9wdGCCGEEEIIGRroqIW4UKlaP6/6NRw6crZLoyGEEEIIIYQMI3TUQqwpeJHbRDlzhBBCCCGEENIJdNRCCBG9zU3FQvYDIYQQQgghhAwtdNRCXJ73rZ8XvDwmdm/p0mgIIYQQQgghwwgdtRD5iJDaU/dtpeojIYQQQgghJFPoqIWoWdoVlIoFOmmEEEIIIYSQzKGjFqJkqT+789b1XRwJIYQQQgghZFihoxbCVn/28qlvd3EkhBBCCCGEkGGFjloM5v06m10TQgghhBBCMoeOWoioZtZsdk0IIYQQQgjJGjpqIaKaWbPZNSGEEEIIISRr6KiFiGpmzWbXhBBCCCGEkKyJdNSEEBuEEMeEEN8QQpwRQvyGZpsHhRBfF0KcFkL8lRBiWzbDzZ6J3VtQ8PLaz9jsmhBCCCGEENINXCJqiwAellJ+AsBOAL8uhPhEaJt3APyMlHIrgH8O4CvpDrN7jI+VcP/tJYTbXgsA999eYh81QgghhBBCSOZEOmpSyvellG8u/fcHAL4JoBTa5q+klJeX/nkCwI+kPdBucvjr7yPc9loCeO6N96j6SAghhBBCCMmcWDVqQohNAMYAvGHZ7J8A+NPkQ+ot0zNlXJ73tZ/VpMQjL5+ms0YIIYQQQgjJFGdHTQhxA4CXAOyTUn7PsM2daDhqv2X4/ItCiJNCiJMXL15MMt7MiZLfr/o1SvQTQgghhBBCMsXJURNCeGg4ac9KKV82bPNJAP8awC9KKb+r20ZK+RUp5Q4p5Y7169cnHXOmuMjvlynRTwghhBBCCMkQF9VHAeAPAXxTSvm7hm02AngZwBeklP853SF2Fxf5fQEw/ZEQQgghhBCSGS4RtV0AvgDgLiHE7NL/fkEI8WtCiF9b2uZLAH4QwO8vfX4yqwFnzcTuLW2Kj2EkolMkCSGEEEIIISQpI1EbSCn/ErD7LlLKXwXwq2kNqpeMj5Wwb2o2cjuXFElCCCGEEEIISUIs1cdhoeSQ/uiSIkkIIYQQQgghSaCjpuHOW+1CJwUvj4ndW7o0GkIIIYQQQsiwQUdNw+Gvv2/9/Kn7tmJ8rGTdhhBCCCGEEEKSQkdNg6nhtYJOGiGEEEIIISRL6KgRQgghhBBCSJ9BR01DseAZP8tFafcTQgghhBBCSIfQUdPw+L23GT9bOcJTRgghhBBCCMkWeh0abDVoVb+O6ZlyF0dDCCGEEEIIGTboqBmw9VJ75OXTdNYIIYQQQgghmUFHzcDE7i3wDAVpVb+GQ0fOdnlEhBBCCCGEkGGBjpqB8bESblg1Yvy8XKl2cTSEEEIIIYSQYYKOmoWKpZ+aAJj+SAghhBBCCMkEOmoW1lhk+iWAfVOz2HXwKB02QgghhBBCSKqYc/sIhEPPtHKlikdePg3ArhY5PVPGoSNncaFSxU3FAiZ2b7FuTwghhBBCCBleGFGzYEt9DBIlLjI9U8YjL59GuVKFxHXnjpE4QgghhBBCiA46ahZuskj0h7lgERc5dOQsqn6t5W9UjiSEEEIIIYSYoKNmYWL3FhS8vNO2OSGwefKwtmbN5MTZnDtCCCGEEELI8EJHzcL4WAn3316CQ6kaalI20xr3Tc1i7InXmw6bKTIXJ2JHCCGEEEIIGR7oqFmYninjpVNlyATfvTzvN+vQdJG5gpfHxO4t6QyUEEIIIYQQsqygo2ZBV1sWB1WHNj5WwlP3bUWpWIAAUCoW8NR9W6n6SAghhBBCCNFCeX4L5RRqyFQd2vhYiY4ZIYQQQgghxAlG1AxMz5SdatOiYB0aIYQQQgghJC6MqBk4dOSsc21aTgB1zcZZ1KGxcTYhhBBCCCHLHzpqBlyl81evyOPqQg15IVCTsvn/pQycKNU4W9XNqcbZAOisEUIIIYQQsoygo2bgpmLBqUbt6kLDaapJiYKXz1QkxNY4m44aIYQQQgghywfWqBmY2L0ldo2acpqARvRr18GjxibYSYjTODuL4xNCCCGEEEK6AyNqBsbHSjh57hKePXE+Vh+1cqWKTZOHIYDm99JKUTRF+cKCJUyRJIQQQgghZLBhRM3Ck+Nb8fTe7ciL+PqPYecuGG1LimvjbFuKJCGEEEIIIaT/oaMWwfhYCZ+/Y0Mq+3IVKLGNxaVxdpwUSUIIIYQQQkj/wdTHCKZnypj6m/dS2VcaPdVcGme7pkgSQgghhBBC+hNG1CI48OoZ+LU4VWp6suipZsI1RZIQQgghhBDSnzCiFsHleb/jfawd9fDYPbe1RcKyal6t9sHG2IQQQgghhAwmdNS6wOiKEa2TlqUyo0uKJCGEEEIIIaQ/iUx9FEJsEEIcE0J8QwhxRgjxG5pthBDifxNC/J0Q4utCiJ/MZrjdp1jwOt6HTsSDyoyEEEIIIYQQEy41aosAHpZSfgLATgC/LoT4RGibnwfwY0v/+yKAf5nqKHvI4/fe1vE+dCIeVGYkhBBCCCGEmIh01KSU70sp31z67w8AfBNAOKfuFwH8kWxwAkBRCHFj6qPtAWmkD5YrVew6eBTTM+Xm30wKjC7KjNMzZew6eBSbJw+37ZcQQgghhBAy+MRSfRRCbAIwBuCN0EclAEEN+2+j3ZkbWEopyNqrGjTlVCVVZlS1beVKFVKz3+UGnVJCCCGEEDKMODtqQogbALwEYJ+U8ntJDiaE+KIQ4qQQ4uTFixeT7KInpCVrH6xBi2pebXJQhqm2bdicUkIIIYQQQhROqo9CCA8NJ+1ZKeXLmk3KADYE/v0jS39rQUr5FQBfAYAdO3Z03pysS4yPlbBvajaVfQVr0EzKjDZFyGGqbbM5pVS0JIQQQgghyxkX1UcB4A8BfFNK+buGzV4B8I+W1B93ArgipXw/xXH2lDQjOC41aCYH5eHn51Ac1atQuux30Bgmp5QQQgghhJAgLhG1XQC+AOC0EEKFlX4bwEYAkFL+AYA/AfALAP4OwDyAX0l/qL1heqaMh5+fS21/8wuL2DR5GHkhUJMSpVAz6umZMsoGR6QmJb7/4SK8vIBfux6QdKltG0RuKha052I5OqWEEEIIIYQEEVL2JgNxx44d8uTJkz05tivhFMROyecEavX28+3lBG5YNYLL8z4EgKgrUix4WL1yBBcqVdwUcvTC4z905Gzkdv2K7vwXvHxLLR8hhBBCCCGDihDilJRyh+4zpxq1YUWXgpiEvBD4gULDEdPh12XzMxe3+UrVx+xjd1u3sdW5mZycfnPs1LH7aUyEEEIIIYR0AzpqFtKqhapJaXTSktBJnZtJiCOJY9cNTIIrhBBClhf9tlhICCG9JlYftWGjH2uhvLxwqkeLK8QxTLL/hBBC+gu2YyGEkHboqFmY2L0FoteDCLF6xQjGx0qRjaBNTqbp71RYJIQQ0iu4WEgIIe3QUbMwPlZyqhnrJleqPqZnyph4ca5l5XHixbkWZ21i9xYUvHzLd23qkHEdO0IIISQtuFhICCHt0FGLoNQDR6VY8IzHvalYwIFXz7TI8wOAX5M48OqZ5r/Hx0p46r6tKBULEGj8DptaYlzHjhBCCEkLLhYSQkg7FBOJYGL3llQl+qMoeHk8fu9tAKCVpp/YvQX7pma13w0LlsQR4qDCIiGEkF6he9dysZAQMuzQUYsg6MCYGlGnySov13bcsONkctSSElbaenrvdjpoAwyV0wghgwYXCwkhpB06ag6oyNQtj/wJahk3CL8877fI4uteUgUvh6pf1/49Lv0qy0+SwetJCBlU2I6FEEJaYY1aDLJ20hRRSlerQrVkUX+3QaWt5QWvJyGEEELI8oARtRiUioWupD8CdqWriqF5tunvQcJpcabfk0Rpa9BT7gZ9/ACV0wghhBBClgt01GIwsXsLJl6Yg1/PPrKmlK6mZ8p4/JUzqFQbTtjaUQ/FUa9NOCT4HRO6tDgBaFsQxFXaGvSUu0Efv8LkfFM5jRBCCCFksGDqYwzGx0pYMZL9KRMA7rx1faNf2gtzTScNaNSwXan68PKtrbiD6limZti6tDi5dDzTvlwZ9JS7QR+/Im6bhajG6YQQQgghpDcI2aW6qzA7duyQJ0+e7MmxO2HT5OGuHMcU6VIUCx4ANJ24nADqsvH3qwuLbX3WoigVCx2l/G2ePKwdrwDwzsE9sfbVCwZ9/EFcUzjDUUSg4dTZ+u0RQgghcVkOpQWEZIUQ4pSUcofuM6Y+9ilRblal6rdETlQ2ZjD65kqpWMDxybtify/IoKfcDfr4g7gqp9miiOr7fLkSQgjphOVSWkBIL2DqY0xUJKsfSKMJdycNRYNpc1evLbalYwLA/MKiMZ2un9Lu4qYMLgeihEfUy7VcqULi+suV6ZGEEEJcWS6lBYT0AkbUHFGRhSQRq34lLwSeum8rAGDXwaMtURPA3ng0vEJWqfrwcgKjXg7zgR5v4b5wpu8nXWGLG/ExbT+MzVajooguETdCCCHEBtWICUkOHTUHdLU8aRNVk5YF9aX6xLDDNPHCHCDQrHPTOVE6I96vS+gEMXXGfRpOQBxnL6yeqdt+2JqtTuzeoq1RU446X66EEEI6ZTmVFhDSbZj66IDOqUibH/3Y6jb1xaxZU/CMDldYjCScpmAy1k1NwcPbp+EEuKZTKIdOFw3th/QLlxTQLNJEx8dKeOq+rSgVCxBo1CoGhURML1G+XAkhhLgyjKUFhKQFI2oOdCOC8LffuZr5McJcXViMlcoZPA+mFbK8EFpnLWzcp7HC5ursRTnavYwQuUQFsyzEtkURoyJuxB2KshBChpVhLC0gJC3oqDlgciqUpH1vGhx0jl+TEAJw7dAQdKJMRvz9t5fw0qlypHGfhhNgui4SjZq7O29dj2NvXdRuY/pdYbI2sF1SQHtVK8aXazpQ8YwQMuwMW2kBIWlBR80Bm1Nx4NUzuDw/uAIjUgJeXrSkOno50VKjBrQ7UTojXjlGVb/WjKyVDMZ9Gk6A7rooypUqvnrifOQ+bM7ho9On8eyJ801HPAsD2yUq2MtaMb5cO4eiLIQQQghJAh01B0xOBQB8/8PFXg4tFVavGMHqlSOxVB+BViM+7NTUpGw6QeNjJavaohL62Dc1i31Ts1g76uGxe26LNGKD1yUqaqbDdpzpmXLL71GkbWC7pICyEHuwoSgLIYQQQpJAR80RXWRh18Gj8HUyh31IseAZ69GuVH3MPnZ3299dnZFHp09ro1dBoQ5T6hcATLww13IeL8/7mHhxzmkM6rpsnjzsnIJqivIFOXTkrHF/aRrYLimgrBUbbOhoE0IIISQJdNQ6YJBWxP1a3fhZ2GB8dPo0nnvjPdSkRF4IfP6ODXhyfKv2uyryZOJCpWpM/Xr8lTP44MNFrfiIX5OxIlcmYzhMqVjA8cm7IrezXVt1vtKoX3NJAWWtWPfIoiaRjjYhhCSHYkxkmKGj1gGuzkE/cHVBr3oYNhjD0bGalPjqifP4D2+WMb9Qa5skbZEnAMgJYTxHUYqTcRxhW72aIo5xbLq2YulYaQpEuNSBsVYse7IS/aCjTQghyaAYExl22EetA3S9QQaJvBDN9ETVl+u5N97Tbnt1oQaJ65Ok2j7KmTL1VXMhTmqYrifYQzs3GnuERaG7tgLAgzs3Ynys5NzDjQwOcfryxe1pNz5WwvHJu/DOwT04PnkXDQxCCHGA71oy7DCi1gG6lfL5hcWBUYFUTlRwhcrFsQoKamQVVfTyohm5co1EmKJOah/7p2Zx6MhZp2hGVBSEAhHLD5drqlvd3b8kguNS+0gIIcQdvmvJsENHrUPCzoFJWKObmJpO26j6NTz8/BxyAnDRR1GTpC7lUADWdMhShEOr1BgBswiJqzFsS5sAomvDTMdxEYhgXv1g4XJNdau7WbZvIISQYYZiTGTYYepjikzPlPHSqeg0qKzZ+fG1iVIya1LaPawAapLUpRw+vXc7SoZJVIl5PHbPbW1jLHh5PLN3O2a+dHdq6YU2IZNHXj6N8lLD8nBKZxS61MhgDZxyEJPu35UkaXhET9Q1BaJXcZmSQwgh6eEyLxOynGFELUV0TkGWrBzJ4dpiu5rjN97/AE/dtxWPv3ImUrAjjFkbspX5hUVMz5SbUSddBMGmdOcisJBGyoNpW915CRrZunGFI2T3317CsbcuasffjSbHLLJOF5d70iXVlyk5hPQHzGoYfCjGRIYdITsQe+iEHTt2yJMnT/bk2FkRp5dX1rx7cA+Axovq4efnOhL1MFHw8laBjk5fkrsOHtUaxa4S+7Z92Ch4+TYH8/7bS3jpVLnt76bfb7oXBIB3lq5Np6Rxfkg8ws6xjiTnvxcGJY1YspzRPatR7yxCCOkFQohTUsodus8YUUuRfpLrD0a7AGDf1Gzqx4iKEMWRlNcZjUn6T4X3c+et67UOVk7oWxbkBLSRMNVXzvX3dyOvnkXW3Se4uluuVNvqMZOk5PQiMspoLFnudCOrgejhIhAh6RFZoyaE+DdCiO8IIf6T4fM1QohXhRBzQogzQohfSX+Yg8HE7i3wcqLXwwCAljqZ8bES1o562u06HW25UsXYE6+31EbFrZsy1XMBaNa/qbFW/Rr2Tc22HdO0n5dOlXH/7aU2mX4vr7/1TUIqpoikySkyyfvfeet6/QESYHL6ellkPQw1c0pq/92De5r1mElaQCh6IT9NyWuy3OFCVm/oVn02IcNCZOqjEOIfAPg+gD+SUv6E5vPfBrBGSvlbQoj1AM4C+GEp5YJtv8sx9REAxp54vSvy/LaokEIAzajSa3Pvt9VlqZS+NFQqvbzAoc9uA9BemwYAo14OK708KvN+2wqbKYUvLwTqUmJNwcP3PvTbnKicANYUvOY+TUqSulS0tNJU1456mPnS3drPHp0+jWdPnG85jgDw07esw7vfrXa82qjbf9apPbaV0n5NNer31d1upMn2wzGXI/1+bw0zTA3vDTzvhMSno9RHKeWfCyE22TYB8BEhhABwA4BLABYTjHMgCb+os3TSlOy+alQdhVrN0jliDccph2dPnE8k5x/Gr0kcOnIW8wuL2rHN+3XM+w2pknCalWmFU43JJIhSl2ieb1vKqW7/ptTEYsHDtcW6syiM7bQde+timzEsARx/+1Lz30lTzpTCaNgJvP9293TTuESly3WSapSVwTsIKX69kJ/ud8nrQXCABuHeGmaSpM6TzmEkk5B0SUOe//cA/DiACwBOA/gNKaWreOBAMz1TxsQLcy0h/iz5+PpRFLx8KsIgVb+Oy/M+JNyaXLtQrlSdHdVgmlXWxqFu/6bUxErVx8qRnDFVNMwVi6qm64tJ9bCLky5o6ud17K2LTsdMQlS6XNIXdJapMoOQ4tcL+el+lrzuZupUJ6m6g3BvDTO61jG9ju4PA/2Ykk/IIJOGmMhuALMA7gJwC4D/WwjxF1LK74U3FEJ8EcAXAWDjxo0pHLq3PP7KGfgu3aFT4m+/czW1fWUx6riRuXKlis2Th7Gm4Dk32o6Lyfi0iUJUqj4KXh5rR71Ix9P28okjLqPOm+uqfC9WLaOOmTRKk2XRf69Xd10iQ72Qn+5nyetuiUDEjYiFr6Xp2Y57b3USPezVdweFOIJWJB0YySQkXdJw1H4FwEHZKHb7OyHEOwBuBfC18IZSyq8A+ArQqFFL4dg9JW6PsuWMlxfwa/EvqUR25zEvBO6/vZGSt39q1mqMhEde9WtYOZJrk+oPEvXymdi9BfunZmM7xS5GaT+myyV9QWfpTPUyxc/mCADtTlK36zf61YjtlnMdxyHUXUsTawpu0XjTfl3TJ3v1XUJs9PMiEBmOBZrlRhqO2nkAPwfgL4QQPwRgC4BvpbBf4ohSRexVa4C1ox72fPJGrYR9lkQ5hzUpW6T5w4ZyVD+sK1UfT+/d3pzU1hQ8CAGtIIqJVV4OVT9+JnCUUd4ZsoQAACAASURBVJr1qmWSdglJX9BZOlO9XN01OQKPv3KmpQ4yyoEbtpdot5zrOA6h7lqaEDGkdDuJHvbqu2Qw6aaB3q+LQMMOF2gGk0hHTQjxHICfBfBRIcS3ATwGwAMAKeUfAPjnAP6dEOI0GmU+vyWl/PvMRtxHuKTGZU3Q6Jx4Ya6rqZhKxUk9/N1y0pSSINBIPzVF5HSiK8EakijD66ZiIfEL5/qE2OqkKaf22FsXcaFSRc6QLhpllIadIuVE7p+axaEjZzt6CZsm86fu24qn7ttqfdknOV86Z8rLCcwvLGLz5OGOjIpOV3c7MW5MjoDufo1y4IbpJdot5zqOQxgnmleJ8U7oJHrYq++SwYMGOgG4QDOouKg+fj7i8wsA9Prky5zH7rkNEy/OJUr5S4tVXkMPZnyshAOvnuma4xg0nGKtNqPz+jg1sdx563pcWzRHq0yOo2vksZOeZ6ZzMrpiBE+Ob23+2yRp72KUKqco7ZewLRK0euVIRyuyOscHAFaO5FqO6ddli6JnJ7+nc2c72XmNU6MImB245fASjePwdit1Ko5DGOdaxon8dRI97NV3+wmmcblBA50AXKAZVNJIfRxabIIU3eLyvI+JF+a66qStXpHH7/xSw9kw9Uwxkdb5KVeqbT3E0mbqb97DjpvXJXqRuU6IaRilab+EbZEg5UwkcZ50js/EC3OAQORiRy+MCpvD6jIOkyOwysvFelYH/SWaxOHtRupUnGdPdy11xI38me6RO29dj10Hj1pTrjuJPC4HwQdGidyhgT586BYxlssCzbBBR61DggbF9EwZDz8/l3oKYJQDGIw+pHGsYkRKp5fPWVMO41IseLh6zUfcUq6snWLVGy7JSz/OhNipUZr2S9g1ehDXedI5PnFSdbttVNgc1umZcuTvNjkCQHt9pM2BG/SXaD+v5rs+e+FFOR15IWLLv+vukTtvXd9SWxucZ3XOSJJFnuUg+NDP91VapBUxpIHeO7oR9Q0fIzyHqHnj/ttLLX8HBmeBZpij53TUUkTdNC4rr654OQEvL5rNorNGAvgw4lguDpqXa9Reudjh1xbrsZ20bmFzDoITR3DVe03Bw8Ji+/XPYkKcniknrnMz4Ro9ABrnx3UC7dTR6rZRYXNYTcag7lyY1BxdHbjwPTNoL6zlsppvSjUGrtfNppGau+vgUeuzF3RGOlnkGXTBh+VyX+mYnim3LYaWK1Xsn5rFyXOXWtLnXVgOEdRBpBtRX90xdNlGVb+GY29djKwz70eGPXpORy1lwiuVSVT/VE+xYsHD1YXFrjlpik6dzNLSw79/arYrx+uEYsHD6pUjRoPc5ByEJ47gC1XnyK4d9fDYPbdl0gfKFMGdX1h0ivyECUcPbP3x1hQ86wQadCpMDqULAui6UXHnrevx1RPntZ/pjEHTy+TkuUtN8ZigUxbGJcrR6QurF07eclvNzyoapa6NSzTbpYl8L4yxbh53ud1XCt1CgEICePbE+dgp+cshgjqIdCPqqzuG6S17oVIdyAWaYYie26CjlgHhlde4rBzJN1c9Bq1Xm1KCBOxpQknIogbw6sIiPrPtRkx97b22NDwvL4xGdRwBFaAhItLJhBJeYV072ujVZBvD5Xk/8aqT2t4mllPw8hCifQxBZc2gwaFz0ryciKxREwAe3LnRqTdUXEPE9J3pmTJeOlU2fm9NwWvWEKnvmV4mwdVNXV2ezaELj7+TF1avViV1q/li6fi7Dh4dSIMxbWPHZpzrsDkjj06fbrvnunGdu31/LYcokW7+iXq3SJgj+jYG0UAfdLoR9Y2zr0FdxFjO0XMX6KhlSFxjXqEMr17ehIUEkcCwpHo4T7oThAA6Kf0rFQuozC/g6kKoRqomm/3fgo6gioABwPYDr7c4SI/dc1vsa9PJtZyeKbe1XnCtSexk1enAq2esDpRNFONCpWq8//NCoC5lS3QpHL1T/1+K4XDFNRJt37E9u15O4OrCYpuwimn78BnU1eXpHDrd+Dt5YfVqVVLtO7jQ0E0nYhBSReO8K2zOyPRM2Zj2FLzOWZyTbt9fush/cJGo365xGNP845py3i0G4fnpV7oR9TUdI7ywbZo3BuH6LtfouSt01DKkk8lUPTS9amL91H2fxD7H1EVFWFL9qyfOY9ct63D87Usdj6cTJ01NUKZUTBXpkWhE0Q59dlszqqJzkCZenIsUXAnTyYRy4NUzHfXHKy/VkcWdfKN+3+V53xjlvKlYMN7/dSnxzsE9LX/rhZFo+47t2b1h1Ujbuan6NWuKqAtRxjXQ2Qur16uSplYaWRrzaUR5umHIuF6D8MJFeGzzC4vWtCf1nSwiX724v3R14YNSv2Kaf1zmEQl0JRo97LVBndKNqO/E7i1tdoqXE9j76Q2RGRqDcn2XQ/S8E+ioZUgnjpaKSGUtQa9j1MthfKyUSuri8bcvNWvueoUyBF2uh1+T+M3nZ61Oql+T+DBGlLCTCWV6ppyKoqdp8jX1NXv8lTNO+9VdVvV7TfdPToiWqGvUy8REcOxRxmmcz2yLJCWLA1qTEgUv35bi18mtHz5WJy+stFYlkzguURGjrIz5TqM83TJkXOYmAbQI0+jGFnUMILvIV69WvQe1fiXOPKJDiYvsm5p1zjyIy6Ce236ha7WBov3fO25eFyk6MyjXd9hrLHO9HsByZmL3FhS8vPaz1Sv0f1eotMFe+Dfzfh2bJw/j0tVrqeyvl06a4kKlar0eQVzGa0sLHfVyWDvqNVodFDys8nLYPzWLXQePYnrGXPekQ6XxmCgWPJSKheaxvHx4xlbjrbXsa3qmjO0HXse+qVmUlxwdVT818eJc4trIvBC4//ZSs8+T7nzXpGwe76snzrcc/5GXTzudI2Wkli1OGtBuJE7PlLHr4FFsnjyMnNCfK/USCI9dOUQmw7NULOCp+7Y2r0epWMCDOze27UcpuQbRj6R9/ONjpbZjuKoN2n5TkOA5Ct+z4fNuumbhfbg6EWnTaZTHZMjsS/g8m3CZm8LnKE66ZFCMJ6vIl+v9lTa9jhQnxXUeKRa8Zk1ymHAKcVr3o2JQz23a2ObEKMbHSjg+eRfeObgHxyfvysSZDpcoqNZCUQzK9R2E9MwsYUQtQ3S1GYqFxTrWWtLn/viN80aHoRuNtSXszkivKcWMVt5ULDSvR9yUzrhIiGZ9W3jFe9/ULA68egZ7PnmjUyTJNmF6eYHH721VkpyeKRt/nyn1KUgnKZZAwwl76VS5RZUsjuqja0NpFyM1bCSGf7duLOo7USt4uvM3v7AIoD3q8drc+81tg7WP4bq8qJqC8Mvq6b3bU1d+i4oguazA6vZhm7OyNOY7jfLYnr80o2vheiuX+hJXYyosxpP0nEQZS66r3mkbXYNav2KLjuuEPzZPHra+96t+DQ8/PwfAfj/GOf+Dem7TJGpO7LUT0YmzNQjXd1DSM7OEEbWMMd1Ifr2RPmdaSbfZy30QoOo5cQy7oJEzPlZCKeNJSBmuJmfi8rxvjSS5RH2EQLOWLojt99lSn9IkXNCvVhNd67dUQ2kbtpeQKdJkEzeJE51SUa1ioXWV+/K8j31Tsxh74nVMz5SbL5jgIo3qURiMOAZrJNXVDo/FNZIVRdTqrs0RA9yMApNctO5OXjvqJe4/5kKnUZ4og0UZx2lEMtS1effgHjy9d3tk1NQ0tmCUvbTk0AdToJKcE9f7L+r+Sus+DtKrSF6nxI2OuxjPNSmt5zPu+R/Uc5smtjkxi/s5Lqb7wuV+GYTrG/VOGgYYUesCpjSyfo5Y9RqV6qGLOIYN5ChWea3rERO7tySOqtnGFSRubV8cSfuo5ro2OfSggmWWhA366ZlyrEhwVI78moKn/R3FgofZx+52GpNCJ27isop3xXAeVVuERg9Fc/TJ5NAEW1woulVLEOWIuazAmvahfls3V551WQ3h+cCGS52wMo6Dx+sUFyl1U0QmHGXX7fvkuUtNtdtgurKJtO6/LO7jOPUrvY5+hIkjma+73jps5zPu+R/22iDAPif2Q41XJ3XLuut7563rcejIWeyfmu24zU0aDEp6ZpbQUSN9ycyX7tam6BW8PD6z7Ub8ZgxH6/K8j4kX5nDg1TOozPu4qVjA6hX5Nql+13E9On3a2Ai5E+JI2kcZYoA+jSrKSXPpa+aCrp4mzh6jJmFDoNH4dzUm1zSPqFW8R14+HZmGZDKo1G+L8wLq1ssq6hy5GAU2IZawA9otgoqTrv0FVS89l/s2TePMJPBjMoSS9A586VS5uQikS1cOY7rP4i5IpXEfm4xCl9/drRSqtA1XtT/XTIi459l2/uM4k8sR25zYTSci6r5Per8F9+H6jATHsqbg4erCYltf0PB3kjII6ZlZQ0etC9hq0fqJYsHDB9cWUeux+ke+xdpuHUvVryVyksKtA5SgQxyHREXyjr11MfbxXYgraW9DTb4uQg6KYP2Urq7SFd1qXlyDTjcJu6g82p6zOCuPcVdR45ATAtMzZeMLSNdMu1svK10EKZw6DNiNgn6TUk666h33OkcZZy7Gu85QMjVIB5IZ0QdePRP7fNh6NcVp/RH3Pg6fs3BvzjhGYbeiH2k7hHGboQP2dLhhN3p12J5N23xmUjZO63yqcYUXXHVzQBr3cJIaZJ2dkOZz1W/vk17AGrUu8Ng9txnV+OLiolqYhId2bsS1xVrPnTSgscKrFAmzSg/16xKrV8Rbp1D+YxarZVGKgkknftexPrN3O2a+dHdzwl+90u3cKKda/b+pziJvC3WFCE7Cql5v0+Rh7A8oVJpQhqMOXU3I/bc3UhDDal6m872m4Dk7nQUvp63LUqlym35Qf4wPri221Tzceev6zGsJdBEkAbSlxUXVIY2PlXD/7aWWeyMqtS4rpmfKxutVrlStCm5xn3PbM+pay6IzlPy6bFtQCtbIxFGjs7X7sP1e030mEa1MG96P632sO2fPnjifuF6lW9GPtGtq4i4Y2OYF3fn3cgLzC4uJFA2XA1HPpq2WMMsar+C4AHOfzTRJWoMcZ19xiVvLuRxhRK0L6Fah5xcWY0fZhEAsOeYRx4hRDsAfnziPfqqY60Yd1ZWqH6tJcWXpeiXtj6dWxEpFe/+wOKtHUav0rmPdNzWLQ0fONr/vMskWvDzuv73U/B22Xj5xGkGrF9DJc5daVs9d9qAMR1OaRvAcmVa+T567hKvXFtv27eUEri60/93EYl0ax1z1azjxrcvaz8KLJVW/hmNvXcT9t5ci64o6SbnSRVok4keQo1LrulUnpK6vDVu0w/TsFAseri3WY63wuq5Ux5lX1NjjRG5shl3Wi0FAvJonUw1n0jF0K5qUdnpnnKXTqH5q4fOv0taC2SZpp4NGPe+2+bkb84TLs2mKWKVV4+U6rjBppAwH6aQG2fadThn29Fs6al0ifKMlSWeIYesCAD69aS2Ov30pcrt+ctC6SVyHSwIYe+J17PnkjS0ORJzvR9XpxC2MNxlqah9R0uhBgo6K6UvBWrk4aUhx2ymo1fMk8d3gi8R2jkwvaF1qrauIjEI41PnFcV7LlarR+QH01zqO0WWLtKjIk6vB5FLfZ7pngym3KhU36QvadeXXlKZjE+tQ+3c9J1HGu4tTGSYvROxUviRRMyBdB8/V6IpjhLoq3JnaasRJ33QZSycOYRLbAHCvAQ2e/10Hj7YtiqZdbxklbW9aLEua4qobg+1ZdXk2o9pS2Gq8wvXxtrkiroOe9J4ync9OapCDDFtqYtbQUeshK0faVeHS4qZiwbhiTxrMx4iOKJS0flLCk3+wj1ax4EEINCd0U5+s6ZmysYas6jf6kF29ttjsiRac8IsFD9/70De2f7DVAHp5gb2f2tCMoKnoju74QPsqY1znNmkSbk4IbJ48jDWG36oMkbipGRVHJ63g5Z1+Z5xorskof/yVMy3RHV16jGtfOhNKMRRwM5iSqKSF71lgSQToxei+UCbiXF/dtlGLJi5jUs+56SonbZlhu8dsv9sWJUziaALRrVKSRkZcF9LiKtyF504XcZk4vyFpTU3wnRCXpIZxp9G/qPMSFa0yfW56t8R1IF0cFJtjHbfe0JS67BKxjOugx7nmrvWZYSEy9d5R7weV7hkep5cTuGHViJMzSuJDR60HJF0xc0U9wFk3dh5k8jnRE4GXm4oFPDp9uiVapF5IQePBprY08cKctTG1KW1USddvnjycaOwjOdHibJmcjErVb7n3VDTo/ttLqaplmhQqdeczjDIsXI0i272iRGaC8u+rvFzk/bXz42vx5vkrkfOAzSh3SRFWfelsL07beTDVRsQVnrCJ5Zh+h1+TkcaZyViMc31NK9OdpNxEzfNBQyvuokHVrxkdfdsqu8mJ+My2G61RU9O5LHg5a5pXJ82CXeToo1L9wijnIE4UKa6xHicrwnQMEwJoLnyZUuezbGgdV+0vyhE0fW56t8R9TlwclCixkDhRa5fxmb7vslgTLJ+Ic9/Hcch1JRi6azvMLRu6DR21HtCpalwUqk9QnBX7YUEAGE0ozZ8GLn2ZFLoaloefn0t8TStVH5smDyMn4qfRNsaTPEm26tfw2tz7sXqp2VAvKiCZQqV6ueyfmu1oPMrIfenU9QL8y/O+k6rou9+t4qn7tlpX0fNCRG7jwsPPt0emgkZXXGzpkBO7t2DixbmW3+7lhVUlzYZpfLrIcrlSxf6pWeybmkWx4LVdA51zHyfaEccwsc3zYUMrSd2rqcdiUIjHRdLfJYXZtIq+WJfWaGtUGmzwPilXqi0RVLUP05znmuoXPg+m82y6z0y/IVzXGySug+9iE7j83ripd3HSQZOo/UU5gknu+7EnXneO3Lg4KDbHY79hsVvNf0lrwuO0YFHEdc6CxHXIXe75XrVaGUboqPWArBv1qVSOnR93q1EbFgSAB3dudI7q5ASMKYJJj3/46+8n6iemXpJRTpqXE9ZoG5Dub4pDWgIxeSFaCs6DPbJcUMbs+Fipo6izqqEypbsUCx4++HDRujqsDLrNk4e190VdSqvIjEvkDmgY9UEjuNOofmQ6ZPjHLP3btWlvEFOrBtN+gj0DvZzA2lGvxbADOot2RKUvRdWYCKBp5Jjkt+Mglr7oIpSjxhwc966DR53EFIBoQSyVxhp1Di5Uqjjw6pm2hQy/JnHg1TNtx00qz607D6bzLJfOhYpWBdPSTSSpndI50FE2gevv1QkCmVLvgOsGefic6NJBk6j9RaWB2hbLdNdJArGET1wdFJNjbXO8XBc0TOOKcyw1ZyiV17iRrLjpuLb7Mcv+g4q4vSSXO3TUekBS1cAwqqZJZ6hV/Rre/W4VD+3cmFiUYbkRV8EubYcm+JJxReXJ739+1ikKttjhoL0coAuc5XMCP7BqJPN00dUr8phfqCFnMZBqUmL/1CxOnruEY29djO1sqIgzEF/kJMjoihHrquuVKMdUXG8lYPq96oWuq69Z5eViCdsEjeBOo/q2dMhDR862LRb49UYKo3JQXB1kFYkL4zp+ZaSGV6NNKW46Q8A1/cnV+VXXNLy9xHXjtBhKK7MhZaO9hkttUHBFXG3nGmEKG7OmFOpK1Y9clLG9A8NzTCepVnGUI4GGERpcyHPJXohTO2VyoIuWXquukRSbIFB4vFG1rWq74O9KovbnUudpmgskorOCqn5Nmy2gSKNe0LaAErWgEU4PtR3f5rQmqZcLohuXEMB+Q1Q4ykZNU3AmTJJekssdOmo9wGXVpeDlsG71Su1EEZZFN1GuVPHk+Nbm6iCJ33i511y+ei1W1KdT39KU3fiRlSN47J7bMqutzAngl+/YiCfHtwJoTNa2tEQJJK53uzzvNx29JBEeRblSxebJw0Yny+ZsAg0De9/ULLy8fjvdCz0YPbw87zdr/9RcoDMMglye97H9wOuZtL+IqjtRfx8fKzmnq+791IbEtSBBol7sOuNApVCaCI/BxXkMOp4mJ0KluIUdR1tLl7DRFPVecJlTohTlki44JqmhdkklTBKpSgvdcXTjMTnQuqhWwcvH6hdlEwQK4/r8B3+XzZlU6OasqGtnWiwrWWpag6i+lOpY4WMDnUXQgwsoOsJjD/9eWwpy+LOfvmUd/urtS233QZJ6uTBqXGmpRWf1bJkyVMJk6Sz2G3TUekBYWSdqgg4rBFb9mnOUbHqm3PEDxVq33jGfUcPvuFyp+pH3bSesHMk3peaBxjNy8tylzKLBytF75+L3O6oBkzCvurs+MyanKtwnzfSiPvbWxZZ6gemZstUIzqpHYVTdyZqC10zb0XYC12CKgCdxEnQ1n8pI0jnVUVcv7Mg4zbMSOHnukvV+C9a/uF7XcM2gi1FtwyXqEHeRQwlhKEPV5KwrcZ44hAWaXCJVaRK+F8KiTy7OcdAhSFKPlIXhHIz+fv9DvUpyOLU4SiQmTJSYh8tzbopwB8dgUlEOYqoDt80FeWGfzEyOqs5h+s73PkTByzXf+8EWJabMjbQEVnRRVts7Pu3+g4qkar3d6rvXC+io9Yjgw6tzxIJyqLo8fVfj9ZGXT3ckniEA1rqRZuqFmgjTdt6DL9qwspitnUCnHH/7Ejavv6EZwehUXCRNgk2iAXM0WNfvJ00n2oVw3YlOeOLqwuJ1w9xxcKaXdlIhGBUFDUce497LAu2y9C7Oo1+XTosPugjg+FgJB149o3U8wjWDLmI2JlydBF20wvb73zm4p+Xfj997W5uCrZcTzT51rjw6fVobXa/6NawcyTm3y3DBtdn546+ciawV1hGMqJroVOXUtbY1+Lt06cy28XWqkqmaRsdZEAzOgzqRIZcWDC514GFqUqYmNuTXZct5/jCwUJtWw/Y4yrsSjewuQETe82mRRK23E3XZQUDIHkVKduzYIU+ePNmTY/cjuvqGYGRt18GjA5e2RwYHm+OlUm2TNPlOQvjF7CKQ0gk5AXzrqYYRaTL6eoVqML6m4BkjYcqwzrLlh428EPjyA9taXohhAyupMI/NaN2UsM1EWrwbcjyyuHfCv1/3njAZskXLPeNyvCTGzccfOWy8zuHzlfQY4e9HOezP7N3ekVpueF9AdCpdJ/emQLtTq7DZCYBedCWYGh0UZojTC8skdhTG5gTGUek0zWWjXg4fLta195h6h9kcO9sYomws0zvS5LxHpa26ntNgKrTNRtRhSr+Na0s+tHOjsR2EC3Gec93vNKn1RtnHpndj3LTibiCEOCWl3KH7jBG1PiEq/7hbufbdJKiaRye0t9iMGJUa2C0nIDwSvy5R8HIdtQewEXzpPzm+FS+f+nbfpJy69oTrZc9EpUypq+24vk38/epWbbOM6sZBl+4UR6jIFZ2gB+AWxbpS9WOJ5QTr55IKF/zyHXpV3XxONOXew0ZbsCdYMJPExbg7dMTcTBy4nmH75Qe2RfafVKwcyWmVZEe9nFWMJi1sERKbnaAckDiOr23boMJgVL1tcCym94SLDRMlnCUh8Mt3bNQuGqrxxamrclFpVawcEfDr7c6CEEhUOxZXyt9Wb2dSSdQ9w7pF16goazi93obu+Q63/1D1v+Hovfpu1b/eJzLYisd0v9rqojut7esH6Kj1CVEF+GkpRfYTn7jxI4l6YJHu0g+OdNWv46EYrRWSMj1TRjWm3H+v6XWqpjIskzjzKmIY1cQXaHceelk3W5MStzzyJ/j8HRuaAjhZLKat8nJaOe6gYWNTiosTaV29pGIKmB0CpbCntgmP68nxrfgPb5bbUu1rS6qfQLvxGHymlTF58tylyN5uQPQ5l0vjPD55lzFtNEjBy7Wkm7X+fvd5YW3C2riolDIXoR5X4zMoLnHoyNkWBUAAqT9rUSl6qq7Pdqiq3+jHuXIk1xxbnGh9cAxxW5TM+3Vtuw9br7XNk4eNDnMSKX/d9TUtqqzyctpnOHz+1II5YFbjdZ3bdGPRpXqHa0kV4XtOoNF7NmqBxJYWGqfZd79CR61PiCrAT1u8oR9g3dtg0GsnDWgY9DtuXpeZo6b6J710qpyoGXgcltNzHDQs4774kqja9SK100RNyqYgzbP/zU9lsphW9esttWe63lame6kyvwDgekQ8KgIXXDAzXcualNg3NYt8TqAWEMoIjmveUA9drlSdUhCrfg3PvfFe23a6VXCXc65+S8XBcVrl5Ztqy2Hi1ALt+eSNznNVHAER0+/NCdHmFLhEJOMY+a641vGFca2DCy/sujpp4TEkmU/8usToihHMfOnulv2Y7kEJ8yJDJ1L+QWxKojrC508tTNhqYMP3V3DcUa1Moi5PUJNB991nT5xvqdXWkUSQJisxlCygo9YnuBTgh/vsdCqysJwMRrK8qcnrK/JZEF7ZzxIhkLkz2A2CimTTM2Wn9KhgBK0fVO0UpWIBlfmFRKJLx9++hE2Th5eK7rNFqbO59La6ulDDxItzOPTZbS1pS7c88ifa6xRM54xygGp1sxNl+q6Ae2TG1iQ+iEtUQgkhubzwKvO+tg2JyWA2pZy9dKpsP1AAFwERhen3qvMVNyIZ18h3QYnBuKToBaPoWU+JYRXdpPNJkntQPbe6c5JUyl991um8GHx2TS14gveXradZWuc0iIqK294VUW0YkvTS6ycoJtJHhB9Em+GQtDhfMRqQgCXLmyzru7qFSz3SILSRWC5OGnBdRj1O6vJDOzdix83rEglIZCmo9O7BPbFToXqJanDtck7CTkCU0EVpyYCe+tp7sUR8lAhGHMETE6ZnWefQ2JoTKzEN199i6mGnu0fDLQHU8VwUFcOYBETCitA1KVFcalZcmfeNiyO286eiDEmdo6h5Vicao35LL5+v8L1je3ZKS9EWk0CF6R6Mc07Vvfna3PvNOTS4+BXev0kQI43ShOD9F9W2xITtnEU9/2tHPYyuGDH+DpvAThjT4km/qz7axEToqPUpUb2Q4jLs0bNg80yX8zAIRr8LXr6RopShaGLfMOrlGvLGCSTJk7B6qe2F67OVpkz4oGJKi9Ip07kYK2mwdtRryhdIMAAAIABJREFUpjKZ+ij1GzYluDBhI8fFuSt4eeQEYkUYg2MK1h7HrdcqeHn85MY12tT4h3ZubNYE6nh0+nQzbTIvBD5/xwYce+uikyEbJwUx7VYeJuPfdG2VkZ5kDJ3OQwKwOohvP/ULANoNZlvD9k4pFjysXmk29NW4g8+BzcYSAJ7euz2RWmCnC0peXuDQZ7e1HCOJqmGcBQNTRNdVmRJonLMHd27ULl6o+d10XooFD4/fe5vxegTnaBu2RaIkvQm7ic1Ryz5XgyQirTSvgpfH2lGv605aTjQmnH5A9Tx65+AerHFsptrvhpoOgYazolg76uHQZ7cNhZMGNIq9u+WkAUBxdAWe2bsdT+/d3lxNDN/x6t+lYgFP3bcVa0fjN/M1Hr/gIRfzEfNy11c+e0Gl6mtTrZ49cR7lpdVolUozPdOaPjY+VsJT922NbDAbBy8vmoX06hj1DJ/9OEN/aOdG42flSrWpohh1TnJCtJxLl5Sfql+L5aR5OYH5hUVsmjyM/VOzLVHWD/26832fFwJP3bcV735Xb9A998Z7bfeFYnqmjJdOlZtzd01KvHSqHMtJA643px574nXjsaLUJuOgBBN2HTyKzZOHsevgUUzPlHHg1TNGh0oJuxQN59V0P6g+rZ1wU7GAz9+xQfuZ+rsymIPPdFZOWsHLN3ry7d5ifQ4k0Dy3CtPmNxULzWerVCxA4PocHmXoT+zegoKXT/BLGvg1iYefn2u5F0ypgeVK1TjOx+65rW0cqr9iEHX/KZXP4HHj1HGtKXiNGu/Qvu+/vSEydHzyrrb3o6JS9Y2iLIA9CyU47oefnzPWyJneK4MAHbU+JY16DPXAuhRRp8naUQ+/+8B2HPrstp4ahQqJhprRroNHl7XCpARwbVHimb3b8e7BPZj50t0YHyvFMg6JneBLLpibr4wEk7oVALxw8rzVWPmhj6xoe7HaLl2l6uN3H9jefEEXvFyk4+bXGy/lfrslwuctWGAeZHyshC8/sC2VRSC1kBE2vEzGbxr89MfXOZ37YsHDa3PvW7fZPzWLTZOHcejIWXz+jg2NXkMalADIpsnDGHvidZw8l66IU7HgAQLNe1t3LaWEcXxBVKsHk3NVk9JobJnqrWzGe7HgoVQsaJ2uy/O+8Vi293Ox4MUy1CXQdCiVUzPxwlykY1OTElfm/bZnoeDl8fk7NrSNoeDlO16AVC0cnhzfiod2bmye27wQLdHOToV/ou6UsFMCwKlZtZqzH50+jUdePq11ADqtX9I5TnEX6GpStixameYkATQXbI5P3oV3Du7B8cm7mrVv4XEc+tw27P3UhpbzKwFMfe09TLw417ZYduet653uZVObAonWtiU2x8925a4Y7LbwgoCLUFGWte5ZESkmIoT4NwA+A+A7UsqfMGzzswCeAeAB+Hsp5c+kOchhpBMFsXDovBfy6ifPXXJOOekW/TSWrFCGDHC9wLYwwnrEtAhH7Kp+zSn9qFypRt5//98HC1gRMLrWjnrY88kbtfLGwHVjJpgC5xI9/eqJ88gNQK1cuVLF9gOvN+txgvUGaYQzRpfk6IPpclk7sF9793Lk0Bsph4uR0eHgSvHU196Dy0m5PO/HViO0UVpKaYsa65Wqj6JDCqQy5Gyp51W/hn1LfZiCNT02pUoTQbEu07F0QgZR6pnFgueceibQbuC61gbWAazMCXzsI6va0oZ1taBRtoCXF/ZrGfjoyfGtLWmo0zNlbD/weiqLobZfXyx4bU3g46QrV/0ann3jvHb+UxFdXV9I116C6nNba5E4NL6jb+QdJbShk/M/8OoZba9S3XGPvXURT9231VqGo1IKTREx9VxOz5Rx6eo1435smBy8JAsCgyTLr3BRffx3AH4PwB/pPhRCFAH8PoB/KKU8L4T4WHrDG15clIS8vMDeT20wFqMGC5G7SRxjoB9IWgDuSrfrA4PGxfRMmU5axqR5bRcCRtKHfh07bl4HANrnKfiSjvvCGpR02KDRpwylnHA3ZG1cqFTx4L/665ZaqKxPi0tqbl0C9ZgpvGmcjyCudUwXP/iw5Z41YetnpPBy1xtuuxrdl+f9ptOWpK7Yr8nI711YSjO1NfENU6n6KHh57LplHf7q7UvW+6rTK1f169r6IlNPNZvRXavLZp8wXR2av9QPT1c/6tJQXNWSBc9jHFvBy4mmqqQ6rkskLYxp8+B+kjZKtgnRJK1/tYmBRfVrC48tjp1zYSm10mRHBmvbbDL40zNl/ObULJJYIirCqTuvSWzbQZLlV0Q6alLKPxdCbLJs8ssAXpZSnl/a/jvpDG240fXYWFisNY3uoEOmK67WKVK5YCpOXa4Emz2mWRwepBc2saphCTaTHDY6VUbtNaoO5csPbDMaM8r4TbpKqOTyERFhc1lsEAJ4+oHtOHnukrYPVtz96UhzTlpT8GL1chz0+8kVNSe6LPK5OGlR/YwUQSOuWPBiR2aSpvWpSKrp22sKnrFJt7ondN+v+rVIJ60bhA1cJYKkoy7R7BNmUgctV6ptTdhde6D5tXqbMxFH7GXvpzdEthfoFPXONI3JNtaoKJytDguwz4u2+UelK+6bmsWBV89olSOB+NoHyqmx9SlTmOzG+YVF/PbLX0/kpOWFwP23t0vsBxftbLedTgV2kGT5FU6qj0uO2mu61EchhEp5vA3ARwD8r1JKU/TtiwC+CAAbN268/dy5c4kHPsxESQc/On06UURLKR2pSMwgqJ91SnBFyCZbPYgqkIM45jTxcqKl38sgY3ohqfs3qdKYeuZdVsPj7NO2Jy/XuC976fQkUTVM8p1BpZQg2qFDKcE9Ob7VKf1LOYn7n59NnJqrFh9cvx7lFMZVrew2ShFPp3i54+Z12v6stmddzQmmhUud8RvXWQpn/rgukobVCW2qhEptMMk9XCx4+ODDRauypc4Oi4o8mebpvBD48gPbANgjnq7nOvjcBYmr4qjsQcBsdwb/vqbg4eo1H2km8STNeHr34J62tPbRFXnML9T6UqK/Y3n+CEft9wDsAPBzAAoA/hrAHinlf7btk/L8ybD104g76elQMvad1Mi50E+r08/s3d4iJx1m2Fsb9Csq9deWguQi2zzIrF568awpeE51TWFsvW+WK0IAD96xMZP07CwXR1bkBVavHOnYafDyAivyuUiHs+DlUZcS1xY7s7qCxrVLOn4a8vHvHNzjvHgR1cOpn8nnBL78uW04ee6S9n42Rc9s/RyTzJlJ7vug3eKaAeTaaiJua4a4PGOQ7rfdt6ZejWHJf9uC8TN7tzv3aws7WkD81gGmnniKXvfGM6EWmmz3lEurhW6StTz/twEckVJelVL+PYA/B7Athf0SDba8afV5J2ZCUPUny8L6Tpy0tMc18eKcdUX1wZ0b+0K9klxHKfY9Ob61qfqlo1L1cXzyLjy0c2PfKR2mwdWFGiSWarkkYsn1qzSQQSyu7gQp9TV/abDz42szu88WahJXqj4e2rmxozYPez+1AfMOUcGqX+vYSQNa03KVOl3UcTtBpWu5qptW5n1M7N4ykPPDirzA+FgJz73xnvZzkzNuTXMW8RdualI6qXoGCdotT45vxYMBFUkTwfqi6Zkyrl5bbNtGqV66tmYwYRpLqVgw2mE2xp54HfunZrFyJIe1o55R8r9oaCFULHgt6o5RqBrmIHHS/sI2j06+P4vU0zgUvFzbc1vw8pFOGjBYCpBpOGr/F4D/SggxIoQYBXAHgG+msF+iwWRUdVqroqMTh2/Uy6XaMypI2uvVUVGIJ8e3YmL3ltgvouWMOhVp9rSKolQs4N2De1paDwAN4890afJLPaTC/V2WI36MxuZrR72mgZClHH036ea9aOLd71bxYIaLAvUlJ7OTqNprc+93taA+J0SLcQeYjVEbSpwj6tzOLyzi0enTzs+86pnVb/ODyz2kRCbSjOJW5v3Yz1KpWMCnN6+NfSxV76auV9TvuPPW9QCuR3LCC6xqXjv21sWOHYialNoWB0kXty7P+81FtQ/9Op7eu70ppR/k8Xtva7M1wiIqgNszlNQeFLh+baZnytq+eOrf3cDU8qLq17V92469ddHpeR6UKHqkoyaEeA6NdMYtQohvCyH+iRDi14QQvwYAUspvAviPAL4O4GsA/rWU8j9lOehhxvSCVX/vF0Wbeb+Oy/N+7Ia8/cimycM48OoZfHpzdqvlg4ZyCD6yykU4tnPyAUU4hVrh2zR52Oig1KSMteoXd3GhD3yDRFSW1PJ+/J/9aV/X38Th83ds6Hnk+0KliifHt7Y0Qe83KlXfuT9SGoR7Qk3PlPGZbTfG2ocSFXjz/JU2A2xFqI+YUh3WPfPhx1U16t48ebgvHP0gcVyvNMd+U7FgdZhMzsuJb11OdDwlzuIyR6uG56Y5/XvVRoQtjQVrdc8FHaJVXsNk7tTOskVzxsdKOPS5bW39z1wcujDhRRKXCFK4+fv+JVXVuD0K00I1NFc94dQYdUg0FrJcHbB+e+ZNONWoZQFr1JJhy3EGYK21SsKwC1KQ/iHYUDWL3PhiwcPsY3fHyuMf9dx61LHOMXuUYq2tGL+b4xgfK1nrTTqlk1ouVad37K2L10UAEtQ4JqHg5QCIWGNXtTlprICrOuw1BQ8fXFtEzTEMrQz2NN+vaZCF2InpvV8KiGaERSWyvNeDRPZ6Q3o18DpBqk5ESsKoWjqdcncU0zPlWPaey3yRtNawk/ew7VqVNKIfSYWzTETV4XWLrGvUSBfRdZtXTpouFaATVK53t1ZeCbERrMPIIjdepZbEyeOf9+tOk6juPVTw8li9gs9WWpSXev5klXIdZxwTL8xh7InXMz2OrTYzClWnV14ytj+z7UasXtGd6HjVr8d6dlVtTlpO2vHJu/D03u248qHv7KR5OdHWHFsAffH8ZhER1xnrKnIWrJMKpu51Kzbh2oswlWPVZdvxVCPoNLKFalLiqyfO49FpexudcH3Yo9On2+w9LyeatW+6SFHUMyeQLIVWQLbU3D1kqTUsFrwW23XXLeu09ZIFL49nDKmhaZb39GvWQ5juzMwkVXSNLHcdPJqq4RqUz91x87qer1ITUpMSP/7P/hTXFuupK4YqYzAJdTSiBLampDr6TSlrObBp8nCi+qe08esy05TSgpezNqKNQ7AvWL8h0FhAUbVtnfL+lUaT8zfPX3GW/88LgRtWtattSjTEOlwiPN2klKJis1hKBXCRMx+19GdbbrioLsbhuTfeM0bVdL3ZdM+rX5fNHnibE0Q3JZJlfsz7dfh1iQd3NiL0z544b4zQC4HmfWRqI7V6RR6/80uNcxHu1zc+VkpNkVwg3qJsL6Gj1meEe1Xceev6ZnqKbbLsdJUhKMsfPkZaBsGwUPByWLd6Jc9XBsR1hlxQOfAu0uHdHBdJRr+lpmXBU/d9EoC5yexyYWSp/uzAq2es27kamHWJWE3OgcYCkc3pXr1iBKtXjjTTKRcWa07p0FmhFJvTcCSkbKSdqtY/YcMZwFDaBmsKHq5UfSeJ/KJDaqotkhUne+RCpYrpmTJyCUtWkt4zfk22OF2Vqg8vJ9pKAy7P+80G4Cal0g+Xtjc1Dp/YvQUTL851vDgigb6R5o+CjlofEbVyEu5yH2RNRNNOG+Emkjr61SBYOZJLRUI6Tap+feheXINMTgAnz12y9mMj/UO/1fv1so5XvQfSrk22IQCs6HDedb2Gfk06ZXM8uHMjXpt7vydO+pWqj9nH7g68v3v7Pkr7flQCFGHbZOKFubb6rSQINBzyfopKKkw1akK43b8SbqmpNlGLOIvwawoeHnn5tNP1F0Cm/XL9usSiZhxKSMU0RpMAmPrexO4tbSc/h0ZmS1ymZ8oD4ayxRq2PcFk5MakFmZ7zUS8XmYerJG9tqNq4fkgrCtJvThoZPK4u1JxVx0hvacg099drqxdOmlJPVFHgK1UfxYLXlfo8ic7n3ZGUL+GT41vx+L23tbyfulWrqBQAe91TCmg4EXHvxyjhu3Klioefn2v7bbr6rbgUvDye3rsdhz67LXUFvk73lhcChz63DYc+u61NE6CSclrz5+/YYPzMVWHSywsI4Z5Sv6bgZb6gbLoVbcfNC2F0TsuVKvZNzcIP1T4knY0GpY8aI2p9hOvKiW4708RR9ev4xuRdVqWcl06VsePmdZErCyoFUrdq2W+r3IT0A2tHPXzixo/ETrcieoYhrdGFql/D/qnZljm3UvUHRvgpzaCTl9OrwHaj7YSXF9j0gwXc8sif9FwdubRUKhHV6DeMy7Cz+m1K7l7ZHmlm7fz0Levw5vkrLfvzco1oY1SNs1LSDvbqVKRVL6l4aOdG7Lh5XTOtNJw+O+rlnOog/Vq8uth+nUtrUnYtSyFNYZIsoaPWR7iGoXUrLKbvqm1tN6SK0rmEgE37kUi3iDkM2wQMBwUvhw9DTSwHkbWjHvZ88kYce+tiT5201UNU4D9s6J6RXkd0eoFfR8/Ermo16fx8d2Mx8/DX3x+oufPyUj/H337561i5JPOe1rv+zfNXms2Py5Uq8kLAr0sUCx6+V/WNUZigkJoiWL+c9nXccfO6Fgc17EDN+3XkRDZtGPqVbtl6/dJ3OIr+yiEZciZ2b4lcEVUSuS7fDW4bdUO6riwUDekkqs4tKt3AyydLSKhL2ddSqr1omyjQvfSebpETYqAMDRNSNiLVva5VLI6uwEM7N/Z0DMOCkp5Ok9Ur8qnIgJNscA0M5oXAT9+yLtOIZ7lSTcWQ70UT4Hm/3hx7TUrn96ltOyWjr2wjZfxXLE6aEI3spENHzjYjZypaq+byNN9PeSGcUmbrsvFOGZSI+SDg5cXAqD7SUesjdD3SHtq5sS0/Whf5MvVXU9tGOYEuKwvTM2V8/8PFtr/nBDC/sIjNk4eRi5jkVb53XIIqU/1GXoiercwstxW2foz+qP40cahU/b6IbvSz9PpyQ62EP7RzY2oG1dWFWuqtKEj3qUmJr71zGeFlKJXWFiQ43/TCR1epZ73E9ZaP2s5UX2fcn2zsUwm3qUhaVnN5TUrnRfJ+eacsB9aOejj02W0DISQCAEL2KJ1sx44d8uTJkz059rBi6mQfzsc20WlH+KC65ObJw7FWpmzd6xW9So9UOebhmhGyPHhm73YA7fUTAsPVO4i4UfDyuP/2Ep574z2ma6eIyztgEPFyAl5eNGuSgql30zPlVNM6iwWv2UoAYF15FMUO1LRdWDvqYXTFSM8zL5IyaM+ki8J5rxBCnJJS7tB9xojaEDE+VsLsY3fjmb3bWyJv99/eEAlRXe9NxbKdFF56OdGMuu06eNSYQmkiajIoFQv48gPJonWdsHpFvinE8uDOjT1ZASXZUSoWmg3m77+91HJ9VcPbMAK9WQkn/UHVb6iI7vz42sSp3qSVXbesw7ee2rMs03j9umzrNbVvahabJg/j0JGzqaa3P37vbTg+eRfeObgn9ju4E4oFbyDnxKwFN6RsZDt5A5rfPEhOGjA44iFhKCYyhCjDE9D3bjP1aovbc0M10VZd6lWaXrlSba4iptU75c5b13ekGBUuEFb/jiocvrpQa56vJ8e3YsfN65oNy6N621Eps78J1nhOz5Tx3BvvOffOIf1FXgjUpcRNS8p4L536dub9ro6/fWkgjdMoso4y6Hjz/BU8On0aL51KV3Gv31Hv23xOoJaCVRx8p6ctMW+jXxUGe02l2qiHC8vNd4NRL5dKi4VBYlDEQ8Iwojbk2BoLhnERO1GoEPM7B/dg9cqRtsnAr0usXjHSjOwVCx5GE/ZHEqKhdpWmk/bgzo14Zu92rHL4vcHzNT5WavndJvJC4MEUa1myQgmWqGuko8/aWqXC2lGvmQ786PRp7J+aZRrbgFLw8vjyA9vwzsE9mNi9Ba/Nvd+1psTL7Y7Zdcu6nhy36tfw3BvvOc3xSd8j/UytLlMXlRlUo3W50au0x6pf75qT1uu6S2CwxEPCLL8ZjcTCFArW/T0oWGIjrExpOsaVqo/jk3fh6b3bcW2x3pL+EedhljK5qIYuqiUBHHvrYqwiYt1vtIXZv/zANjw5vtXpfPYSCWB0xQjeObgHj997W3vhe17ghlXLS3kSAL5/rSGaMz1Tjt2XiPQPeSHw1H1bAQDbD7yOfVOzQ7+6XyoW8O7BPYkMpuNvX+rZ+XNdKJn365FNnAeRNIIuwdKGid1blmXEt1cMWvpit95pQgCHPrcNez55Y0+ziPZ+asPAiIeEoaM25JhW1XR/VwpIF5Z6kuhQhlHwgYg6hs4hUmmHaaL2p8ZeKhaMk8aFSjVWPrOpt52OgpdraaR5fPIuvHtwD57Zu90YteolFypVTM+U8fDzc+2R0ZhNNrMgJxor/WneL35N4tCRszh05OxQOGnLoc2Drh7sBwojOHnuEiZemBt6B01x+eo1jD3x+rK+r7MKfvdaDbFTypUqJl6cw/RMuVlXTTpD1frv/fSGXg+lL1HPYq8XPKf+5r3Um5V3C6o+DjnhGjVArwKp2y6MUjs79tZFXKhUm/Ugr829b1WatClApt1EO6z6Y1KyVFEul2ObVDOnZ8qYeGHOmn+ua6756PTpvpJULxY8XFus97U0cE4AP/XxdU7NZxsrnxJR2W/KJFvOBm0ndKr4pVRa01plDavnLWdY3zqcpFXXPerlsGIkz8WLFCgttQ5SDbFJO/3SrDsvBL78QH/K8lP1kRiJ6r+mMKUB5oVoUY9UTX5VL5Kvnjjf9jII1v8A5siTcqrePbjHKT2w4OUiI1LhKJmtUbhLTZ4ugjg9U26mWUUVCSuFr0enTze/208F8/mcgBDoaycNaDgMZy584FTHcehz2/C3/1MjgmlbIZegMWyjLjuLetelxLsH9+DpJRXaTvHrEtcWh+OKFQJ1WGtHPfzYx1b3cDSkW9Tq7s2gbcz7dTppKaEE2Oik6VmRF33hpAGN9GnVH2+QoKM25ATTGVVTad1qgykNsC4l3jm4B8cn78Kxty66FXuvGGlRnbx6rb2JdrjOzcVpWuXlm+0HTAZ42CmMclRXjpgfESVSEHbSkqRZffXEeYw90XDu+skpymFwmmpXqj5++Q57Ko+S2wca1/7LD2wbuNqCfqITt2hNwcOug0exf6lPVBqNoodF8CUYNbwy7+Nvv3O1h6MhSVDTjkBjQcyFusx+8aiwDMVYsqbq1wYyLbYbY+6FoqUNk1heP0N5/iEmDWn+oOPjWtOltjOlU+rSAdV/29ILlNyw2laX0qlT/Qm2K1DoxublBG5YNYLKvN90aoFG+qRydOcXFhNPTP3oEPl12bNG4kk49tZF7LplHf7q7UttBo3u+qvrrmsET7Llg2uLzXNerlTx0qkyfnLjGu2162eEyK4myoXln+i5PBFLCaw5IZqRsn6471d5eXzo1yPby5BWBuUdGaQbY+7H0zJo/dS4dDLEdCrNHzZ8XeV+bSIiQGvELYgS3jClSQWPryJlwdXBa4s1nDwXXcOkhDPCY/PrsqmAqOrcVMqDSvXsR2erU2pSZtJGIAsZ7XKliq+9c7nZzDUoHKNL6VWsXjlCBbQuE+4LVfVrON4DJ23tqIeHdm5MJORT8PJ9aYjEpVjw8Mze7XwGuogyktX/q9toRY+bpF+e9yHRyFCggUg6pR8jjWv6ULTNBp/DISapNL+pls0lPTHo3MU5fhAXpxEATp671NIvqS4bKYaqHkyHiqSZVpqCY4sj3z/INAzYdK3RYsHD2tUrU92nwq9fV6JUTmY4pXd6poxdB49i0+Rh7J+abTrbUfTfKyc5TPlscHnex2tz7+PqQnsKtg01By4HVOPd/5+9ew+Pqz7vRf99ZzSSRvJFlmX5ImxLAWPjC7aDMCamKTgN5hKoCwkuhTbZJw2nabt3odneNTnZMbR04x4/PdD9tN2nOTlt2h1KTALxIaFgUmyabFIudmSDr1ziG/LdsnyRRtJo5nf+WGuN1yyt68yamTXS9/M8JNZoNLNmrTVr/d7f5X0/VaE6aXRZOqPw4PJZkWicGXfPQhvbTclEpMvPjBZRna2aiAvuv2Fm5OrFpjPVNQ8hooeXyiFIan4gv5jzG+tWjhidsAvmHlw+yzG4C/r+bu9jN1ry7FtHbf/e6XHAO/gqZKpnGNxuk6VsbidiggsD6dALBPem0r72XxixhHWU2AjGjSm0QULQqA+e+G1QtTUlsfELi0u8NdWjN5UOlE1PgNw1MIolNdw4Ddh096bwiyPnXdflUukpAD/edRxxnyNr5ehuKXSK3GN3L4jkNLNCZnM0JRORCzgM6Ww0OxHTGYVNbx+JXId231C0tscL16iNYWtXzfW9jssvu/VepXh/P+/jdHNxu+m43VTspnrarZczGm7G/H6vNOYCoMaSdtm6Hu6WeVPw/I5u2/V8d147HZvePuq5Ns4on+DnuYZx9TUlm87pZwvCWods1IIb7SmUvRpUjbVx9JtuUk7lL5KJGAbS2bxjlIhJ5BaGNyUTuDgwXPb1IeYOm8fuXoA/3rSzataKZZSWuGXb/tMjjn1YDapC1rU2cU1UTpD9EK1v5GUrrmzG6qVtkbzmFlLC43OLp+Pg6Uu+SsBUQinPg0RMsGbZzILKBo2Baiklx0BtDDMn6PDK+liN7+/UWHAaddjc1a0t7Hb4G7upnnaB5mN35ydC6Vj3kuM2JuKCjZ/XRja89kPn7GbH57z07nHXgMqcoKVzdjO+9twuXw2pXo8gbU5ro23GOSMotKuhVwn1iZhnHUCrsGoWVYJx7hv/n0zEkEpncz2J3b0pPLxpJ2rjgnhM8taLJeKCJ++5FsDl5D1xkUgmlnns7gXYfrin7HUHb5k3JZdEaGIyASm2qFyZPfvWUWRLeBwLOUciuJSFinDorBac2d0nq1GQDs7RIC6CrFJ5bY0o1XctRrVlNmXBa6o4vyUCgnIqHP3g8ll4YnX+2hK3gt7mgtbWbb1l3pS8At922+5UVNtpWwrhVjT86TVLRmx66SBmAAAgAElEQVST2/MNTckEGutqHLfdqRC2NWtnu0ugGmXmzx+1AMWLADi44U4A2rn9yKadgXpcjfPST6H7SprUkEBvfxr1eiBaDokYUBOPl2WfxGOC+5fNtB1NL1ZUitAaBEBTxLaJCicAnlqzZERG3YZEDOlMNvSRlrhoo8UUDuP4mds7vf1DVTdt0I7RQR6lwtduBa85okYVFaREQFBGAPTsW0dzowv33zDTNjByW5tWr/e+bO7qxtof7MqNsnT3prDpnaO2X3hzQDcxmXAcnXl+Rzc6ZzfntsEp4PMKZp2mYZrrhpk5Pd+QiAkeu3sBADhO60pnRgZpwMisnU7T66LufCqNx+5eUPZAJS6CT0xpyBuprKuJYXDYf8vGPDVv45YDgafFfPfNI7kR3KgGacDlkhZhB2ltTUm0T07aTnNKZ4F0Nvg+KWTQLZNVeOnd46Efg7hI5LJVTkxqHTwP63X1qLKKLRcwMZnA2u/vGjEKNTCc1evGhXsCMkgLlwJGtHdGi3RGYeOWA5EK1NxwRI1CFXR0zGm0qa0pmUuBXw5eI0zJRBwxsV+EOqkhga5v3pr72akGW0Yp24aa3ciUdRTP+nrGba7NVM/NbhqmkZXOekzsnm99TeO4Lf3TVwP1clt74oqtx1PqWWVOo2VGtrKo3qCM6aXW0RbzuQP4Gz2109aUxDGf2TBHE/NoZNBz389rj7X9GURjbRxLZlZfLb3RJhEXrLl+pu06RrNkIoa6mviI63syEUd9IsbRUYos83U+CtxG1KproiZFmjmbnlFX7NEX3sPmru4Rz1uxYSs61r3keBMod6Yor0yTqXTGccjfejOyG4VIZ+2DNEBbOO5Wz87u9YyX6u5NYe33d+HxH+1BKp0ZUTds++GevPTz5hFLa+bMp9YswSGbjJ5Bb7YTk4m886DYejylCtIE2hS/8fUjJxYYiWOilrHMWMZjHN8nVi/yzIDqt76h1bHeVK4e3VhirrHjtUYzKAYf7vqGMnj70Dk8YMoWzKVr5ddYW4MnVi/yTOz15D3XYuf6W/H0miUjrkFhf3eqRSIumFTB6+aKK5urLhNtJRR6X6wEBmoUGj8FtK3BnJNyf4nWrpobWl2psBr3xut4vZ5T3TAAeObNIyP2cyqdwdee24VH9ClGT61ZYltuAdCOl9NesXs8mYhDZGT2uKz+/CiV7lLQpp7ajfZ9ctZErF7aVrLzcMWVzQVdfI0RT2O0ecnjr+JhPRBvakjglnlTsHHLAXSsewkrNmzF5q5u3DJvSkGN3ZjImOwR7xsaznUuReFmPtYaXemMwrb9p/HGupV4as2SUIPbSQ2Jijaiq0VvKo0rH/0Xz2moxj3DrnTPWOzkAYDhrMKd106vWJ3KXxzpjUQCLy+TGhLautQKXN+KzW5ebgzUKDR+Clj7WfNSiS/R6qVtGGczsmLmdNm1XmicGnd2dVgSMXEMXozXCdpYNIJjt7VJGaVcRz0Nbq9hfXxSQ8K1J1VBm2aY8FkfqNTsAkrDGx/14Bub38PaVXND7dEX0ZK7PPOVGzGxwIaMUWpg7fd35d2Qz/Wn8d03j+SNnq79/i5seudoQY3dakqeEpRbvTlj/QIAX9ehUtcxNNaKjiXGOW6M/oelobYGd147PZKjdIlY4UXoa+MSetFjr++/VwN7FF8+XCmlZYhcs2xmRbILliupkhFkFRqPXhoYxlNrlmDn+lsD17VLJmJFBcKlzHhbCgzUKDR+Cli7jQ65Fa8uB7epGslEHA8snzXi4mDXkFq7au6IgMxI22+eptaUTAAO66/Mward63np7k35XltlHfU0CzI6aCQRcQss01mFxtqa3PovJ+XojPS6Vj/71lGsXtqGB5bPCu09m0KYVjejKYmNWw74ShWdzqqqLTFQSl6N0O7eFDrWvYSNWw64NuqfXrMEDyyfVbqGv/7CfguZjxbGOR52EpXu3hSe39Ft23Fh7OGmZGVG3dJZrSOnkGLMQxlV9npVn1s83fX31TCqUyrprMKPdx1Hc2NdpTelZB5YPgs719+aN1U8iHRW4dEX3gUA1AVs39x73RXY+IXFBb0vAAwOZ107qKOGgRqFxilAsRaJttPWlMybNlFq5nVyxhQxp20zaqg9sXoRNn5hcd5c/I1fGJnxcfXSNsd1Q+YpIo11NbaNaGvNNvPrAaXpwXcK6oKM5nX3pnLJYdy28XwqjTfWrXQN1vysSyt10zWjFDZ3dWPb/tMj3q8hEStoZPBcfzp3gyhkWp3xfYpqgpPRxBiVdDrKRkHfbftPl2ztmTG6d/8NM0v0DtGTiIvv9aENiVigTqy4iGPwp3A5sVOlpvxqAZfCg8tnBe6cK7fnd3Q7NnS/sTnckdBq1JtKh77GOUr9Nd998whWbNha1Hcllc6ifd1LgV/D7p4c/L2dO6ijhoEahcYtQDH4CeZKzSnpyS3zpthu21/et9h1Lr4dP89zuohnlbIN/t5YtxKHNtyJp/SF22ESwPame8u8KYFewwgg/Kw/LGSk0CxI41igTQ8KyjhPjPdLJuJ4es0STGqssw2yBfAM4IwbRNDPb3yfjPcphSi0A6K0jhHQ1lcmE7HcqFZcBA8un4VnvnIjgNInPjrWm8qV8AhbxHY1GmvjuXInfnrq+9P2JUKceI2k2iV2CksiJr5Gy9IZhe++eQR1NbHcyF7UjhPg3tB99q2jZd6aaApznV4iLnjghvBmd4ShUh2G3b0pfO25XUV3kEUtWZgTzzpqIvL3AD4H4JRSaqHL864H8O8AflMp9YPwNpGqiTFq5PZ7wL1mWKk5JT3Ztv80nrxnUdm2zamWmddIi7GPgxYjntSQwJ3XTrdNMKIA27oiRs+VF79px43ecuDyuVDquklxEfzlfYsLeh+78+Rrz+1ybfBt/Pzi3Dnk9KxjvalAn9+cQGTFhq0lG8GJwiRJpcpfey8ugqy+btPOQDprm8p5c1d3yXPuz2hK4v/4YbgjFEYJjsd/tKegHvFSfWTzSLrb6EG1lTlorI3jz3/jcrkVP9/53lQ61zEEYETh6Cjo1tcSAvn39NG8vjWISwPhHa9MRuGFHR+H9nrVLoxzLArJovzwM6L2HQC3uT1BROIA/gLAqyFsE41yfkelSsUt6cnqpW1Yu2ouZug1pDZuORDaPGbrdEunETy/o4t2I5huayuU0oqAO13ejKmL5umgbj1ORtamtqak70ZTo6UY9uqlbYFHB5OJOBpr/Y1EWUdEw5BRyrGHe4ZeYNw4v50+m/kG4dVbLshPalEtvYBenBriE5MJzyys5jIUYbj/hpk4uOFOxwQJdqM7RkKXMNuk1k9sXA+cSoMU4mlTlle/6ySTiVjetimUZpTHGKXZ3NXtGkBWWxjQN5TB9sNa8fTVS9t8r4Ezj1oFKXhfTg9v2pnLPGvMUCFNmOsGs9BGkCkc1ZT50TNQU0r9FECPx9P+I4DnAZwKY6OISskt6YnfWnCA/To3J3av+/yObtx7XZvrVFEv1qD3zmudF3j3ptLY3NXt2Lg1pi6aP7fT1I2mpFbk23hfvw3m8zY9wkGmVxr7KBF3vnQ11sZz+/Pe69py6erDnN9v11C0u/B7TfV1y6ppfq8w6qJ5iUn5ph02JROO03guDg4DADZ+YfGIwMlYG2j0pnqth/TrpXePA3AOHu0ef/xHe3wldAnCXD+sFImVRIBHNu30XJdrlUpnbUfh/XQyBNXdm8qVDhlNvvvmkdzarSCJhIwOw1JNyaSxwe/U27De68Hls4pKvW/tHApbpZLWFaLooyYibQB+A8D/8PHch0Rku4hsP33a35QqorC5NZ791IIDghf3fnjTTsfplka9ICC/ERXU5q5uPL/D/e+c1kbZTSVKpTNQCo4ZLM38rreyNgz9bLN5G42RALuAz2BMU1u7ai6e39GdO0Z2Ix+JuOSKtRbCWoA6SGIZwN/omHXb7PZ1GLffrCpdcXGr3lTacR1LJqvwted2AQAeu3tB3mftT2dHrA0MY5ON0RunBrT1ca8Rn0I0JGJ4YvUi29kGfhosMXivj1QKnutygzAScJg/g3mk/YECk2KEdRpGbW3Xd988gvn/9eVAnUYxkaodpYraetOg/M7ccBKVjy8A1iybif92z7XlSVKjf/DGupqC90FMpGQj51E5Ln55rlHz4WkAf6KUyorH1Ucp9S0A3wKAzs7Oapu9QKOE2zo5p55ca4PaLaAzXt/PGjJzvSDjeUaP8vbDPeic3Zy3nbfMm4Jt+0/brqHz0+tqTO/cfrgHz751FBmlEBdxnO9trIkwntPWZL9uz7pPJyYT6BsazmtU2404BekpNgd5Tuv7AG1qopF90o6xHsnYfwDQp4/iBGUtQG3Hbd2m2+cAtJ7J/qFhdKx7yfZ4m88D6zFd/olJ+MWR85HsiRe4rzHIKIVHNu0s+kYtogUnbue4md91o6XIFmbUP9rc1T3i2D6wfBa+++YR17/PAqiLCVrH1+e+gyJ6kCkjOyrM63ILXSdqZEk09KezUJBcx5Px/fa7/8NWzDuWah1c0Olr1bzeq64mjnuva8PzO7rzrkMxaOdrWBIxwXDWeY1pofqGMgWfB0aQGoXDp6AlePnhL7rLcj9IZ5TtWvggwpzubeW0Jj+qRPk4i0SkHcCP7ZKJiMhBXA5QWwD0A3hIKbXZ7TU7OzvV9u3bg24vUUk5NfCtjfGOdS851uIxkg64BQvm1wWc5/Un4uJaByuZiOdGaJy2yfp+a1fNDZSExO69/LBrcBqL6b2SbXi9t98F+XaMY7S5qzuUBfrmY25sm9+ENHbBvNEwaHIIdgs9BvWJWNmKoUaJ13cI0Pb1zvW32h4PY58D2s29mNENgdZTbNf4dvpuGu/v93x/es2SvPPD7btinLt+rlVWyUQc9YmY7ciiEcDZnddWYQdxMYfalE4aEjGu/SmxJlOngbmDLKwEKU+vWRJKx44T49x1C9qSiRiaG+ty1/3+oeGKlXkY65KJuGf7xnrfrjQR2aGU6rT7XdEzZpRSHUqpdqVUO4AfAPh9ryCNKKr8lg9wWtuhAF+JOMyv6/Y8rwameVqm13oTt+mdfqTSGTwcYGqmXdIY65RRJ4L8ZCXW4GT10jasuLKwlOUTk4ncdoTRSDDv9yBrHAH7qZFPrVmCQw519oLWfjEfgyfLNe3FRSmmnCQTcdfkDF7fIXPReqepqkB+qYZCGLUi//K+xbaJUoyU004j9X6n5xrJOIz1s8Y0UjvGuRtknShgBJxwbIjapbm3W9NmJPsJs6B3kCBtTmvjmOy8KLfeVBoD6SyeMiWyWb20DY/dvcD3sXd6VpuewKmUGfwUtA4Ft1OrPhHPu9+VIkgrVeF741pn/DsKCl3fFhfJrf13Uy0ZHwEfgZqIPAst7f5cEflYRL4sIr8nIr9X+s0jKi8/teAA9zVZXok4YHndYi8YRqDntPbM+n7FZg30Cj7c+A0SFTDixm71zFduxIPLZ+XVt/Kjb2gYj/9oj+d2+GkYW4N4pymxbgGuU6ZRp6Cg0GChHAkJvI5BGD3eiZiMCOLX37XA8++cWIvWG8HtU2uWoG9wOJfVrph9Zy5LAcCxNeQ0snSsN+U7Q5nx/TQ6C9xGq/qHhtG+7iU84zGt0kqhsKlJ1jVt9Xpyg0pM70vEgA9O9ZVsFCYRK22jd8WVzUW/fjkL6Vo7mYxOLb/H3iuBU7F1Ob14bee5fi1Z1+aubix5vDQJ0LP68oOwKWjrv59es8RX/cJSM9bB+82OapZRWh3Cc32Djs+xZlKOOs81akqp+/2+mFLqS0VtDVEEeNWCM54DOE+FSqUzqKuJjRiCt5u6tnbV3KKmbRiBnt8adV7rovywrsfzK0iQ6Oc9nli9CE+sXpT7+Zr/+rJnD3k6ozx7O42prl5TwuotWbTcPp/RgAYwYmqadY3ioy+85ziFK0ivaiHTTAvlNHUvzPUocRGkswoNtTVYf9eCvP1YSE2wSQ2JEedXWFNizdZcPzPv++k1ymdllH34+gvvek7TExlZ/8+Jsb/CPDfcpkQKkLdfz/Wn8cfP7Qy8DqgpmUBjXQ2O9aYcp5J6KflAmggeWD4TP951vCT1z974yCsZt7tJDYlcB8cjz+0sy1oq8/UxjI4jc/Bnd/+7Zd6UvP3fkIhhYDhbsqRJpa4JOsPhGhuGJY+/OmKqfZgSMUEiLr6mGZvbSIV+Vrf3sWZSjrpydqgQjSpGz7tTs/l8Ku1rdG710jY8sHzWiNcxLmxurCM65qluxjRHa/mAsHoeCxmZCzp6GPQ9wqg1FKSX9lx/Om900evz2U1ddBqFc2pMWBuldmUijF5dc32jUusf0hKyWM/5MIcVzGn5raO66+9aMOJYJWLimnnO2jgNc0qsmblwfNBz2nw+1vn43vppcJdqpMcoieG0DXYPZ1WwIM3obTeuc9kSRRjJIlOZpzMK2/afxs71t+Yyy0oIrxuWBlNNy3INaJqvj2HVgzRfC6z3v+d3dOd9lxUEv3WDeybSpF4CpBImNSQc09ob5XM2bjmAe69rC7XcDKB1oJQqSGtrSmLjFxZjUmOdr+ca52XQWrFBtqeahJH1kWhMc8sU52d0DtBGhqwZHs21tvxkfTRzGqUBRo4GWnuzEzEBxHttTyFTNp2SJTj1wDu9h1PCjmJ7SmPi3ktr13tvfr6f7JHWBkrQBov5JmMUXTbqeXX3pvDHm3Yi7iOBhqGxNo7+oYzjyERcBBOSNZ6jVUbQ+uQ9i/IS77Sve8nxbxIxyatFlogJNn5hsa/RMeuIq9OIsttrWcs8lGp6qPkY+xnRNn8nzaO2Qepv+XntMBWTqCgI6yi21/5sSMSQzqoRSXkGh507QwAtC2fQxCRWxnG33gdKMWoblLFtpchgasfaqRjGzA6D3ewLpw4wr0ynpV6zWKNnqLQyJyx7YvWi3D3Oeo82arA+cMMsbHrnaMmCqzAk4oKNn788tdyrPqJdPgC7704x15hqKnRtYKBGVS9Ilr1ScAo+gl4M7II665qmztnNeVP9nHiVDzC/l93+M14jSKp9v5/R/Nrm9/O7D92C0EIyyAkw4jNaA1tjuzscgg7j+X5uHtbgM0iDJZmI45Z5U7Biw9ZcQ8v6abMAsgFu3v1DGSgA4+trHLNMAiOPj1PtPWuDyemYSO5/rA/6D0i6e1MjShdYv0NujQPrsSimlz8ugvpEzHbtlvl91q6ai7U/2OXYwLJ2lBgBsPE6xTZujcQIYa4LKzZRURDm/WGs73TKlOl2jdt+uMe75IGPXWRMw/RT1sEsjNH/YhjbFtbIlhtjmqU5669dJ2ExnQjW/e/0uYwSNcVmcC2UXZAGjNxe41pmN/0+lc7gu28eyWXTPNefzn2njZIkUVATE2zccgAPb9rpmYzF7hyxa9P5KQVkZp4mXYn2YRgYqFFV8zNyVGp+14YFVcxnc7tJWTmN+ll7scL6fG6jjH7ewy0Ivf+GmbaNL6cGgHktmrV32y7ocGoox0V8NVDtgk+/oxBt+oiqtSZRsYz90ptK5xJ1mNNoW3upjePj1MixnmNOx6ShNj4iqElnFDZuORAoIDFn1wRGnrdOI4V2C8oLDYSMnmPAZ4eDQ4ulzSGtt3Eurl01N28E1Ylb8ABoU0j9pLB2Yv4+mRtYXj3mYbF2OgHu1w67a47xs1ewBjgHtsY0TMB/R5OxreWsb2i9/pm3LcyRLTsPLp+V61y03tPM2zSpIYE7r51ecP0t69pdp88VE9FmIpRh9DeImIhtvUy3QNq4Zxn7btv+05Eqjp5KZ3Pb49UxNGCqI2lXV/bhTTvz6rj6GaE2vp/VFphZMVCjquan8HQ5+J3iGEQxn81v4V6/SvH5Cn0PpxtXd28Kz7x5BMlEDIP6gvG4CO6/YSY6Zze7NqTcXtNY+wA4j576vdk7rVEE3GsKmQPKUjYsjEQdXd+8dcTvrMfHKdGK9RwzGmnmYtz33zDTMdPgsd4UnlqzJHAjyqngvFOQ9sDyWbbZXAtpvKUzKteQuPe6NtfpyRu3HLANtIxj7DRqa5yjwx4NHnPjxK0u5NpVc/G153YVNLJm/osB01SxMBv9iRjQOsFfh4DTbASvjp8nVi/y1bg19pH12nLvdW0jjm13byrXcWOdQm237YWKAZjYkPCcIpxMxF3PyWKTWXkxr890C1AH0ll0zm72FTjbsZ7HTt/ljFK5KdpP3rOoYiNrVtb1twByWaG9tu9cf7rg/RYV5u+LXWkP4PK+2X64Z8T5vHP9rfjG5vfy7jPW72e1YqBGkeR3BCfIyFG1KeazhTUdM4rcblwKWi+e0bc6bWI9Omc3e/a6u72m3RRI6+v4udmbF0lbGVNx7AI188hPOc5rv+8R5ByzZucE4NhAnmHaT3brM932s5+scnER/OV9i22Phd37BmnEGetH3IqSe32vnd5zYjKBR194z3Vak7nHeXNXt+2aSeMYFZtVzWAOkG+ZNyW0BqMR/01yCEasHQLme4bXVGYzv6OUQP76pYxSeH5Hd+76snppG7Yf7sEzbx7Ja3QbowHGqFxbU9J2NNngZ42wOWOj11q3+kQsb8q8MfpgrNGa1JAoabIh8/nu9l0y1wssJHCyJogwjofd+Wi8l1H6JeyC78WyfqcKHWUstXhMkAkxhaafY5BKZ/L2R3dvCg9v2omvv/Au0lmV++5Zv5/VjIEaRU6QKX9hjxxFgdHgcLr8+flspZqOGQV+Rj2sPXCA+4id22vaJa2wex23bfITJDs14M2phEs9Tcl4DzOnTpOg55j1deymcZr3k9Moidt+9pNVLquU6/fA+rmCrufyGvX2umbZnYsCIJ3Jup7z5mQETvvJPE3R7rMW2uQy9rV59MSs0LVH3b2pXPZbtzWy1s9rF7g4HRc/I9pOzKMATn9vbLU5eHPjJ2BsqNWabn6C7HP9aaz9/i7H0g6lCDTMzN9Jr++SMZruN3A22J0PXsfTfH0IEqwD2ucIsna3EN29KSx5/NWiE86IAA0J544BX68BraNIBLlp8XZTtIvld9/ZPcfu3K7E7KpSYKBGkRNkyt9oGznyaogG+WzlmK5YCUEbl34u1sbvnDKBeY0y2S1wNt/U/ATJTg14c0+x3fmeiGm3N+t9yq3HN7fwHM5rVwDvThO/59jmru68BBrdvSlseuco1lw/01cWU4Pb1Cm/WeW8Ojqsn7mQqYFu54vXNcs8MmO8s1eBaetndyrorpS2Dx/ZtHPE/vYaFXZaP2dsn9uohPE57M63mLh/tnRWeSYE8Lvmy+m4WJMrBamH1d2bChxYFOtYbyrQOrd0VpV1+wzWdaBe36XcdzNA6vk2y/ngNyug+Trgdf03M6aSGtf6+kQslx3UmNYdxloxa83BQinlv66iHXMHkJnTFO1ilOIMHQ2zqxioUeQETYQBjJ6RI7ebr/WGNJaZG1Z+pq34uVi7ZQIzGqJu+7/YwNhPp4PTlMDnd3Qjnc3v1b33ujZ0zm4e0Yg0UuB7Zdcy3ieMNaCP/2jPiKlc6YzCd988gramJJ5as6So9Ynm7QLcswF6dXS4TZnMKoUZTUm0T07i5x/1FDTq7eeatW3/ad+NFqN339xQderp7k2lc40/83oPtwQ15gyKbo1guyx+VgqXgzXjegYAf/zcTtcMi+dTaexcf3ndpFE7MOj0VL+zEYKuWyp3EDSjKRn5BqjdOlCvaY3GNPKgKee3H+5xLaFit21BO3MFsJ0JYDcdtth1xGGX0Cj09DRqt9nd+wqd3RF2xlkv1Ty7ysBAjSInaE/4aBo5crr5CmDbq0X+pkL6vVi7vZbRc/74j/YEGinzy2+ng11SD7vF19v2n0bn7GbHFPh2r2UV1hpQtykyQbKZejUOnOoFBunEcfpsGaVwaMOdALR97tTU8BMMFrrf7d7Luh4uSF2sVDqTW3xvx9o55DX6Zg7EnCggV9DXGNlLekzNMn9/7UZ5/TRsg8xGiFpGQDNzKYRKJcHwyioKwLbzZe2quY6jVgLt/AqaObS7N5W3Ds1PEGCeTg5cPqfcHDR9993Oi1Q6U1QwEvRvjQDyzKXBoks+xESb4niuPz2idpv1uup2LJ08vWZJ2TLDAtU9u8qMgRpFzmibzuiXWypxP4FGpevJVYq18eg1nS/Ia1mlsyoXdPgJMIIek0I6HdyCKbveaSMFvp/3KdcaUL+jdH4a0E71Av1y+swC5DKAugVSbolEit0GM6cR9qBBtFOj0KlzyNinHetesg2OjEDMbdqWdWTPTSIm6B8azqUu7x8atu2Y8ArWgmSAs14HjMazuW5VJVjXGNp9F+wKfPvR6JLgxMoY4XTLKuqUrOf724/gjY96RvzugeWzAJRnHW5TMpE3Imt3TpnF5PJ338+22ZW/8Jt8xG5KuhPztMT2gFMRjc4S8/d0YlI7v+zuf3ZrtR//0Z7A3wWn49uUTGBw2H0drpdETDCuvsa2I7Wa20exSm8AkdXqpW148p5FaGtKQqBdjMJo/ESZWypxP4GG8ffd+potI4iwFswerVYvbcMb61bi0IY78dSaJQWdO8Z0qiA9fubpdnavV45j4hQ0uU2PCpLZMZmI5z1mnI/G/upY9xJWbNjq+rmMRoEbv9NTzdcGt9cKsn1ma1fNtX1thcujVU773C2zZxB2+93MCKLs3stp28Rhhzk97hWMNzXYH1OjwRUaPTAyvkNODUOjkLcTp0QnVtbrwNNrluCjJ+/AoQ13Yuf6W9H1zVuDLKEaIS6CB5fPyrtGTXLYl1YNtTV5DWXrffLpNUuw989ux8bPL8497ue7BwB//huLRpxzTp/TnPjG7vpwy7wpjt+9Z75yIx5cPit3rIz9YWSldHpNY58VKxET9A0N512XvYKNrELu2u12jhmM+471PrT+rgWu32uD0fHgxtou8LNd5r997O4FeOzu/O0xisn7rZHp9/MYjFqQdsf3sbsX5O2zpmQCiXj+Z4DAV1AAACAASURBVDISC9kRAGuWzUTXN2/FwQ135l0fq719xBE1iqTRNJ3RD7d1MX4CjajUk4uCQs6dQqdTAc4BRqHHJGjPn1OCkf6h4aIyhwLO0wcBuCYZsXrs7gWeCReCbJPX+sSJyUTe+xnTVq3b57SvvZLKlHrU39hGpzpnbvvK6XxIxMU2M5oAqPHIqmi1uasblwZGpv5PxAUixSUvsPI7MmSMMBaaEAjwn3HYa9QnHhPcv2ymbVZTu+u538QMxmewnrfWaYbWa6DXaIu5gyFoZtbth3vyald9ctbEvL+x24d25TrM227dDvM10G8qfWNNaVjZCo1rt9e0RHP5C6frtp/yHwraCKqx3UZ5Eqf7QiGZae1GEoMsIXAaeXZyrDfla0q6W1IuwP66aEz3t1Pt7SMGakQRUGgqca+/j/qCczflnKpgdyH3M50KcG40F3JMgpSmMNhlnOxzaYwEDSjsGhx26zTcbnxhTk81cwqYhoYzI4LCdFbhsRf3OGaHM+9rp6QHxrEuRxIjp+ltXvvK6XywC9IAbbRgQm2Na1ZFK6ei3Y212rQjJ26ZI0XgWiPOjblx7DQdy09HgN8Gndf6nPF1NXhi9SJ0zm72dY74ne43oylZ0DXCbSqqV0kMt8+wuasbz+/ozqtdZZdkJ2ij2C3I8TP92SkgBorLVnisN+WaEMVPwi/jsxn3NzcD6WwuCN/c1e06Khy0/lwh00v7h4ZzU0Ct9+enTdvp9N0wXz/t9pFdmY1kIo6n1iwB4F1CI+g9t1raRwzUiCKg2LVAUaonF0aAVUhjpBhuNcza9CmE1iK6gHujuZBjUmjPn3WUyelmFlbm0EJufOZtLOYcsf7tvde1jehpdmoomPeL2772m4Gz1L2xbgGh2z70ez4YrFkVvTgd5/OptGuZiTfWrbTNHGldz+PFLV3/+rsWOI4wG+vcnM43v+e11/qc8/r+9nuO+A0+jCQiduftYy/uyWvImtezOc2Ki4n3mkqvkSG7Di47YTWKnUb+/Jb58AqKBXBdK+5Ucy0RF9/XMb8lBMxT673uh+VIgHOuP41HNu3E97cfwS+OnLfdHrfg0xzo2XE6tx/ZtBMi3tkrjXur9drY1JAouPMmChioEUVAsVOpopKAJawAq9xTFbwal4YgAUYhxySMnr9yZA4ttmOg0CDH7vx6fkf3iMamn2xkbvs6yIiZtbiuNeFDsez2VZDvmZ9zJ2iDxe34+6kTB+RPl3LLlGc3+vrY3c7712uE2W1fBTmv19+1AI9s2mkbmFizVDoF2ubzpiER8zXVzWkNrTUYP9efxtofaNN9nUY5lSqu4yvIdcluHxbaYVNMJ8naVXMdj5txvf/G5vfyMkkabpk3xTFID5KkKUgNPKeaeXbJPYzXLmUyFgXYJoMxtsftnDCKr9tlTt7c1e06DdRrtN28dtp6bTSmfgeZ3h0lDNSIIqDYqVTlmIrlR1gBVrmnKvgNqoI0EAo5JmGMjJZjdLVSHQN+z69JDj2o5qQNXvvJz7He3NU9onfd3EAu51Rdp++ZnxGEoMfN7fj7Oe/tpnU6JVKyGzH12q9eI4pO+yrIeW2szzIXJrc+3ymg3n64B5vePpp33vSns0hnlWdNwSBZEY3goVTXBLcMqV5Tm8s9a8LgdNyAyyM+L7173PZvX9jxMbbtP+04kmrUHPMa/Q4SSE1MJgKN9K5e6pyR1Y6fUgt+dXtMDQVGZk5+ZNNOPLxpZ1EJeswzReym5aezynUUPuoYqBFFRLFTqcoxFctLWAFWuadylirQDXpMnBqKRha1Uo3kBRXG/iqkN93v+bX+rgVY+4NdeT2oibhg/V0Lcj+HsZ+c1moF6V0vRJDvmduUKLuixH54HX8/572fwuLlnqYb9Lz2WofmFFA71a5LZxS+9tyuXH05u/cOOsXtWG8KT61ZUpJrgtN3yE9wXakED8ZaL7s1yEbWQ6d925/Oot+j08O4bzkVlA+SqArQ1m4GvR8GCQY/t3g6nli9yPd0TDdxkcDnp7L8f1DWWS9u07KDTO+OEgZqRBSasAKsSozYRCHQdVp/4ZVFzes1StF7WMz+KrQ33e/5FWRUp5j95NYBUYrRXyO4DZLN0zolytxIbGpIaEXRC1Ds98UtgZJRXDgMQa9JQT+X2/Pdiqc7MX7n9J2wO2/dMhnOcMjoGFZHlN/X9TuqVMoED9brjt1RKDRQsQu+nILyIEFJb3/adt2leaqfdf/b3T+dgkMjQUkYUyczSuUC8GKKfvtlrAs0i9J6/bCIKvGOdNLZ2am2b99ekfcmotJwShRQSB28ai5QGSa3orJhrTkrt0I/U5jnVxjcUoWHfXy8erz97Ico7b9yndeV/MxOnzFII9bPd8IpG14iLtj4+cUVv27aHQOnwKGU1zW/qf2DCppx0WDsA7dsqOYkPF5lUozXfGD5rBEjvU7bJ8CIjhGv/eR07KyPxwCEVVHRWmLEaS1wlK5xQYjIDqVUp93vOKJGRKEJs+c2CiNcUVDtqYXtFPqZorIW0+CVAS5MbgkI/GbzjFI9oXKNmlfynHGbGmhdo+bE7Tvxjc3v2a61AsJPauOXXUDht/xJqWdNlOKaaXz3nGoeujGCNLdsqG6lE+zWYykAz7x5BJ2zm/MCXqfgy26kyW1Ers1mlof592bFBmnm5DpBEs0A0blHhIGBGhGFigFWuEbTVI5Cpu5ZRen8MrajlFkfDW7ZPI3GsNvaJrfXKHXQ7zY6Xo4GVaXOGbfP2Dm7Oe+8iTmkH3f6Tmzu6nYM0so92m4cX+vU2u7elGsGVmtR51I3qP2s3Uom4qhPxHwVxRZomSAffeE9XxlL7RjfvUK+D25lZYzOF6djA7gnyvHaFr8jdm689k/XNwtbUxale0QYGKgREUVYVEovePGaqupn6l7UPpMf5WoUODWGJiYTvtf7VaKekNd6xNHUoLLj9Bmtj3uNqFi5dXiUc7Tdz7ovN+aizl7vU2zyor7B4RGPJ2KCcfU1ecEiMHJKoR0FbY2XU1IcPyNs5u9e0O+D11o/u2NjHhlz24de22L9fZBppU3JBHauvxWbu7odRyLbQrwmVfsyCgZqREQRVg1TOfwkBwlj6l6pRfmG7hSwi4xMgGA3nXFzVzcuDdg0VEswTdMsStMto8xudLY+EXN8vlswVs7R9iA1wez4OReKTeXv1EnkNfptXAucCmC3NSVdk+J4rV0rtnPKrSbcjKak45TTUoy42l2fEjFtv1lHii8MpLH0T19Fb386V+ewVDXOKlUGIkzOVwEiolFqc1c3VmzYio51L2HFhq3Y3NVd6U1ytXppG95YtxIHN9yJN9atjNwNxq0xbvAqxF3pz2Tc0Lt7U1C4fEOPyrmxemkbnrxnEdqakhBoja0n71nkWMzYur+dSgk01taUdN+PxjWWpTQ4fHllj5Eu3u4cdArGCqmLV4wwjqPXazhdX7723C5f30+nYLLB57k/vr4GiXh+pS8jmHBLkb921VwkE3Hb3wuAe68rviTPA8tnjahBZmxbOb97dtenjV9YjInJxIjnZpV2bivohdqVFjSbr2thXZP83JuijiNqRDSmjIYetqjx0yCI+lq7ahj5sZuO5JRO27pf3eoLlcrmrm7H0YioHPcoCXIOOiV8KKQuXjEKXZ9kfQ03bmUO/Fy7gwYs1ntEbyqNREwc19N5FX+3m95nTJssllstP7/XhrDYXZ8ecVmjaEhnFRpqawpek+ZmNHQUMVAjojGlGhrk1cZPEBb1tXbVekP3u1/LHSgbjV27IC1Kxz1KSlmc26+g03+DFji28nMuuAWDqXQGj724J5T6iwa7e4RTMOF1HFYvbXMMVtyuLUGOg9N6siDXXD9rjAs51/wG8qW6zka9g9APTn0kojGlWhvkUWY3xcfaIHCauheV4NhtClOU+d2vfo5RmJymm8VFInXcoyToORj2lOhCpv/anX8PLp+V+7kpmcib1mb+nd9rgNsUQkAb8XLbxqDnvtO9oLs3ZTtl3us4BD2uYU3D9ntt8Hq/YrbH69gZSnWdLfd1rxQ4okZEY8po6GGLGr+9+1HO8hf1ET83fvZruZPSuCVZiOo5UGmVPgcLnW1Q6u+12xRCg9s2Bj33nbKjCpC7dwSZMh/0uIY568PPsfF6v2K2x7rvS508xOv9o5Ykyg8GakQ0plS6MTRaRTkI82M03NC9lPMYsUMkuEqfg1GebWDsA6e6bF7b6Pfcd8qOCowsPVBosOJ1XN2OQyky03od92LPC7tSFOU8x6v93sRAjYjGlEo3hii6qv2GHiVR6xCJcukFs0qeg1EPrlcvbcPjP9pT0lqATtlRnRQarLgJo2ZiEF7HPezzgtfZYLhGjYjGnKinuyeqdlFak1joGptqK+NRrGpYz7P+rgUl3cago4elCGKdjoNbzcRSvJ+xT6vhvBjNOKJGREREoYtKz3kha2zGYhmPaphtUOptDFJuoFTBitNnLCR7ZDHvZ85c6fZ7Ki1RDgszc08Q+XsAnwNwSim10Ob3DwD4E2jrLC8C+KpSapfXG3d2dqrt27cXtNFEREREfnSse2nE+iJAa7Qc3HCn7d+s2LDVtsHe1pTEG+tWhruBFBnWAN3N02uWlDVYqbZzslqmG0eBiOxQSnXa/c7P1MfvALjN5fcHAfyqUmoRgD8D8K3AW0hERERUAoWUXohyYg0qHeuU3biI7fPampJlDzqqaQpiWCUGyEegppT6KYAel9//XCl1Tv/xTQBXhLRtREREREUppIFbrXX1qHjmNcx/ed/iyARHUVr36cVtujEFE/YatS8DeNnplyLyEICHAGDWrFkhvzURERFRvkLW2EQtayVVRtTWZ0Vl3acXjkiHJ7RATURugRao3eT0HKXUt6BPjezs7PSf/5SIiIioQEEbuFFroFPlVEtwFCWFpPQPuqZtrKyBCyVQE5FrAXwbwO1KqbNhvCYRERFRpbCBTlSYoCPSQbOsjqWsrEXXURORWQBeAPDbSqn3i98kIiIiIiKqJkbtwUc27URdTQyTGhK+1tMFXdM2ltbAeY6oicizAG4G0CIiHwNYDyABAEqp/xvANwFMBvC3omXHGXZKMUlERERERKOLdZSrN5VGMhHHUz7KGARd0zaW1sB5BmpKqfs9fv+7AH43tC0iIiIiIqKqUUhheUPQNW2FrIGrVkVPfSQiIiIiorGrmFGuoCU0qqmmXLHCTs9PRERERERjSDGjXEGzrI6lrKyiVGWy5Hd2dqrt27dX5L2JiIiIiCgc1jVqgDbKFdWi3FEiIjuc8ntwRI2IiIiIiAo2lka5yomBGhERERERFYW1B8PHZCJEREREREQRw0CNiIiIiIgoYjj1kYiIiKiENnd1c+0OEQXGQI2IiIioRKzZ8Lp7U3j0hfcAgMEaEbni1EciIiKiEtm45UBeynIASKUz2LjlQIW2iIiqBQM1IiIiohI5ZlME2O1xIiIDAzUiIiKiEpnRlAz0OBGRgYEaERERUYmsXTUXyUQ877FkIo61q+ZWaIuIqFowmQgRERFRiRgJQ5j1kYiCYqBGREREVEKrl7YxMCOiwDj1kYiIiIiIKGIYqBEREREREUUMAzUiIiIiIqKIYaBGREREREQUMQzUiIiIiIiIIoaBGhERERERUcQwUCMiIiIiIooYBmpEREREREQRI0qpyryxyGkAhyvy5u5aAJyp9EbQmMHzjcqF5xqVC881Kieeb1QupTrXZiulptj9omKBWlSJyHalVGelt4PGBp5vVC4816hceK5ROfF8o3KpxLnGqY9EREREREQRw0CNiIiIiIgoYhiojfStSm8AjSk836hceK5RufBco3Li+UblUvZzjWvUiIiIiIiIIoYjakRERERERBHDQI2IiIiIiChiGKiZiMhtInJARD4UkXWV3h6qPiLy9yJySkR2mx5rFpGfiMgH+v9P0h8XEfnv+vn2roh80vQ3X9Sf/4GIfLESn4WiTURmisg2EdkrIntE5I/0x3m+UahEpF5E3haRXfq59rj+eIeIvKWfU5tEpFZ/vE7/+UP99+2m13pUf/yAiKyqzCeiqBORuIh0iciP9Z95rlFJiMghEXlPRHaKyHb9scjcRxmo6UQkDuBvANwOYD6A+0VkfmW3iqrQdwDcZnlsHYDXlFJzALym/wxo59oc/b+HAPwPQLtAAFgP4AYAywCsNy4SRCbDAL6mlJoPYDmAP9CvWTzfKGyDAFYqpRYDWALgNhFZDuAvADyllLoKwDkAX9af/2UA5/THn9KfB/38/E0AC6BdJ/9Wv/cSWf0RgH2mn3muUSndopRaYqqRFpn7KAO1y5YB+FAp9Uul1BCA7wH49QpvE1UZpdRPAfRYHv51AP+o//sfAaw2Pf5PSvMmgCYRmQ5gFYCfKKV6lFLnAPwEI4M/GuOUUseVUr/Q/30RWqOmDTzfKGT6OXNJ/zGh/6cArATwA/1x67lmnIM/APAZERH98e8ppQaVUgcBfAjt3kuUIyJXALgTwLf1nwU816i8InMfZaB2WRuAo6afP9YfIyrWVKXUcf3fJwBM1f/tdM7xXKRA9Ok+SwG8BZ5vVAL6VLSdAE5Ba4R8BKBXKTWsP8V83uTOKf335wFMBs818udpAP8FQFb/eTJ4rlHpKACvisgOEXlIfywy99GaMF6EiPxRSikRYU0MCo2IjAPwPICHlVIXtM5kDc83CotSKgNgiYg0AfghgHkV3iQahUTkcwBOKaV2iMjNld4eGhNuUkp1i0grgJ+IyH7zLyt9H+WI2mXdAGaafr5Cf4yoWCf1oXHo/39Kf9zpnOO5SL6ISAJakPaMUuoF/WGeb1QySqleANsA3Aht2o/R4Ws+b3LnlP77iQDOgucaeVsB4G4ROQRtCcpKAH8FnmtUIkqpbv3/T0HrhFqGCN1HGahd9g6AOXpmoVpoi1BfrPA20ejwIgAjA9AXAfx/psd/R88itBzAeX2ofQuAW0Vkkr4Y9Vb9MaIcfR3G/wtgn1Lq/zL9iucbhUpEpugjaRCRJIDPQlsTuQ3A5/WnWc814xz8PICtSimlP/6beqa+DmgL8t8uz6egaqCUelQpdYVSqh1aO2yrUuoB8FyjEhCRRhEZb/wb2v1vNyJ0H+XUR51SalhE/hDajo0D+Hul1J4KbxZVGRF5FsDNAFpE5GNoWYA2AHhORL4M4DCA+/Sn/wuAO6Atcu4H8B8AQCnVIyJ/Bq3zAAD+VCllTVBCtALAbwN4T187BABfB883Ct90AP+oZ82LAXhOKfVjEdkL4Hsi8gSALmgdB9D//3+KyIfQkiv9JgAopfaIyHMA9kLLWvoH+pRKIi9/Ap5rFL6pAH6oLxmoAfDPSqlXROQdROQ+KlrHAxEREREREUUFpz4SERERERFFDAM1IiIiIiKiiGGgRkREREREFDEM1IiIiIiIiCKGgRoREREREVHEMFAjIqKqISKX9P9vF5HfCvm1v275+edhvj4REVEQDNSIiKgatQMIFKiJiFft0LxATSn1qYDbREREFBoGakREVI02APgVEdkpIo+ISFxENorIOyLyroj87wAgIjeLyM9E5EVoxW8hIptFZIeI7BGRh/THNgBI6q/3jP6YMXon+mvvFpH3RGSN6bVfF5EfiMh+EXlG9MqpRERExfLqXSQiIoqidQD+s1LqcwCgB1znlVLXi0gdgDdE5FX9uZ8EsFApdVD/+X9TSvWISBLAOyLyvFJqnYj8oVJqic173QNgCYDFAFr0v/mp/rulABYAOAbgDQArAPyv8D8uERGNNRxRIyKi0eBWAL8jIjsBvAVgMoA5+u/eNgVpAPCfRGQXgDcBzDQ9z8lNAJ5VSmWUUicB/BuA602v/bFSKgtgJ7QpmUREREXjiBoREY0GAuA/KqW25D0ocjOAPsvPvwbgRqVUv4i8DqC+iPcdNP07A95XiYgoJBxRIyKianQRwHjTz1sAfFVEEgAgIleLSKPN300EcE4P0uYBWG76Xdr4e4ufAVijr4ObAuDTAN4O5VMQERE5YM8fERFVo3cBZPQpjN8B8FfQph3+Qk/ocRrAapu/ewXA74nIPgAHoE1/NHwLwLsi8gul1AOmx38I4EYAuwAoAP9FKXVCD/SIiIhKQpRSld4GIiIiIiIiMuHURyIiIiIioohhoEZERERERBQxDNSIiIiIiIgihoEaERERERFRxDBQIyIiIiIiihgGakRERERERBHDQI2IiIiIiChiGKgRERERERFFDAM1IiIiIiKiiGGgRkREREREFDEM1IiIiIiIiCKGgRoREREREVHEMFAjIiIiIiKKGAZqREREREREEcNAjYiIIklEXheRcyJSV+ltISIiKjcGakREFDki0g7gVwAoAHeX8X1ryvVeREREbhioERFRFP0OgDcBfAfAF40HRWSmiLwgIqdF5KyI/LXpd18RkX0iclFE9orIJ/XHlYhcZXred0TkCf3fN4vIxyLyJyJyAsA/iMgkEfmx/h7n9H9fYfr7ZhH5BxE5pv9+s/74bhG5y/S8hIicEZGlJdtLREQ0ajFQIyKiKPodAM/o/60SkakiEgfwYwCHAbQDaAPwPQAQkS8AeEz/uwnQRuHO+nyvaQCaAcwG8BC0e+M/6D/PApAC8Nem5/9PAA0AFgBoBfCU/vg/AXjQ9Lw7ABxXSnX53A4iIqIcUUpVehuIiIhyROQmANsATFdKnRGR/QD+DtoI24v648OWv9kC4F+UUn9l83oKwByl1If6z98B8LFS6hsicjOAVwFMUEoNOGzPEgDblFKTRGQ6gG4Ak5VS5yzPmwHgAIA2pdQFEfkBgLeVUv9nwTuDiIjGLI6oERFR1HwRwKtKqTP6z/+sPzYTwGFrkKabCeCjAt/vtDlIE5EGEfk7ETksIhcA/BRAkz6iNxNAjzVIAwCl1DEAbwC4V0SaANwObUSQiIgoMC6aJiKiyBCRJID7AMT1NWMAUAegCcBJALNEpMYmWDsK4EqHl+2HNlXRMA3Ax6afrVNLvgZgLoAblFIn9BG1LgCiv0+ziDQppXpt3usfAfwutPvrvyulup0/LRERkTOOqBERUZSsBpABMB/AEv2/awD8TP/dcQAbRKRRROpFZIX+d98G8J9F5DrRXCUis/Xf7QTwWyISF5HbAPyqxzaMh7YurVdEmgGsN36hlDoO4GUAf6snHUmIyKdNf7sZwCcB/BG0NWtEREQFYaBGRERR8kUA/6CUOqKUOmH8By2Zx/0A7gJwFYAj0EbF1gCAUur7AP4c2jTJi9ACpmb9Nf9I/7teAA/ov3PzNIAkgDPQ1sW9Yvn9bwNIA9gP4BSAh41fKKVSAJ4H0AHghYCfnYiIKIfJRIiIiEIkIt8EcLVS6kHPJxMRETngGjUiIqKQ6FMlvwxt1I2IiKhgnPpIREQUAhH5CrRkIy8rpX5a6e0hIqLqxqmPREREREREEcMRNSIiIiIiooip2Bq1lpYW1d7eXqm3JyIiIiIiqqgdO3acUUpNsftdxQK19vZ2bN++vVJvT0REREREVFEictjpd76mPorIbSJyQEQ+FJF1Ds+5T0T2isgeEfnnQjeWiIiIiIhorPMcUROROIC/AfBZaMVF3xGRF5VSe03PmQPgUQArlFLnRKS1VBtMREREREQ02vkZUVsG4EOl1C+VUkMAvgfg1y3P+QqAv1FKnQMApdSpcDeTiIiIiIho7PATqLVBqwtj+Fh/zOxqAFeLyBsi8qaI3BbWBhIREREREY01YSUTqQEwB8DNAK4A8FMRWaSU6jU/SUQeAvAQAMyaNSuktyYiIiIiIhpd/IyodQOYafr5Cv0xs48BvKiUSiulDgJ4H1rglkcp9S2lVKdSqnPKFNsslERERERERGOen0DtHQBzRKRDRGoB/CaAFy3P2QxtNA0i0gJtKuQvQ9xOIiIiIiKiMcMzUFNKDQP4QwBbAOwD8JxSao+I/KmI3K0/bQuAsyKyF8A2AGuVUmdLtdFERERERESjmSilKvLGnZ2digWviYiIiIhorBKRHUqpTrvfhZVMhIiIiIiIKFI2d3Vj45YDONabwoymJNaumovVS60J7KOJgRoREREREY06m7u68egL7yGVzgAAuntTePSF9wCgKoI1P8lEiIiIiIiIqsbH5/rx+I/25II0QyqdwcYtByq0VcFwRI2IiIiIiKqWUgq/PNOHtw/25P7r7k05Pv+Yy++ihIEaERERERFVjWxWYf+Ji3j74Fm8fUgLzM5cGgIAtIyrww0dzXjo05/A32z7EKcuDo74+xlNyXJvckEYqBERERERUWSlM1ns7j6fGy1751APLgwMAwDampL49JwpWNbRjGUdzehoaYSIAAAmJhN5a9QAIJmIY+2quRX5HEExUCMiIiIiosgYSGew82hvLjDbcfhcLtj6xJRG3HntdCzraMb17c24YlKD4+sYCUOY9ZGIiIiIiCigS4PD2HH4nDaV8WAPdh09j6FMFiLAvGkTsOb6mbnAbMr4ukCvvXppW9UEZlYM1IiIiIhCVM11m4jK4VzfEN7R15a9fagHe45dQCarEI8JFrVNxH9Y0Y5lHc3onN2MiQ2JSm9uxTBQIyIiIgpJtddtIiqFUxcG8JYpI+OBkxcBALU1MSyZ2YQ/uPlKLOuYjKWzmtBYx/DEwD1BREREFIJ0JosnXtrrWLeJgRqNBUopfHwupQdm2lTGQ2f7AQANtXFcN3sS7lo8Hcs6JuPaKyaiPhGv8BZHFwM1IiIiogL19A3h9QOn8Nr+U/jp+6dxUc9EZ9Xdm8Lf/dtHuH3hdMya7Jz8gKjaKKXw0elLeSNmx88PANCyLl7f3owHbpiNZR3NWDBjAmrisQpvcfUQpVRF3rizs1Nt3769Iu9NREREVAilFPYdv4it+09i6/5T6DraC6WAKePrsHJuK36y7yR6+oZG/F0iLkhntDbXghkTcPvCabht4XRc1Tqu3B+BqCiZrMK+whIDkAAAIABJREFU4xfyUuWf1c/5KePrsKyjGTfoqfKvbh2PWEwqvMXRJiI7lFKdtr9joEZERETkLDWUwc8/OoPX9p/Ctv2ncqMF114xESvnteIz86ZiwYwJiMVkxBo1QKvb9OQ9i3Dd7El4ZfcJvLz7OH5xpBcAMKd1HG5fNB23L5yGedPG5+o/EUXF0HAW7+VqmJ3F9kPncHFQGzm+YlLSFJhNRvvkBp7DATFQIyIiIgrg43P92Lb/FLbuP4Wff3QWg8NZNNbG8StzpmDlvFbcPG8KWsfX2/6tn6yPx8+nsGX3Cby8+wTePtQDpYCOlkbctnAabl84DYvaJrLBSxUxkM6g64hew+zQWfzicG+u4+HKKY1Y1jEZN3Q04/qOZrQ1JSu8tdWPgRoRERGRi0xWoevIOby2/xS27juVy0o3e3IDVs5rxcp5rVjW0Yy6mvATH5y+OIhX957AK7tP4OcfnUUmq9DWlMTtC6fh9kXTsHTmJE4fo5K5OJDWa5hpUxl3fdyLdEZBBLhm2oTciNn1Hc1oGReshhl5Y6BGREREZHG+P43X39emM77+/mn09qdRExNc396sBWfXtOITLY1lHdk61zeEn+w7iVd2n8DPPjiNdEZh6oQ63LZAW9O2rKMZcQZtVIQecw2zgz3Yc+w8sgqoiQkWXTExF5hdN7sZE5Njt4ZZuTBQIyIiojFPKYUPT13KjZrtOHIOmaxCc2Mtbp6rTWn8lTlTItM4vTCQxtZ9p/Dy7uN4/cBpDA5n0TKuFp+dr02PvPHKyUgwgx55OHF+AG8fupwq//2TlwAAdTUxLJ3VlJvKuHRWExpqmRC+3BioERER0Zg0kM7gzV+exbb9Wgr9j8+lAADXTJ+Az+ijZouvaIr8KFXf4DBeP3AaL+8+jq37T6F/KIOJyQQ+O38qbl84DTfNaSnJtEyKHrc1kEopHO1J4S09KHv7UA8O6zXMxtXV4LrZk3IjZouumMhzJgIYqBEREdGYceL8ALYdOIXX9p3CGx+eQSqdQX0ihpuuasEt81pxy9xWzKjiJAgD6Qx++v5pvLL7BH6y7yQuDgxjfF0NVl7TitsXTsOvXt2KZC0b4KORXVbRupoY7lo8A0PDWbx9sAcnLmhZSZsaEljW3qwHZpNxzfTxrGEWQQzUiIiIaNTKZhV2fdybGzXbc+wCAKCtKZlLBHLjlZNRnxh9wcvQcBZvfHQGr7x3Aq/uPYFz/WkkE3HcMm8Kbls4HSvntWJcHaezjRbLn3wNJ/TyEFat4+twwycm50bMrpoyjkloqgADNSIiIhpVLg6k8bMPzuC1fafwb++fwplLQ4gJcN3sSbhFr2129dRxYyrF/XAmi7cO9uDl3cexZc9JnL44iNqaGD49ZwpuXzgNvzZ/amTW35G7bFbh6Ll+7D12AXuOXcDe4xew59h5nLwwaPt8AfDLJ+8YU+f7aOEWqLGLhYiIiKrCL09fwla9ttnbB3swnFWYUF+Dm+e24jPXtOLTc6ZgUmNtpTezYmriMay4qgUrrmrB43cvxI7D5/Dy7uN4ZfcJ/Ou+k0jEBZ+6sgW3L5yGWxdMQ/MY3ldRMjScxfsnL2Lv8QvYe0z7b9/xC7mi0vGY4Kop4/CpK1vw2r6TuDAwPOI1ZjQlGaSNQhxRIyIiokgaGs7inUM9eG3fKWw7cAoHz/QBAK6eOi43avbJWU1cd+PBmBr6il5g+0hPP+IxwQ0dzbh94TSsWjANrRPsi3dTuC4MpLEvb5TsAj48dRHpjNYeTybiuGb6eCyYMRHzZ0zAghkTcPXU8blpu3Zr1JKJOJ68Z9GIoupUHTj1kYiIiKrC6YuDeP2ANmr2sw/O4NLgMGprYrjxE5PxmWu0RCAzmxsqvZlVSymFPccu4JXdJ/Avu4/jl6f7IAJ0zp6E2xZOx20Lp6GtihOtRIVSCicuDFyeunjsAvYcP4+jPancc1rG1WL+jIlYMGMC5k+fgPkzJqB9cqNnBlK3rI9UfRioERERUSQZgcNr+05h64FT2HW0FwAwdUKdnghkKlZcNZn1nUpAKYUPTl3Cy++dwMu7j2P/iYsAgMUzm3D7Qq1W2+zJjRXeyujLZBUOnrl0OSDTR8t6+oZyz+loacwFY/NnTMCC6RM4ikkAGKgRERFRhPQNDuOND89g635tSuPJC4MQARZf0ZTL0rhgxgSuuSmzg2f6cmva3v34PABg/vQJWtC2aDquah1X4S2svNRQBvtPXA7G9h67gP0nLmAgnQUA1MZjuHraOCyYfnnq4rzpE5h5kxwxUCMiIqKKOnK2H1v3n8TWA6fx5kdnMZTJYlxdDT59dQtWzpuKm+dOQcu4ukpvJumO9vRjyx5tTduOw+cAAHNax+WCtnnTxo/6QLqnbwh7jp3PGyX75elLyOpN5wn1NdoI2XR9+uKMCbiqdRwSXDNJATBQIyIiorIazmSx4/A5bNVrm3146hIA4BMtjXoikFZ0tjejtoaN2qg7cX5AD9qO4+2DPcgqoH1yA25bOB13LJqGRW0TqzpoU0rhaE9KC8qOX56+aBSOBoAZE+sx35TgY/70CbhiEjMtUvEYqBEREVHJ9fQN4d/eP4Wt+0/j3w6cwoWBYdTEBDd8ohkr503Fynmt6GjhmqdqdubSIF7dcxIv7z6Of//oLIazCm1NSdy2cBruWDQNS2dOinSR5aHhLD44dTFvlGzfsfxU+FdOadSyLhpryqZPGNNlH6i0GKgRERFRwZyyzCmlsP/ExVxts64j55BVWja7m+dqo2Y3zWnB+HoWWR6NevuH8JO9J/HK7hP42QdnMJTJYuqEOqxaMA23L5yOZR3NnhkMS+niQFqrS2YaJfvAJhW+NkqmBWZzp11OhU9UDgzUiIiIqCB2dZtq4zFc396Eg2f6cey8Nj1sYduE3KjZtW0TIz2qQuG7OJDG1v2n8PJ7J/D6+6cwkM5icmMtbl0wFbcvnI4br5xcsrVbSimcvDCIvcfPY0/35fpkR3r6c88xUuHPnz4ht57MTyp8olJjoEZERESBDWeyWPEXW3HywuCI3wmAX5s/FZ+Z14pb5rViKlONk65/aBivHziNl3efwNZ9J9E3lMHEZAK/ds1U3LFoGm6a04K6msJGrbRU+H1568n2HruAs6ZU+O2TG3IFo43AbMr4Oq4no0hioEZERES2MlmF7nMpHDzbh8Nn+3DwTB8OnenDobP9ONrTj+GsfTtBABzccGd5N5aqzkA6g599cAYv7z6Of917EhcGhjGurgYr57XijkXT8KtXt2LLnhO2U2tTQxkcOHkxL/OiXSp8LRjTArNrmAqfqgwDNSIiojEsk1U41pvCobNaEHbwTL8WlJ3tw9Ge/tyaHUBbtzN7cgM6WhrR3tKIZ986gt5UesRrtjUl8ca6leX8GFTlhoaz+PlHZ/DK7hPYsucEzvWnkYgJMkrB3B8QF0HLuFqcvjSYe3x8fY2ebfFy5sUrp4xj1lCqem6BGrsciIiIRoFsVuH4hQE9EDNGxbSRsSNn+zGUyeaeW5+IoX1yI65uHY/Pzp+KjslaUNbR0ohWyxSxuVPHj1ijlkzEsXbV3LJ+Pqp+tTUx3Dy3FTfPbcUTqxfi7YM9+Mo/bUffUCbveRmlcD6Vxh+unJObushU+DQWMVAjIiKqEtmswsmLA3og1o9DpqmKh3v6MTR8ORirq4lh9uQGfKKlEZ+Z14r2lsbcSNnU8fW+k32sXtoGALZT04gKVROP4VNXtaDfEqQZBoez+OPPXl3mrSKKFgZqREREEWJksMtNU9T//9CZfhzu6cutzwG0NTqzJjegfXIjbp47RRsVm9yI2S2NmD7BfzDmZfXSNgZmVBIzmpLo7k3ZPk401jFQIyIiKjOlFE5fHMTBM304fLY/F4wZP5unGSbiglnNWjB205yWXDDW3tKA6ROTTC9OVW3tqrmcWkvkgIEaERFRCSilcObSUP70xLP9ejDWl7cupyamB2MtjfjUlS1ob9ECs46WRsxoYjBGoxen1hI5Y6BGRERUIKUUevqMYKw/N1Xx8FltquKlweHcc+MxwcxJSbS3NGJZR3Muq2L75Aa0NSVRU6JiwERRx6m1RPYYqBER0Zizuavbdw++Ugq9/WnTWrE+HDzbn8uqeHEgPxi7YlISsyc34rpZk7RATJ+q2DYpiQSDMSIi8omBGhERjSmbu7rz1sR096bw6AvvoX9oGNdMn5CbnnjItG7sgikYiwnQNimJ9smN+I1ZbZg9uREd+lTFKyY1sK4TERGFgoEaERGNOoPDGVwaGEbfYAYXB9PoG8zg0mAalwYzeOzFPXmJCwAglc7g6z/cnftZBJgxMYmOlkbcvWRGbr3Y7MmNmNmcRF1NvNwfiYiIxhgGakREFAlDw1n0DQ7jkvW/geH8xwfyf983OIyLA8PoGxrOBWfm4s5B/D+/04mOlgZcMakB9QkGY1Sgd58DXvtT4PzHwMQrgM98E7j2vkpvFRFVGQZqRESjQJA1V2EazmRzo1Z5QZNpBOuSHkRdHLgcWJmDrb7BYVwcHM4r1uymoTaOcXU12n/1NWisrcHM5obcY411NRhff/nf5ueOq4vjwW+/jRMXBka8bltTEp+dPzXsXURjhVLA8CDQ9Qzw6teBYf0cO38U+NF/0v7NYI2IAmCgRkRU5ZzWXAGwDdYyWXU5YMoFVi4jWDaBlfFvc/FlN/WJGMbVJTC+vgaNdVqgNaOp/nIwVV+DcbVGMGUKwupqMN70nMbamqJT1a+7fR7rNtFlw4PA4CVg8AIwdAkYvJj/X+4xu+fo/x7Sf84O279HOgW8sg64ciXQ2FLez0dEVUuUUhV5487OTrV9+/aKvDcR0Wiy/L+9ZjtCVF8Tw5JZTfro1uWAy7o+y0ldTSxv1GpcfX7QlDeCZfO4EXw11sUjl3q+UiOQFJJM2iGYcnnMGnAZj2WG/L1n7Tigbvzl/zf/l3tsnDbl0c2Ua4D2my7/x8CNaEwTkR1KqU7b3zFQIyKqDtmswqGzfdhz7AL2Hr+AvccuYM+xCzhzadDxb65vn6QHTQmM00eyrNMBjUDLCMjG648xlTzlhLHmKjOsjzyZAqgh68iUKYga8Zjp5+GRHRO2Eo1a8OQZYDn8bDxWOw6I+fw+PLVQm+5o1dgKLP8qcOh/AUfeBNJ92uPmwG32CmDcFH/vQ0SjQtGBmojcBuCvAMQBfFsptcHy+y8B2AigW3/or5VS33Z7TQZqRETOBtIZvH/yYi4Y23v8AvYdv4D+IW00LBEXzGkdj/kzJuAne0/gfGrklKu2piTeWLey3JtOo827z2lrrNKpy4/Fa4HF9wOt8+0DLrvHhlPO72FWk7QEVxMuj1blPTbOEmBNyH9O7TggXoEVHnb7K5EE7vrvl4PbTBo4vgs49DMtcDv87wzciMaoogI1EYkDeB/AZwF8DOAdAPcrpfaanvMlAJ1KqT/0u1EM1IiINL39Q9irB2N7jmkjZR+evoRMVrs+j6+rwTUzJmD+9AlYMGMC5s+YgDmt43P1uqxr1ABtzdWT9yzidD4qTDYDnNytjfz86/r8oMNOvM4SOI13CK7Mj1meYzwvnijPZyyloCOQroHbPFPgdhMDN6JRxi1Q89PVtAzAh0qpX+ov9j0Avw5gr+tfVambb755xGP33Xcffv/3fx/9/f244447Rvz+S1/6Er70pS/hzJkz+PznPz/i91/96lf///buPE7Oqs73+Of0knRn7+xLJ2QlISQhgWzsAVT2RREQxUFEWZ1Rr+MMznUcr+OM3HHmXsY7JAEEcVBBZFdBVAigkgRCEiBAgOzpDlnJnvR+7h9PdXpN0iHdXd3Vn/frlVc/dc7TVb9KVzr1rfOcc7jqqqtYv349n//85xv0f+Mb3+Diiy/m3Xff5cYbb2zQ/+1vf5uPfexjLF26lK997WsN+v/1X/+VU045hZdffpl/+Id/aNB/xx13MHnyZP74xz/y/e9/v0H/XXfdxdixY/n1r3/Nf/zHfzTof+CBBxg6dCi//OUvmTNnToP+Rx55hL59+3L//fdz//33N+h/+umn6dKlC7Nnz+bhhx9u0P/CCy8A8O///u/85je/qdOXn5/PM888A8A///M/89xzz9Xp79OnD48++igA3/rWt5g/f36d/sLCQn72s58B8LWvfY2lS5fW6T/22GO5++67Abjhhht477336vRPnjyZO+64A4BrrrmGoqKiOv0nn3wyP/jBDwC4/PLL2bZtW53+c845h3/8x38E4Pzzz2f//rpvdi666CL+9m//FvC111Fee6WpJei79hvKhM98k7c37OKNh/6N8g+TCxI65WTRtVMOY46bwLe+dzvHD+7Jt/7mBopfLGIz8ELq/mu/9h74/t+QvXYD2z/cT2lFJZ1zsjnlvI9z2ZTzAF97vvaa8HsvViWjXiW7mDwA7jizFMp2c81j+ynaVXfBmJMLs/nBx/KAwOVLT2fb9l3JxnMpDX/vrarz/clr76+ATH7tXckfN/euee09ORuYDTTxtTd4IL+864fMuec+KFkFJUsh/icAj9w8ib4TzuL+pWXc/7vFDYJtu3vt1eL/uf7eg5Z/7bUnTQlqQ4DaF1sXATMaOe/yEMIZJKNvX48xNrhAO4RwA3ADwLBhw468WklqJ6piZPnGZHRs4eptrPxgF/tKK6moSt70dirfSdeteznpmAKyhvZiT+fddOmUfWBe2ORjCjh/4iCgznvgg+rbrTN9u3U+cPu4QT2a/0kpc+zdCuvmQ9Gr8MGaZN5X9RU2vfvBpKth2Mnw9oOw+NlkZcT6ehYml0A25QWqI5OdC31GJX/HPQuTn03ZHijZCT0Gw+sPwasfwvpyyO0CeT1r/ijzrXgu+bdbUQo5naFgOHR1pDUTNeXSx08D58UYv5S6/XlgRu3LHEMIfYA9McbSEMKNwFUxxkNOjPDSR0mZYm9pBe98kLp0sTj5+u6m3Qf2BcvLzWLcwOSSxerLF8cN7EF+JzdUViuIET5clVzGuG5+8nXb+0lfdmcYchIMm5kEs6HTIL+g7vc3Zc6VWlf9SyXXLUiCHHipZKZr7N9jTh6c9wOYeGXy4Ul2rh+g1NbGN6A/2jlqJwPfjTGem7r9LYAY4w8Ocn428GGM8ZAf6xjUJLVHm3eXHJhHVr3y4pptew8MRhR0yeX4wT0PzCU7fnAPhvfp2uaWp1cGq6yAjW/UDWZ7Nyd9eb2SQFYdzAZPTj6RP5w2/kanw6usqBfc5tcEt75j624H0K1/emtV0+3fDttWwYcrYdvK5OvbTyRB/XCycpPAlp2bCm+pAJeVW3Ncuz27fvuRnNsJsnLqnVN93Eh7ViN1tVSwbAcfNB1tUMshuZzxHJJVHV8FPhtjfKvWOYNijB+kjj8J/H2Mceah7tegJqktq14Kv/YCH/WXwh/Wu0udBT7GD+7BwB55BD/JVGsq3ZNcBlUdzIoW1SxE0WtYrWB2CvQ9tunLzKv9Mri1H/t3JAHsw9U1Yaz66/7ttU4M0HMo7Fx38Pv6+D8n+wJWlidfq8prjmu3V5bXO27iuVVNCIgfVUsFywVzoHRnw8frORS+vqzlns8RaI7l+S8A7iBZnv++GOO/hBC+ByyKMT4VQvgBcAlQAXwI3BxjXH6o+zSoSWorDrcUfk5WYMyA7kkgSwWz4wb3oEdeBqxOp/Zn9yZYvyAJZmtfho1vQqwEAgycUBPMhs6Enq76KQxu6Vayq1YAW103jO2ruyAKPQqhz0joPSqZp1j9tdcxkJt38H36WiN4xHiYYFcv/FUfVx2kvbHzqyqaFiwPem5Z8no/bLAM8N0dLfv31URueC1JKbWXwq8OZrWXwu/WOYfxg2pGyMYP6sGYAd3onON8MqVBjLBtRc0ljOvmJ/PNIJmXUjgtNVo2Mzl2MQk1hcGt+ZXuTv5tHhgVW5Xc/nAl7N1S99weQ6D3yORP7TBWMDy5LO9Q2sGlfG1GjHDHhOSS7foyaUStJRjUJLWkGCPFO/bXGSV7e8MuinfU/Oc2sEdenQU+xg/uwdCCLmRleemi0qSiLDW/rFYwq/7EPb93Mlp2zMnJ14GTIKdTeutVZjhkcDu27uIk3Qekt9Z0Kt0D21fXC2Mrk0C2Z1Pdc7sPSgWwVCA7EMZGQKcuR1eHc0abrh0EW4OapIxWXlnFyi17kpGyWsFs5/7ksocQYGTfrhw/uOeBBT6OG9SjznL27Z7/cbdPJbtS88tSwaxoEVSk3lAUjKi78EffMa7kptZRWQEbX09CW/UG3GW7k75MD25l+1IjYbUX8UiNlO3ZWPfcbgNqhbFRtUbIRkKnrumpXw218f8fDWqS2p0nlhTzw2ffZcOO/Qzulc83zx3LZVOGsLe0guUb6y7wUXsp/M45WYwbVHeUbNzA7nTp1JRtI9updvCJoVJ2fVB3tGzTsmSz6ZCVjJAdCGYzofvAdFcrJTItuJXvrzdXbFVNGNu9oe65XfvVmi9WK4j1Hgmdu6enfmUUg5qUZgcLHaorxkhlVeTxJcX845PLKCmvOtCXHaCgaye27S1rsBR+9SjZ+EE9GNG3gyyFHyPs+zB5c/GLK2H/hw3P6dIHrrgfuvRN3mx06Q1ZzrVrNVVVsPW9usFsx9qkL7dLan5ZKpgVTvVNn9qP9hDcyktg+5q6C3dUL+axq96cpS59680XqzVCltcjLeWr4zCoSWn0xJJivvXYm+wvrzzQlp+bzQ8+NbHFw1p18CmvjJRXVVFeUZUcV1ZRVllFRa3jA32NnFfe4NzkODk3dXzg3EjFgdsxdV9VlFfVOq6suf/q4+rHOdSvpLzcLG6ZNToZLRvSQZbCrw5jBz71rfWmo6SRJYcPKSSbGXftB137JkHuwHFf6Jq63aVv0pbfO9kDR01TUQobltYEs/ULapbX7tqv5hLGYTOT0bNsVw1VhjhUcOszpu7iJM05UlxRmgpjq+qFsVWpBSRq/YeS37vh4h3VI2P5vZqvJukIGdSkNDr19uc5adcf+LuchxkctrIh9uXfKq7kxc6z+Ouzx9QNPvUCTHXwKa+ooqKqXvA5SNipOILgczSyswK52YHcrCxyc7KS4+wsOmVnkZM6rr6dm5PczsnKolNOTV/yp/a5yfF//OG9Rh8zAKtvv7BlnlA6Ve+jU3ti+sH20ek1tO7E9N6j4Ddfhd0bG95vt4Fw+T2wd2uyIMXeLanjrbA3dXvf1iQM0tgLpTrYVY/I9al1bLBj/46688uKX4OKkqSvz+hawezk5GeW6R8qSNWONLgdag5RRVkyEl39O7H278edRcmlw9XyetX8Xqw/QpZf0Pp/D1ITGNSkNCkpr+Tv/unb3J77Y7qEsgPt+2Inbiv/Ek9VnXagrSWDT25OFjlZgU45jZ+Xmx1S95uVut/a54ZUDXW/J7sFV0Y89fbn66zOWG1Ir3z+ctvZLfa4LarOPjqr6r7pqLOPTkjeqNSZC1Fr6eacRhZAOdo5alWVSVjbtzUJcnu3pILd1powd6TBrjq8HTjulwS7A8ftMNjtLKq5hHHdAtj0FhAhKwcGnVB3/7Ju/dJdrdR2HCq4dR2Q/F6JNVedkJWTXEJZvg92rKsbxjr3rLfPWK3jLr1b93lJzcCgJrWiqqrIgtXbeGJJMc+8uZFn4i0UZm1tcN4OutPpMz8lu8cAcnoMJLtLb8jqAHOrmiCdl4seldLd9S5RrHWp4r56r4HqfXTqf/pbMCLZ1PRIteaqVocKdvtSt/duqzk+ZLDrdfARujqBr1/zBrvD/X1VVcGWd2rNL1tQs8lsp24188uOORmGnOQKb9KRqKxItqFY82eY9y81I9G1ZeXCcRc3vFSxSx9Hp5VRDGpSK1ixeTePLS7myaUbKN6xn66dsrlqXC7/+N6naNJ/KVk5yZvSbv2TP137J5/KdxtQc9y1f3I7vyDjQ12bXYCldE+9uWK1Vg7bu7nuud0HpwJYvUsVC4Yf/T467UlVZXIJZ51LL7ceJNilLtNsNNhRM8euOtjVHqGrP+euS5/Gg11jI5A5+XDKXycjkevmw/qFNXMAuw2ouYRx2EwYMKF9jQRKbdl3e3HQD3K+u6O1q5FanUFNaiFb95Ty1NINPL6kmDeLd5KdFThjdG++XLieGdueIPu9Z+pezlFbt4Fw+Y+TN/d7tiSbZVYf790Me1J/qsobfm/ITt6QNhrkage9/skoRIaHumZ3YB+dWgt3fLj6IPvoDEwFsBF1w1jvEY6yfFQHgl3tSy+3HiTkNSHY1b/0ctmjULrr4I/fd2zdhT8KhvsJvtRS/u+EmtHq2noOha8va/16pFZ2qKDmR4LSEdpfVskf3tnE44uLeOn9rVRWRSYM6cH3PzGYy8ILdHvzAXh5ZRKQTr4lCVLz/qXh/KFP/DOMOP3QDxYjlOw4SJDbVHO8eXly+3ChrrEg1zUV9jpaqGuwj06tSxV3f1D33K79kwA2+mOp0bFaSzd37pae+jNZVnbNpY+MO/z5Bwt2dRZQ2QZb34e98w8R0gJ8c2US6CS1jnO+0/gc23O+k76apDbCoCY1QfW8s8cXF/PMso3sKa1gUM88bjh9BJ8dvImhK38Gf3kcKkth6Aw48+9h/KU1c426Dfho84dCanGG/ALod+yhz60d6uoHuerRub2bYcu7hwl1fQ8e5Lr2SwW9Ae0j1JWXwPbVjSxtvwp2Fdc9t2u/JHiNPKvupYoFI9xHp6070mD3f49PLd1dT89CQ5rU2qr/L2ytObZSO+Klj9IhvL9pN48tKebJJcVs2FlCt845nD9hIJdP7Mn0Xc+R9dpPYNObyeICk66CqV+EgRPSXfbhNTXUVbdVljW8j+pQV2eUrlaQqz7u2j+ZK3Skoa6pi2NU76NTfw+dxvbR6dKn3rLNtb7m9Tyy+tR+He0qmZIkNRPnqElHYMvuUp56fQOPLyliWfGuZN7ZmL588sRCPtFnK3lLf5K80SvbAwMmwrQf0lz2AAAgAElEQVQvwsQroHP3dJfeMmJMFlU4EN421z3eu6Vu0DtsqOvXMMjVvhyzS+9kDlGDxR7yYNqXoMfgupcq7lxP3U1NC+rNFau1mIebmqpaa66SKUnSQRjUpMPYX1bJ79/eyONLivlTat7ZxCE9+eSUIVx8fG/6rfsdLLo3WQkuuzNM+BRMvR4Kp7rIQG2NhboDQa76uFbQazTUpUbeau+bU19ez7ph7MBeOiPdR0eSJLUbLiYiNaKqKrJg1TYeW1LM71Lzzgb3zOPGM0byySlDGJOzGV67F+7+Oez/MAkEn/gXmPxZw8DBhNS+WPm9mjinbmfjQe5P/36wB4C/W5WMmhmQJUlSBjOoqcN5b1P1fmfFfJCad3bBxIF8ckohM47pQdb7v4Nnb4NV85JL9sZdCNOuh+FntP3FM9qT2qGu75i6fW/88iDLNRcakiVJUodgUFOHsHl3yYH9zt7akMw7O/PYfvzDBcfxseMGkF+yCV67G574abI0e48hcNb/hCmfhx6D0l1+x+NyzZIkqYMzqCljVc87e2xxMX9eUTPv7DsXjeeSyYPp2yU3GTV77Jvw7jPJnKjR58CF/wFjzoVs/3mkjcs1S5KkDs53osooldXzzhYX87tlH7C3rJIhvfK58YyRfOrEIYzu3x32boOlc2HRT5I9trr0gVP+Gk76AvQeke6noGqTrjSYSZKkDsugpozw7sbdPLakiCeXbGDjrhK6d87hwkmDknlnI3qTFUhWbHz0Xnj7iWS1wWGnJJc3jr8Ecjqn+ylIkiRJBxjU1G5Vzzt7bHExb39QM+/sf154HB8fP4C83Gwo2QWLfpyMnm1+Czr3SEbOTroOBoxP91OQJEmSGmVQU7uyr6yC37+1iceWFPPn97dQFWFSYU/+6eLxXHzCYPp2S42MffAGLLoP3vxVsjH1wElw8X/ChE9D527pfRKSJEnSYRjU1OZVzzt7dHERzy7beGDe2c2zRvHJKYWM7p8KXuX7YemDycbURa9CTh5MuDzZmHrIie67JUmSpHbDoKY2a/nGXTy+uJgnl9bMO7to0mA+eeIQpg/vTVZWKnhtW5mMni39OezfDn3GwLk/gMlXJxsjS5IkSe2MQU1tyuZdJTz1es28s5zUvLNvX5Tsd5aXm52cWFkObz8Nr94Lq1+ErBwYd1FqY+rTHT2TJElSu2ZQU9o1Nu/shMKefDc176xPt1orMu4sgtd+Cov/G/ZshJ5D4exvJxtTdx+YvichSZIkNSODmtKisioyf+U2HltSxO+WbWRfat7ZLbNGc9mUITXzzgCqqmDl88ncs/d+BzHCmI/D1P9MvmZlp++JSJIkSS3AoKZWVT3v7ImlxWzaVUr3vBwuOWEwn5wyhGm1550B7N0KSx5IltbfsRa69oNTvwYnXQsFw9P2HCRJkqSWZlBTi9u8q4Qnl27gsSXFvJOadzZrbD++c1Eh5xzXv2beGSSjZevmJ4uDvP1ksjH1MafBx/4Jxl0MOZ3S90QkSZKkVmJQU4vYV1bBs29t5LHFxfxlxdZk3tnQXvyvS47nokmD6s47AyjZCW88nAS0zW9D554w9YvJxtT9x6XnSUiSJElpYlDTR/LEkmJ++Oy7bNixn8G98vnmuWO5+ITBvLxyK48vLuZ3byXzzgoL8rn1rGTe2ah+jWw0vWFpamPqR6B8LwyeApf8v2T/s05dW/+JSZIkSW1AiDGm5YGnTp0aFy1alJbH1tF5Ykkx33rsTfaXVx5oy8kKdOmUxa6SSrrn5XDRpEF8ckohU48pqDvvDJKNqZc9liwOUvwa5OTDxE8nI2hDTmzlZyNJkiSlRwjhtRjj1Mb6HFHTEfvhs+/WCWkAFVWRsorI7M+dyNnj6s07q7b1/ZqNqUt2Qt+xcN7/hhM+A/m9Wql6SZIkqe0zqOmIbdixv9H20ooqLpg4qG5jZTks/00S0Fa/BFm5cNzFycbUx5zqxtSSJElSIwxqOmKDe+VT3EhYG9wrv+bGjvXw2v3J8vp7NkHPYXDOd5KNqbv1b71iJUmSpHbIoKYj9s1zx/LCI//F32Y/zOCwlQ2xL3fwGU77xI3w3u+T0bP3n02W2j/2XJh6PYw+x42pJUmSpCYyqOmIzdz7HJ/I+TFdQhkAhWEr/xbmkvWHX8D+D6FrfzjtfyQbU/caluZqJUmSpPbHoKYj1unF7x8IadWyYgWU7YUr7oexF7oxtSRJknQUstJdgNqXou376FW2qfHOyjI4/pOGNEmSJOkoGdR0RO55aRUf0Lfxzp6FrVuMJEmSlKEMamqyLbtLeejV9bw09OZkmf3acvOTVR0lSZIkHTWDmprsvr+spryyipkXfwlyu0JOZyBAz6Fw8Y9g0pXpLlGSJEnKCC4moibZub+cB+av5YKJgxix5Tko3QFX/xLGnpfu0iRJkqSM44iamuSB+WvYU1rBLbNGw4I50HskjPlEusuSJEmSMpJBTYe1r6yCe/+8mrPH9Wd81ftQ9CrMuAmyfPlIkiRJLcF32jqsB19Zz/Z95dx61ihYOAc694DJn013WZIkSVLGMqjpkEorKrnnpVXMGNGbkwpK4a3HYco10Ll7ukuTJEmSMpZBTYf0+OJiNu4q4dazRsOrP4aqSph+Q7rLkiRJkjJak4JaCOG8EMK7IYQVIYTbDnHe5SGEGEKY2nwlKl0qKquY8+JKJg7pyekjusFrP4GxF0DvEekuTZIkScpohw1qIYRs4E7gfGA8cHUIYXwj53UHvgosbO4ilR5PL9vI2m37uPWsUYQ3H4F922DmTekuS5IkScp4TRlRmw6siDGuijGWAQ8BlzZy3j8D/xsoacb6lCYxRmbPW8Ho/t34xHEDYOFcGDABhp+e7tIkSZKkjNeUoDYEWF/rdlGq7YAQwonA0Bjjbw91RyGEG0IIi0IIi7Zs2XLExar1PL98M8s37uaWWaPIWvdn2LQMZtwIIaS7NEmSJCnjHfViIiGELOD/AN843LkxxrtjjFNjjFP79et3tA+tFhJj5L/mraCwIJ+LTxgMC+ZClz4w8Yp0lyZJkiR1CE0JasXA0Fq3C1Nt1boDE4AXQghrgJnAUy4o0n4tWPUhS9bt4MYzR5G7cy28+zScdB3k5qe7NEmSJKlDaEpQexUYE0IYEULoBHwGeKq6M8a4M8bYN8Y4PMY4HFgAXBJjXNQiFavFzX5hBX27deaKkwrhlXsgKxumfSndZUmSJEkdxmGDWoyxAvgK8CzwDvBwjPGtEML3QgiXtHSBal2vr9/Bn97fypdPH0Fe1T5Y8gCMvwx6DEp3aZIkSVKHkdOUk2KMTwNP12v7zkHOnXX0ZSld7py3gh55OXxu5jGw9F4o3QUzb0l3WZIkSVKHctSLiShzvLdpN79/exNfOHUE3XKzYOFdUDgNCk9Kd2mSJElSh2JQ0wFzXlhJl07ZXHfKcFjxB/hwJcxwg2tJkiSptRnUBMC6bft46vUNfHb6MAq6doIFs6H7YBjf2N7mkiRJklqSQU0A3PXSSrJD4MtnjITN78CqF2D6lyA7N92lSZIkSR2OQU1s3lXCrxYV8emphQzokQcL50JOXrJ3miRJkqRWZ1ATP/7zaiqqqrjpjFGw70N4/Zcw6Uro0jvdpUmSJEkdkkGtg9uxr4yfLVjLJScMZlifLvDa/VCxH2bcnO7SJEmSpA7LoNbB3f/yGvaVVXLzrNFQWQ6v/hhGnAkDxqe7NEmSJKnDMqh1YHtKK/jJX9bw8fEDGDuwO7zza9hVDDMdTZMkSZLSyaDWgf1i4Vp27i/nllmjkoaFc6FgBIw5N72FSZIkSR2cQa2DKimv5J4/rebU0X2YMqwAil+D9QuTDa6zfFlIkiRJ6eQ78g7qkdeK2LK7lFtnjU4aFsyFTt1h8mfTW5gkSZIkg1pHVFFZxdwXVzJ5aC9OHtUHdm+Etx6HKddAXo90lydJkiR1eAa1DujXb2ygaPt+vnLWaEII8Oq9UFUBM25Id2mSJEmSMKh1OFVVkdnzVjJuYHfOHtcfyktg0X0w9nzoPTLd5UmSJEnCoNbh/OGdTby/eQ83zxpFVlaAZY/Avq3JIiKSJEmS2gSDWgcSY2T2vBUc06cLF04cBDEmi4j0Hw8jzkh3eZIkSZJSDGodyF9WbOP1op3cdOYocrKzYO1fYNObyWhaCOkuT5IkSVKKQa0DuXPeCgb06MynThySNCyYA/m9YdKV6S1MkiRJUh0GtQ7itbXbmb9qG18+fSSdc7Jh+xpY/luYeh3k5qe7PEmSJEm1GNQ6iNnzVlDQJZerpw9LGl65B7KyYdqX0luYJEmSpAYMah3A2xt28dzyzVx36gi6ds6B0t2w+AEYfyn0GJzu8iRJkiTVY1DrAOa8uJJunXO49uThScPSB6F0J8y8Ja11SZIkSWqcQS3Drd66l9++sYFrZh5Dzy65UFUFC+fCkKlQODXd5UmSJElqhEEtw9314kpys7O4/rQRScOKP8KHK2HmzektTJIkSdJBGdQy2Ac79/Po4iKumjaUft07J40L50D3Qcn8NEmSJEltkkEtg93z0mpihBvOGJk0bF4OK59PVnrMzk1vcZIkSZIOyqCWobbtKeXBV9Zx6eQhFBZ0SRoXzoWcPDjpuvQWJ0mSJOmQDGoZ6v6X11BSUcnNs1Kjafs+hNcfgolXQNc+6S1OkiRJ0iEZ1DLQrpJy7n95DecdP5DR/bsnjYv/Gyr2u4iIJEmS1A4Y1DLQzxasZXdJBbfMGp00VFbAK/fAiDNgwPHpLU6SJEnSYRnUMsz+skru/dNqzji2HxMLeyaNy38Nu4pghqNpkiRJUntgUMswDy9az7a9Zdw6a1RN44K5UDAcjj03bXVJkiRJajqDWgYpq6jirhdXMm14ATNGphYMKV4M6xfA9BshKzu9BUqSJElqEoNaBnlyaTEbdpZwy1mjaxoXzoVO3WHKNekrTJIkSdIRMahliMqqyJwXVzJ+UA9mHdsvady9EZY9BlM+B3k90lugJEmSpCYzqGWIZ9/ayKote7n1rNGEEJLGRfdBVQVMvyG9xUmSJEk6Iga1DBBj5M55KxjZtyvnTRiYNJaXJEHt2HOhz6hD34EkSZKkNsWglgFefG8Lb23YxU2zRpGdlRpNW/Yo7N3iBteSJElSO2RQywCz561kcM88Lps8JGmIERbOgf7jYcSZ6S1OkiRJ0hEzqLVzr6z+kFfWfMgNZ4ykU07qx7n2Zdj4Jsy4Earnq0mSJElqNwxq7dyd81bQp2snrpo2rKZx4RzIL4CJV6avMEmSJEkfmUGtHVtWvJMX39vCF08bQX6n1GbW29fC8t/CSddBpy7pLVCSJEnSR2JQa8dmv7CC7nk5fP7kY2oaX7kbCDDtS2mrS5IkSdLRMai1Uys27+GZZRu59uTh9MjLTRpL98DiB2D8pdBzSHoLlCRJkvSRGdTaqbkvrqRzThbXnTq8pvH1B6F0p0vyS5IkSe2cQa0dKtq+jyeWFHP19GH06dY5aayqgoVzYchJUDgtvQVKkiRJOioGtXbonpdWEQJ8+fSRNY0rn4NtK2DGzS7JL0mSJLVzTQpqIYTzQgjvhhBWhBBua6T/phDCmyGEpSGEP4cQxjd/qQLYsruUh15dz6emFDK4V35Nx4I50G1gMj9NkiRJUrt22KAWQsgG7gTOB8YDVzcSxH4RY5wYY5wM/Bvwf5q9UgFw319WU15ZxU2zRtU0bnk3GVGb9iXI6ZS+4iRJkiQ1i6aMqE0HVsQYV8UYy4CHgDrDNjHGXbVudgVi85Woajv3lfPA/LVcMHEQI/p2relYOBeyO8PU69JXnCRJkqRmk9OEc4YA62vdLgJm1D8phHAr8D+ATsDZjd1RCOEG4AaAYcOGHWmtHd5/z1/DntIKbpk1uqZx/3Z4/SGYdAV07Zu22iRJkiQ1n2ZbTCTGeGeMcRTw98C3D3LO3THGqTHGqf369Wuuh+4Q9pVVcN9fVnP2uP6MH9yjpmPxf0P5vmQREUmSJEkZoSlBrRgYWut2YartYB4CLjuaotTQg6+sZ/u+cm49q9ZoWmUFvHIPDD8dBk5IX3GSJEmSmlVTgtqrwJgQwogQQifgM8BTtU8IIYypdfNC4P3mK1GlFZXc89IqZo7szUnHFNR0LP8N7FzvBteSJElShjnsHLUYY0UI4SvAs0A2cF+M8a0QwveARTHGp4CvhBA+BpQD24FrW7LojubxxcVs3FXCD6+YVLdj4VzodQwce156CpMkSZLUIpqymAgxxqeBp+u1fafW8VebuS6lVFRWMefFlUwq7Mlpo2stFrJhKaybD+f+K2Rlp69ASZIkSc2u2RYTUct4etlG1m7bxy2zRhNCqOlYOBc6dYMp16SvOEmSJEktwqDWhsUYmT1vBaP7d+MT4wfUdOzeBG8+ApM/B3k901egJEmSpBZhUGvDnl++meUbd3PLrFFkZdUaTVt0H1RVwIwb01ecJEmSpBZjUGujYoz817wVFBbkc/EJg2s6Kkph0b0w5hPQZ1T6CpQkSZLUYgxqbdT8VdtYsm4HN545itzsWj+mZY/B3i0w86b0FSdJkiSpRRnU2qjZ81bSr3tnrjipsKYxRlgwG/qNg5Fnpa84SZIkSS3KoNYGLV2/gz+v2MqXTx9BXm6tpffXzYeNb8CMm6D2CpCSJEmSMopBrQ2aPW8FPfNz+eyMY+p2LJgD+QUw6ar0FCZJkiSpVRjU2pj3Nu3m929v4gunDKdb51r7ke9YB8t/AydeC526pK9ASZIkSS3OoNbGzHlhJV06ZfOFU4bX7XjlbiDA9C+noyxJkiRJrcig1oas27aPp17fwOdmDKOga6eajrK9sPi/Yfwl0LPw4HcgSZIkKSMY1NqQu15aSXYIfOn0kXU7Xn8QSnbCjJvTU5gkSZKkVmVQayM27yrhV4uK+PTUQgb0yKvpqKqChXfB4CkwdHr6CpQkSZLUagxqbcSP/7yaiqoqbjpjVN2Olc/D1vdg5i0uyS9JkiR1EAa1NmD73jJ+tmAtl5wwmGF96q3ouHAOdBsI4y9LT3GSJEmSWp1BrQ24/+U17Cur5OZZo+t2bHkPVvwRpl0POZ0a/2ZJkiRJGceglmZ7Siu4/+U1fGL8AMYO7F6385W7ILsTnHRdeoqTJEmSlBYGtTT7xcK17Nxfzi1n1RtN278dlv4CJl4J3fqlpzhJkiRJaWFQS6OS8kru+dNqThvdl8lDe9XtXPwAlO+DmTelpzhJkiRJaWNQS6NHXitiy+5Sbjmr3kqPlRXwyj1wzGkwcGJ6ipMkSZKUNga1NKmorGLuiyuZMqwXJ4/sU7fz3d/CznWOpkmSJEkdlEEtTX79xgaKtu/n1lmjCfX3R1swF3oNg7EXpKc4SZIkSWllUEuDqqrI7HkrGTewO2eP61+384PXYd3LMP1GyMpOT4GSJEmS0sqglgZ/eGcT72/ew82zRpGV1choWm5XmHJNeoqTJEmSlHYGtVYWY2T2vBUc06cLF04cVLdzz2ZY9ghM/izk92r8DiRJkiRlPINaK/vziq28XrSTm88cRU52vb/+RfdBZRnMcBERSZIkqSMzqLWyO+etYGCPPD554pC6HRWl8Oq9MOYT0Hd0498sSZIkqUMwqLWi19Z+yIJVH/LlM0bSOafeQiFvPQ57NzuaJkmSJMmg1ppmz1tJQZdcrp4+tG5HjLBgNvQdC6POTk9xkiRJktoMg1oreXvDLp5bvpkvnjqCLp1y6nauW5Asyz/zJqi/p5okSZKkDseg1krmvLiSbp1z+KuThzfsXDgH8nrBpM+0el2SJEmS2h6DWitYvXUvv31jA9fMPIaeXXLrdu5YB+/8Gk66Fjp1SU+BkiRJktoUg1oruOvFleRmZ3H9aSMadr5yDxBg2pdbvS5JkiRJbZNBrYV9sHM/jy4u4qppQ+nXvXPdzrK9sPincNzF0Gto43cgSZIkqcMxqLWwe15aTYxwwxkjG3a+/hCU7ISZN7d+YZIkSZLaLINaC9q2p5RfvLKWy6YMobCg3vyzqipYOBcGTYahM9JToCRJkqQ2yaDWgn7ylzWUVlRx05mjGnaueh62vpeMprkkvyRJkqRaDGotZFdJOT+dv4bzJwxkdP9uDU9YMBe6DYDjP9nqtUmSJElq2wxqLeRnC9ayu6SCW2aNbti59X1Y8QeYej3kdG7YL0mSJKlDM6i1gP1lldz7p9WceWw/Jgzp2fCEhXdBdieYel3rFydJkiSpzTOotYCHF61n294ybj2rkdG0/Ttg6S9gwqehW//WL06SJElSm2dQa2ZlFVXc9eJKpg0vYPqI3g1PWPIAlO+FmTe1fnGSJEmS2gWDWjN7cmkxG3aWcEtjo2lVlfDK3XDMqTDohNYvTpIkSVK7YFBrRpVVkTkvrmT8oB7MOrZfwxPefRp2rIMZjqZJkiRJOjiDWjN69q2NrNqyl1vPGk1obG+0BXOg5zAYd2HrFydJkiSp3TCoNZMYI3fOW8HIfl05b8LAhid88Aas/QvMuAGyslu/QEmSJEnthkGtmbzw3hbe2rCLm88cRXZWI6NpC+dCbleY8vnWL06SJElSu2JQayaz561gSK98LpsypGHnni3w5q9g8tWQ36v1i5MkSZLUrhjUmsErqz/k1TXbueGMkeRmN/JXuug+qCxzERFJkiRJTdKkoBZCOC+E8G4IYUUI4bZG+v9HCOHtEMIbIYTnQgjHNH+pbded81bQt1snrpo2tGFnRRksuhdGfxz6jmn94iRJkiS1O4cNaiGEbOBO4HxgPHB1CGF8vdOWAFNjjJOAR4B/a+5C26plxTt58b0tfPG0EeTlNrJIyFuPw55NbnAtSZIkqcmaMqI2HVgRY1wVYywDHgIurX1CjHFejHFf6uYCoLB5y2y7Zr+wgu55OVwzs5FBxBhh4RzoeyyMOqf1i5MkSZLULjUlqA0B1te6XZRqO5jrgWca6wgh3BBCWBRCWLRly5amV9lGrdi8h2eWbeTak4fTIy+34QnrF8KGJTDjRmhsXzVJkiRJakSzLiYSQrgGmAr8sLH+GOPdMcapMcap/fr1a86HTou5L66kc04W1506vPETFsyBvJ5wwtWtWpckSZKk9q0pQa0YqL1KRmGqrY4QwseA/wlcEmMsbZ7y2q6i7ft4YkkxV08fRp9unRuesLMI3vk1nHgtdOra+gVKkiRJareaEtReBcaEEEaEEDoBnwGeqn1CCGEKcBdJSNvc/GW2Pfe8tIoQ4IYzRjZ+wiv3ABGmf7lV65IkSZLU/h02qMUYK4CvAM8C7wAPxxjfCiF8L4RwSeq0HwLdgF+FEJaGEJ46yN1lhC27S3no1fVcfmIhg3rmNzyhbC+8dj+Muwh6DWv1+iRJkiS1bzlNOSnG+DTwdL2279Q6/lgz19Wm3fvn1ZRXVnHjmaMaP+GNX0LJDph5S+sWJkmSJCkjNOtiIh3Bzn3l/GzBWi6cNJgRfRuZexYjLLwLBp0Aw2a2foGSJEmS2j2D2hH67/lr2FNawS2zDjKatmoebFkOM252SX5JkiRJH4lB7QjsK6vgvr+s5pxx/TluUI/GT1owB7r2hwmfat3iJEmSJGUMg9oRePCV9WzfV84tZ41u/IStK+D938O06yGnkSX7JUmSJKkJDGpNVFpRyT0vrWLmyN6cdExB4ye9chdkd4KpX2zd4iRJkiRlFINaEz2+uJiNu0q49WCjaft3wJKfw4TLoVv/1i1OkiRJUkYxqDVBRWUVc15cyaTCnpw2um/jJy35GZTvhRk3tW5xkiRJkjKOQa0Jnl62kbXb9nHrWaMJja3kWFWZXPY47BQYPLn1C5QkSZKUUQxqh1FVFZk9bwVj+nfj48cNaPykd5+BHetgpqNpkiRJko6eQe0wnl++meUbd3PLWaPIyjrIvmgL50LPoTD2wtYtTpIkSVJGMqgdQoyR/5q3gsKCfC6eNLjxkz54A9b8CaZ/GbJzWrdASZIkSRnJoHYI81dtY+n6Hdx05ihysg/yV7XwLsjtAif+VesWJ0mSJCljGdQOYfa8lfTr3plPn1TY+Al7t8Kbv4ITrob8g+ytJkmSJElHyKB2EEvX7+DPK7by5dNHkJeb3fhJi34ClaUuyS9JkiSpWRnUDmL2vBX0zM/lszOOafyEijJ49R4YdQ70O7Z1i5MkSZKU0QxqjXhv025+//YmvnDKcLp1PsgCIW8/AXs2wcxbWrc4SZIkSRnPoNaIOS+spEunbL5wyvDGT4gRFsyBPmNg1NmtWpskSZKkzOd68vWs27aPp17fwPWnjaCga6fGTyp6FTYshgv+HbLMupIkSdJHUV5eTlFRESUlJekupUXl5eVRWFhIbm5uk7/HoJbyxJJifvjsuxTv2A9AYa+8g5+8YDZ07pms9ihJkiTpIykqKqJ79+4MHz6cEEK6y2kRMUa2bdtGUVERI0aMaPL3ORxEEtK+9dibB0IawA+eeZcnlhQ3PHlnEbz9FJz0V9C5WytWKUmSJGWWkpIS+vTpk7EhDSCEQJ8+fY541NCgBvzw2XfZX15Zp21/eSU/fPbdhie/+mMgwvQbWqc4SZIkKYNlckir9lGeo0EN2FBrJO2Q7WX74LX7YdyF0GtYyxcmSZIkqUMyqAGDe+U3rf2NX8L+7TDj5laoSpIkSVJtTywp5tTbn2fEbb/l1Nufb3yq0hHYsWMHs2fPPuLvu+CCC9ixY8dRPfbhGNSAb547lvzc7Dpt+bnZfPPcsTUNMcLCu2DgJDjmlFauUJIkSerYaq8rEYHiHfv51mNvHlVYO1hQq6ioOOT3Pf300/Tq1esjP25TuOojcNmUIUAyV23Djv0M7pXPN88de6AdgFUvwJZ34LI50AGuo5UkSZJa0//69Vu8vWHXQfuXrNtBWWVVnbb95ZX83SNv8OAr6xr9nvGDe/BPFx9/0Pu87bbbWLlyJZMnTyY3N5e8vDwKCgpYvnw57733HtgKuAYAAAulSURBVJdddhnr16+npKSEr371q9xwQ7JOxfDhw1m0aBF79uzh/PPP57TTTuPll19myJAhPPnkk+TnN37F3pEwqKVcNmVI3WBW38K50LUfTLi89YqSJEmSBNAgpB2uvSluv/12li1bxtKlS3nhhRe48MILWbZs2YFl9O+77z569+7N/v37mTZtGpdffjl9+vSpcx/vv/8+Dz74IPfccw9XXnkljz76KNdcc81HrqmaQa0ptq2E934HZ/495HROdzWSJElSxjnUyBfAqbc/X2c7rWpDeuXzyxtPbpYapk+fXmevsx/96Ec8/vjjAKxfv57333+/QVAbMWIEkydPBuCkk05izZo1zVKLc9SaYuFdkJULU69PdyWSJElSh9SkdSWOUteuXQ8cv/DCC/zxj39k/vz5vP7660yZMqXRvdA6d64ZyMnOzj7s/LamckTtcEp2wtKfJ5c8dh+Q7mokSZKkDqlJ60ocoe7du7N79+5G+3bu3ElBQQFdunRh+fLlLFiw4CM/zkdhUDucJT+Hsj0w86Z0VyJJkiR1aIddV+II9enTh1NPPZUJEyaQn5/PgAE1AzPnnXcec+fO5bjjjmPs2LHMnDmz2R63KUKMsVUfsNrUqVPjokWL0vLYTVZVCT+aAt0HwfXPprsaSZIkKaO88847HHfccekuo1U09lxDCK/FGKc2dr5z1A7lvd/BjrUw0w2uJUmSJLUeg9qhLJgDPYfCuIvSXYkkSZKkDsSgdjAbl8GaP8G0L0G2U/kkSZIktR6D2sEsnAM5+XDiX6W7EkmSJEkdjEGtMXu3whu/gslXQ5fe6a5GkiRJUgdjUGvMaz+BylKY4ZL8kiRJklqfQa2+ijJ49V4YdTb0a75dziVJkiQdpTcehv87Ab7bK/n6xsOt+vDdunVrtcdylYxqbzwMz30Pdq5Pbk+4Ir31SJIkSarxxsPw67+B8v3J7Z3rk9sAk65MX10txKAGDX/oAIvugUETM/KHLkmSJLU5z9wGG988eH/Rq8n0pNrK98OTX4HXftr49wycCOffftC7vO222xg6dCi33norAN/97nfJyclh3rx5bN++nfLycr7//e9z6aWXHumzOWpe+gjJSFrtkAbJ7ee+l556JEmSJNVVP6Qdrr0JrrrqKh5+uObyyYcffphrr72Wxx9/nMWLFzNv3jy+8Y1vEGP8yI/xUTmiBrCz6MjaJUmSJDWvQ4x8AcmctOppSrX1HArX/fYjPeSUKVPYvHkzGzZsYMuWLRQUFDBw4EC+/vWv89JLL5GVlUVxcTGbNm1i4MCBH+kxPiqDGkDPwoP80AtbvxZJkiRJDZ3znYbTlXLzk/ajcMUVV/DII4+wceNGrrrqKn7+85+zZcsWXnvtNXJzcxk+fDglJSVHWfyR89JHSH64ufl125rhhy5JkiSpmUy6Ei7+UTKCRki+Xvyjo15T4qqrruKhhx7ikUce4YorrmDnzp3079+f3Nxc5s2bx9q1a5un/iPkiBrU/HCf+15yuWPPwiSkuZCIJEmS1HZMurLZ36Mff/zx7N69myFDhjBo0CA+97nPcfHFFzNx4kSmTp3KuHHjmvXxmsqgVq0FfuiSJEmS2r4336xZbbJv377Mnz+/0fP27NnTWiV56aMkSZIktTUGNUmSJElqY5oU1EII54UQ3g0hrAgh3NZI/xkhhMUhhIoQwqebv0xJkiRJmSgde5S1to/yHA8b1EII2cCdwPnAeODqEML4eqetA74A/OKIK5AkSZLUIeXl5bFt27aMDmsxRrZt20ZeXt4RfV9TFhOZDqyIMa4CCCE8BFwKvF3rwdek+qqO6NElSZIkdViFhYUUFRWxZcuWdJfSovLy8igsPLI9mpsS1IYAtXeDLgJmHNGjSJIkSVI9ubm5jBgxIt1ltEmtuphICOGGEMKiEMKiTE/NkiRJkvRRNSWoFQNDa90uTLUdsRjj3THGqTHGqf369fsodyFJkiRJGa8pQe1VYEwIYUQIoRPwGeCpli1LkiRJkjqu0JQVVkIIFwB3ANnAfTHGfwkhfA9YFGN8KoQwDXgcKABKgI0xxuMPc59bgLVH+wRaQF9ga7qLUMby9aWW5mtMLcnXl1qSry+1pLb6+jomxtjopYZNCmodSQhhUYxxarrrUGby9aWW5mtMLcnXl1qSry+1pPb4+mrVxUQkSZIkSYdnUJMkSZKkNsag1tDd6S5AGc3Xl1qarzG1JF9fakm+vtSS2t3ryzlqkiRJktTGOKImSZIkSW2MQU2SJEmS2hiDWi0hhPNCCO+GEFaEEG5Ldz3KHCGEoSGEeSGEt0MIb4UQvprumpR5QgjZIYQlIYTfpLsWZZYQQq8QwiMhhOUhhHdCCCenuyZljhDC11P/Ny4LITwYQshLd01q30II94UQNocQltVq6x1C+EMI4f3U14J01tgUBrWUEEI2cCdwPjAeuDqEMD69VSmDVADfiDGOB2YCt/r6Ugv4KvBOuotQRvpP4HcxxnHACfg6UzMJIQwB/gaYGmOcAGQDn0lvVcoA9wPn1Wu7DXguxjgGeC51u00zqNWYDqyIMa6KMZYBDwGXprkmZYgY4wcxxsWp490kb3KGpLcqZZIQQiFwIfDjdNeizBJC6AmcAdwLEGMsizHuSG9VyjA5QH4IIQfoAmxIcz1q52KMLwEf1mu+FPhp6vinwGWtWtRHYFCrMQRYX+t2Eb6RVgsIIQwHpgAL01uJMswdwN8BVekuRBlnBLAF+Enq0tofhxC6prsoZYYYYzHw78A64ANgZ4zx9+mtShlqQIzxg9TxRmBAOotpCoOa1IpCCN2AR4GvxRh3pbseZYYQwkXA5hjja+muRRkpBzgRmBNjnALspR1cMqT2ITVP6FKSDwQGA11DCNektyplupjsT9bm9ygzqNUoBobWul2YapOaRQghlySk/TzG+Fi661FGORW4JISwhuSy7bNDCD9Lb0nKIEVAUYyx+iqAR0iCm9QcPgasjjFuiTGWA48Bp6S5JmWmTSGEQQCpr5vTXM9hGdRqvAqMCSGMCCF0IpnI+lSaa1KGCCEEkvkd78QY/0+661FmiTF+K8ZYGGMcTvK76/kYo59Iq1nEGDcC60MIY1NN5wBvp7EkZZZ1wMwQQpfU/5Xn4GI1ahlPAdemjq8FnkxjLU2Sk+4C2ooYY0UI4SvAsyQrDt0XY3wrzWUpc5wKfB54M4SwNNX2DzHGp9NYkyQ11V8DP099kLkKuC7N9ShDxBgXhhAeARaTrJC8BLg7vVWpvQshPAjMAvqGEIqAfwJuBx4OIVwPrAWuTF+FTROSSzQlSZIkSW2Flz5KkiRJUhtjUJMkSZKkNsagJkmSJEltjEFNkiRJktoYg5okSZIktTEGNUlSuxdCqAwhLK3157ZmvO/hIYRlzXV/kiQ1hfuoSZIywf4Y4+R0FyFJUnNxRE2SlLFCCGtCCP8WQngzhPBKCGF0qn14COH5EMIbIYTnQgjDUu0DQgiPhxBeT/05JXVX2SGEe0IIb4UQfh9CyE/bk5IkdQgGNUlSJsivd+njVbX6dsYYJwL/BdyRavt/wE9jjJOAnwM/SrX/CHgxxngCcCLwVqp9DHBnjPF4YAdweQs/H0lSBxdijOmuQZKkoxJC2BNj7NZI+xrg7BjjqhBCLrAxxtgnhLAVGBRjLE+1fxBj7BtC2AIUxhhLa93HcOAPMcYxqdt/D+TGGL/f8s9MktRROaImScp08SDHR6K01nElzvGWJLUwg5okKdNdVevr/NTxy8BnUsefA/6UOn4OuBkghJAdQujZWkVKklSbnwhKkjJBfghhaa3bv4sxVi/RXxBCeINkVOzqVNtfAz8JIXwT2AJcl2r/KnB3COF6kpGzm4EPWrx6SZLqcY6aJCljpeaoTY0xbk13LZIkHQkvfZQkSZKkNsYRNUmSJElqYxxRkyRJkqQ2xqAmSZIkSW2MQU2SJEmS2hiDmiRJkiS1MQY1SZIkSWpj/j//Hqgi1JCHwwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2baNUI65cug",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer network\n",
        "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
        "\n",
        "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
        "\n",
        "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch/layer normalization; we will add those features soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88hubznI5cuh",
        "colab_type": "text"
      },
      "source": [
        "## Initial loss and gradient check\n",
        "\n",
        "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
        "\n",
        "For gradient checking, you should expect to see errors around 1e-7 or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs_b3pid5cuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "77297dfc-b426-40a3-89fa-8b81eb1b371d"
      },
      "source": [
        "np.random.seed(231)\n",
        "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
        "X = np.random.randn(N, D)\n",
        "y = np.random.randint(C, size=(N,))\n",
        "\n",
        "for reg in [0, 3.14]:\n",
        "  print('Running check with reg = ', reg)\n",
        "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
        "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
        "\n",
        "  loss, grads = model.loss(X, y)\n",
        "  print('Initial loss: ', loss)\n",
        "  \n",
        "  # Most of the errors should be on the order of e-7 or smaller.   \n",
        "  # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
        "  # for the check when reg = 0.0\n",
        "  for name in sorted(grads):\n",
        "    f = lambda _: model.loss(X, y)[0]\n",
        "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
        "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running check with reg =  0\n",
            "Initial loss:  0.0\n",
            "Running check with reg =  3.14\n",
            "Initial loss:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qL5Uxx-5cuj",
        "colab_type": "text"
      },
      "source": [
        "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. In the following cell, tweak the **learning rate** and **weight initialization scale** to overfit and achieve 100% training accuracy within 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "4LisWwTD5cuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "2afbca8c-4c22-4688-9e43-378993b93e12"
      },
      "source": [
        "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
        "# tweaking just the learning rate and initialization scale.\n",
        "\n",
        "num_train = 50\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "weight_scale = 1e-2   # Experiment with this!\n",
        "learning_rate = 1e-4  # Experiment with this!\n",
        "model = FullyConnectedNet([100, 100],\n",
        "              weight_scale=weight_scale, dtype=np.float64)\n",
        "solver = Solver(model, small_data,\n",
        "                print_every=10, num_epochs=20, batch_size=25,\n",
        "                update_rule='sgd',\n",
        "                optim_config={\n",
        "                  'learning_rate': learning_rate,\n",
        "                }\n",
        "         )\n",
        "solver.train()\n",
        "\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.title('Training loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 40) loss: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c9de0bdc7280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 }\n\u001b[1;32m     22\u001b[0m          )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/cs231n/assignments/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlast_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 train_acc = self.check_accuracy(\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 )\n\u001b[1;32m    288\u001b[0m                 val_acc = self.check_accuracy(\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/cs231n/assignments/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(self, X, y, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \"\"\"\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhgagdh95cum",
        "colab_type": "text"
      },
      "source": [
        "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again, you will have to adjust the learning rate and weight initialization scale, but you should be able to achieve 100% training accuracy within 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-CMz6Nl5cun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
        "# tweaking just the learning rate and initialization scale.\n",
        "\n",
        "num_train = 50\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "learning_rate = 2e-3  # Experiment with this!\n",
        "weight_scale = 1e-5   # Experiment with this!\n",
        "model = FullyConnectedNet([100, 100, 100, 100],\n",
        "                weight_scale=weight_scale, dtype=np.float64)\n",
        "solver = Solver(model, small_data,\n",
        "                print_every=10, num_epochs=20, batch_size=25,\n",
        "                update_rule='sgd',\n",
        "                optim_config={\n",
        "                  'learning_rate': learning_rate,\n",
        "                }\n",
        "         )\n",
        "solver.train()\n",
        "\n",
        "plt.plot(solver.loss_history, 'o')\n",
        "plt.title('Training loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "5pIASHEy5cup",
        "colab_type": "text"
      },
      "source": [
        "## Inline Question 2: \n",
        "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?\n",
        "\n",
        "## Answer:\n",
        "[FILL THIS IN]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhv7rV2R5cuq",
        "colab_type": "text"
      },
      "source": [
        "# Update rules\n",
        "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYk49Wg05cuq",
        "colab_type": "text"
      },
      "source": [
        "# SGD+Momentum\n",
        "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n",
        "\n",
        "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than e-8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L8x7fo65cuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cs231n.optim import sgd_momentum\n",
        "\n",
        "N, D = 4, 5\n",
        "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
        "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
        "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
        "\n",
        "config = {'learning_rate': 1e-3, 'velocity': v}\n",
        "next_w, _ = sgd_momentum(w, dw, config=config)\n",
        "\n",
        "expected_next_w = np.asarray([\n",
        "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
        "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
        "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
        "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
        "expected_velocity = np.asarray([\n",
        "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
        "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
        "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
        "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
        "\n",
        "# Should see relative errors around e-8 or less\n",
        "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
        "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OzbUdgW5cus",
        "colab_type": "text"
      },
      "source": [
        "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "iBpWmjp45cut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = 4000\n",
        "small_data = {\n",
        "  'X_train': data['X_train'][:num_train],\n",
        "  'y_train': data['y_train'][:num_train],\n",
        "  'X_val': data['X_val'],\n",
        "  'y_val': data['y_val'],\n",
        "}\n",
        "\n",
        "solvers = {}\n",
        "\n",
        "for update_rule in ['sgd', 'sgd_momentum']:\n",
        "  print('running with ', update_rule)\n",
        "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
        "\n",
        "  solver = Solver(model, small_data,\n",
        "                  num_epochs=5, batch_size=100,\n",
        "                  update_rule=update_rule,\n",
        "                  optim_config={\n",
        "                    'learning_rate': 5e-3,\n",
        "                  },\n",
        "                  verbose=True)\n",
        "  solvers[update_rule] = solver\n",
        "  solver.train()\n",
        "  print()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.title('Training accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.title('Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "for update_rule, solver in solvers.items():\n",
        "  plt.subplot(3, 1, 1)\n",
        "  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n",
        "  \n",
        "  plt.subplot(3, 1, 2)\n",
        "  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n",
        "\n",
        "  plt.subplot(3, 1, 3)\n",
        "  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n",
        "  \n",
        "for i in [1, 2, 3]:\n",
        "  plt.subplot(3, 1, i)\n",
        "  plt.legend(loc='upper center', ncol=4)\n",
        "plt.gcf().set_size_inches(15, 15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPgc5JhR5cuv",
        "colab_type": "text"
      },
      "source": [
        "# RMSProp and Adam\n",
        "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
        "\n",
        "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
        "\n",
        "**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes. \n",
        "\n",
        "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
        "\n",
        "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzzk9vUN5cuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "acc12a03-4346-4f51-9249-1d23e18f859c"
      },
      "source": [
        "# Test RMSProp implementation\n",
        "from cs231n.optim import rmsprop\n",
        "\n",
        "N, D = 4, 5\n",
        "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
        "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
        "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
        "\n",
        "config = {'learning_rate': 1e-2, 'cache': cache}\n",
        "next_w, _ = rmsprop(w, dw, config=config)\n",
        "\n",
        "expected_next_w = np.asarray([\n",
        "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
        "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
        "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
        "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
        "expected_cache = np.asarray([\n",
        "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
        "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
        "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
        "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
        "\n",
        "# You should see relative errors around e-7 or less\n",
        "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
        "print('cache error: ', rel_error(expected_cache, config['cache']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-012380bb03e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test RMSProp implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcs231n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrmsprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs231n'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFkfuFr-5cux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Adam implementation\n",
        "from cs231n.optim import adam\n",
        "\n",
        "N, D = 4, 5\n",
        "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
        "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
        "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
        "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
        "\n",
        "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
        "next_w, _ = adam(w, dw, config=config)\n",
        "\n",
        "expected_next_w = np.asarray([\n",
        "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
        "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
        "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
        "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
        "expected_v = np.asarray([\n",
        "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
        "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
        "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
        "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
        "expected_m = np.asarray([\n",
        "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
        "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
        "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
        "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
        "\n",
        "# You should see relative errors around e-7 or less\n",
        "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
        "print('v error: ', rel_error(expected_v, config['v']))\n",
        "print('m error: ', rel_error(expected_m, config['m']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqhXJVhJ5cuz",
        "colab_type": "text"
      },
      "source": [
        "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm9JjJxq5cuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
        "for update_rule in ['adam', 'rmsprop']:\n",
        "  print('running with ', update_rule)\n",
        "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
        "\n",
        "  solver = Solver(model, small_data,\n",
        "                  num_epochs=5, batch_size=100,\n",
        "                  update_rule=update_rule,\n",
        "                  optim_config={\n",
        "                    'learning_rate': learning_rates[update_rule]\n",
        "                  },\n",
        "                  verbose=True)\n",
        "  solvers[update_rule] = solver\n",
        "  solver.train()\n",
        "  print()\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Iteration')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.title('Training accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.title('Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "for update_rule, solver in list(solvers.items()):\n",
        "  plt.subplot(3, 1, 1)\n",
        "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
        "  \n",
        "  plt.subplot(3, 1, 2)\n",
        "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
        "\n",
        "  plt.subplot(3, 1, 3)\n",
        "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
        "  \n",
        "for i in [1, 2, 3]:\n",
        "  plt.subplot(3, 1, i)\n",
        "  plt.legend(loc='upper center', ncol=4)\n",
        "plt.gcf().set_size_inches(15, 15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "trixdiTr5cu1",
        "colab_type": "text"
      },
      "source": [
        "## Inline Question 3:\n",
        "\n",
        "AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n",
        "\n",
        "```\n",
        "cache += dw**2\n",
        "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
        "```\n",
        "\n",
        "John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n",
        "\n",
        "\n",
        "## Answer: \n",
        "[FILL THIS IN]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwZ-QXRd5cu1",
        "colab_type": "text"
      },
      "source": [
        "# Train a good model!\n",
        "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
        "\n",
        "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
        "\n",
        "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Rbk15f9u5cu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model = None\n",
        "################################################################################\n",
        "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
        "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
        "# the best_model variable.                                                     #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "pass\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_lVl2b5cu5",
        "colab_type": "text"
      },
      "source": [
        "# Test your model!\n",
        "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG6NluuI5cu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
        "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
        "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
        "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
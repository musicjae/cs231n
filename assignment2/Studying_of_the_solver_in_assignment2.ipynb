{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Studying of the solver in assignment2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcL56ZhKighk0L9n6p823s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musicjae/cs231n/blob/master/assignment2/Studying_of_the_solver_in_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo6ISb3rtGY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
        "FOLDERNAME = 'Colab Notebooks/cs231n/assignments/assignment2'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# this downloads the CIFAR-10 dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB6yP706uV4e",
        "colab_type": "text"
      },
      "source": [
        " &nbsp;&nbsp;&nbsp;&nbsp;solver는 학습 분류 모델에 필요한 모든 논리를 압축한다. 이것은 다양한 업데이트 규칙을 사용하여 sgd 를 수행한다. solver는 학습, 검증 데이터와 label을 수용하는데 그 때문에, 과적합이 나타나는지 감독하기 위해 학습과 검증 데이터의 분류 정확성을 주기적으로 확인할 수 있다. <br><br>\n",
        " &nbsp;&nbsp;&nbsp;&nbsp;모델을 학습시킨 뒤, 당신은 먼저 solver 인스턴스를 만들 것이고, 모델, 데이터셋, 다양한 옵션(lr, batch size 등)을 구성자constructor로 넘긴다. 당신은 train()를 최적화 절차를 돌리는 메서드, 모델을 학습시키는 메서드라고 부를 것이다.\n",
        " <br><br>&nbsp;&nbsp;&nbsp;&nbsp;train() 메서드를 반환한 뒤, model.params 는 학습 과정 동안 검증 집합에서 가장 잘 수행하는 매개변수를 포함할 것이다. 게다가, 인스턴스 변수 solver.loss_history는 학습 시간 동안 마주한 모든 손실 리스트를 포함하고, 인스턴스 변수 solver.train_acc_history 와 solver.val_acc_history 는 각 epoch 마다 학습셋과 검증셋에 모델의 정확성에 대한 리스트를 열거할 것이다.\n",
        "<br><br>\n",
        "    Example usage might look something like this:\n",
        "\n",
        "    **data** = {\n",
        "      'X_train': # training data\n",
        "      'y_train': # training labels\n",
        "      'X_val': # validation data\n",
        "      'y_val': # validation labels\n",
        "    }\n",
        "    model = MyAwesomeModel(hidden_size=100, reg=10)\n",
        "    solver = Solver(model, data,\n",
        "                    update_rule='sgd',\n",
        "                    optim_config={\n",
        "                      'learning_rate': 1e-3,\n",
        "                    },\n",
        "                    lr_decay=0.95,\n",
        "                    num_epochs=10, batch_size=100,\n",
        "                    print_every=100)\n",
        "    solver.train()\n",
        "\n",
        "<Br>Solver는 다음의 API에 따르는 모델 객체에서 잘 작동한다:\n",
        "<br><br>\n",
        "&nbsp;&nbsp;**model.params** must be a dictionary mapping string parameter names to numpy\n",
        "      arrays containing parameter values.\n",
        "      \n",
        "&nbsp;&nbsp;**model.loss(X, y)** must be a function that computes training-time loss and gradients, and test-time classification scores, with the following inputs and outputs:\n",
        "\n",
        "      \n",
        "\n",
        "&nbsp;&nbsp;**Inputs:**<br>\n",
        "      - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)\n",
        "      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\n",
        "        label for X[i].\n",
        "<Br><br>\n",
        "&nbsp;&nbsp;**Returns:**<br>\n",
        " If y is None, run a test-time forward pass and return:<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**scores:** Array of shape (N, C) giving classification scores for X where\n",
        " scores[i, c] gives the score of class c for X[i].<Br><br>\n",
        "If y is not None, run a training time forward and backward pass and\n",
        "return a tuple of:<br><Br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**loss:** Scalar giving the loss<br><br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**grads:** Dictionary with the same keys as self.params mapping parameter names to gradients of the loss with respect to those parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_hBzu6OtD51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from future import standard_library\n",
        "\n",
        "standard_library.install_aliases()\n",
        "from builtins import range\n",
        "from builtins import object\n",
        "import os\n",
        "import pickle as pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from cs231n import optim\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    \n",
        "    def __init__(self, model, data, **kwargs):\n",
        "        \"\"\"\n",
        "        Construct a new Solver instance.\n",
        "\n",
        "        Required arguments:\n",
        "        - model: A model object conforming to the API described above\n",
        "        - data: A dictionary of training and validation data containing:\n",
        "          'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "          'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "          'y_train': Array, shape (N_train,) of labels for training images\n",
        "          'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "        Optional arguments:\n",
        "        - update_rule: A string giving the name of an update rule in optim.py.\n",
        "          Default is 'sgd'.\n",
        "        - optim_config: A dictionary containing hyperparameters that will be\n",
        "          passed to the chosen update rule. Each update rule requires different\n",
        "          hyperparameters (see optim.py) but all update rules require a\n",
        "          'learning_rate' parameter so that should always be present.\n",
        "        - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "          learning rate is multiplied by this value.\n",
        "        - batch_size: Size of minibatches used to compute loss and gradient\n",
        "          during training.\n",
        "        - num_epochs: The number of epochs to run for during training.\n",
        "        - print_every: Integer; training losses will be printed every\n",
        "          print_every iterations.\n",
        "        - verbose: Boolean; if set to false then no output will be printed\n",
        "          during training.\n",
        "        - num_train_samples: Number of training samples used to check training\n",
        "          accuracy; default is 1000; set to None to use entire training set.\n",
        "        - num_val_samples: Number of validation samples to use to check val\n",
        "          accuracy; default is None, which uses the entire validation set.\n",
        "        - checkpoint_name: If not None, then save model checkpoints here every\n",
        "          epoch.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.X_train = data[\"X_train\"]\n",
        "        self.y_train = data[\"y_train\"]\n",
        "        self.X_val = data[\"X_val\"]\n",
        "        self.y_val = data[\"y_val\"]\n",
        "\n",
        "        # Unpack keyword arguments\n",
        "        self.update_rule = kwargs.pop(\"update_rule\", \"sgd\")\n",
        "        self.optim_config = kwargs.pop(\"optim_config\", {})\n",
        "        self.lr_decay = kwargs.pop(\"lr_decay\", 1.0)\n",
        "        self.batch_size = kwargs.pop(\"batch_size\", 100)\n",
        "        self.num_epochs = kwargs.pop(\"num_epochs\", 10)\n",
        "        self.num_train_samples = kwargs.pop(\"num_train_samples\", 1000)\n",
        "        self.num_val_samples = kwargs.pop(\"num_val_samples\", None)\n",
        "\n",
        "        self.checkpoint_name = kwargs.pop(\"checkpoint_name\", None)\n",
        "        self.print_every = kwargs.pop(\"print_every\", 10)\n",
        "        self.verbose = kwargs.pop(\"verbose\", True)\n",
        "\n",
        "        # Throw an error if there are extra keyword arguments\n",
        "        if len(kwargs) > 0:\n",
        "            extra = \", \".join('\"%s\"' % k for k in list(kwargs.keys()))\n",
        "            raise ValueError(\"Unrecognized arguments %s\" % extra)\n",
        "\n",
        "        # Make sure the update rule exists, then replace the string\n",
        "        # name with the actual function\n",
        "        if not hasattr(optim, self.update_rule):\n",
        "            raise ValueError('Invalid update_rule \"%s\"' % self.update_rule)\n",
        "        self.update_rule = getattr(optim, self.update_rule)\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Set up some book-keeping variables for optimization. Don't call this\n",
        "        manually.\n",
        "        \"\"\"\n",
        "        # Set up some variables for book-keeping\n",
        "        self.epoch = 0\n",
        "        self.best_val_acc = 0\n",
        "        self.best_params = {}\n",
        "        self.loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "\n",
        "        # Make a deep copy of the optim_config for each parameter\n",
        "        self.optim_configs = {}\n",
        "        for p in self.model.params:\n",
        "            d = {k: v for k, v in self.optim_config.items()}\n",
        "            self.optim_configs[p] = d\n",
        "\n",
        "    def _step(self):\n",
        "        \"\"\"\n",
        "        Make a single gradient update. This is called by train() and should not\n",
        "        be called manually.\n",
        "        \"\"\"\n",
        "        # Make a minibatch of training data\n",
        "        num_train = self.X_train.shape[0]\n",
        "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
        "        X_batch = self.X_train[batch_mask]\n",
        "        y_batch = self.y_train[batch_mask]\n",
        "\n",
        "        # Compute loss and gradient\n",
        "        loss, grads = self.model.loss(X_batch, y_batch)\n",
        "        self.loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in self.model.params.items():\n",
        "            dw = grads[p]\n",
        "            config = self.optim_configs[p]\n",
        "            next_w, next_config = self.update_rule(w, dw, config)\n",
        "            self.model.params[p] = next_w\n",
        "            self.optim_configs[p] = next_config\n",
        "\n",
        "    def _save_checkpoint(self):\n",
        "        if self.checkpoint_name is None:\n",
        "            return\n",
        "        checkpoint = {\n",
        "            \"model\": self.model,\n",
        "            \"update_rule\": self.update_rule,\n",
        "            \"lr_decay\": self.lr_decay,\n",
        "            \"optim_config\": self.optim_config,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"num_train_samples\": self.num_train_samples,\n",
        "            \"num_val_samples\": self.num_val_samples,\n",
        "            \"epoch\": self.epoch,\n",
        "            \"loss_history\": self.loss_history,\n",
        "            \"train_acc_history\": self.train_acc_history,\n",
        "            \"val_acc_history\": self.val_acc_history,\n",
        "        }\n",
        "        filename = \"%s_epoch_%d.pkl\" % (self.checkpoint_name, self.epoch)\n",
        "        if self.verbose:\n",
        "            print('Saving checkpoint to \"%s\"' % filename)\n",
        "        with open(filename, \"wb\") as f:\n",
        "            pickle.dump(checkpoint, f)\n",
        "\n",
        "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
        "        \"\"\"\n",
        "        Check accuracy of the model on the provided data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "        - y: Array of labels, of shape (N,)\n",
        "        - num_samples: If not None, subsample the data and only test the model\n",
        "          on num_samples datapoints.\n",
        "        - batch_size: Split X and y into batches of this size to avoid using\n",
        "          too much memory.\n",
        "\n",
        "        Returns:\n",
        "        - acc: Scalar giving the fraction of instances that were correctly\n",
        "          classified by the model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Maybe subsample the data\n",
        "        N = X.shape[0]\n",
        "        if num_samples is not None and N > num_samples:\n",
        "            mask = np.random.choice(N, num_samples)\n",
        "            N = num_samples\n",
        "            X = X[mask]\n",
        "            y = y[mask]\n",
        "\n",
        "        # Compute predictions in batches\n",
        "        num_batches = N // batch_size\n",
        "        if N % batch_size != 0:\n",
        "            num_batches += 1\n",
        "        y_pred = []\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = (i + 1) * batch_size\n",
        "            scores = self.model.loss(X[start:end])\n",
        "            y_pred.append(np.argmax(scores, axis=1))\n",
        "        y_pred = np.hstack(y_pred)\n",
        "        acc = np.mean(y_pred == y)\n",
        "\n",
        "        return acc\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Run optimization to train the model.\n",
        "        \"\"\"\n",
        "        num_train = self.X_train.shape[0]\n",
        "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
        "        num_iterations = self.num_epochs * iterations_per_epoch\n",
        "\n",
        "        for t in range(num_iterations):\n",
        "            self._step()\n",
        "\n",
        "            # Maybe print training loss\n",
        "            if self.verbose and t % self.print_every == 0:\n",
        "                print(\n",
        "                    \"(Iteration %d / %d) loss: %f\"\n",
        "                    % (t + 1, num_iterations, self.loss_history[-1])\n",
        "                )\n",
        "\n",
        "            # At the end of every epoch, increment the epoch counter and decay\n",
        "            # the learning rate.\n",
        "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "            if epoch_end:\n",
        "                self.epoch += 1\n",
        "                for k in self.optim_configs:\n",
        "                    self.optim_configs[k][\"learning_rate\"] *= self.lr_decay\n",
        "\n",
        "            # Check train and val accuracy on the first iteration, the last\n",
        "            # iteration, and at the end of each epoch.\n",
        "            first_it = t == 0\n",
        "            last_it = t == num_iterations - 1\n",
        "            if first_it or last_it or epoch_end:\n",
        "                train_acc = self.check_accuracy(\n",
        "                    self.X_train, self.y_train, num_samples=self.num_train_samples\n",
        "                )\n",
        "                val_acc = self.check_accuracy(\n",
        "                    self.X_val, self.y_val, num_samples=self.num_val_samples\n",
        "                )\n",
        "                self.train_acc_history.append(train_acc)\n",
        "                self.val_acc_history.append(val_acc)\n",
        "                self._save_checkpoint()\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(\n",
        "                        \"(Epoch %d / %d) train acc: %f; val_acc: %f\"\n",
        "                        % (self.epoch, self.num_epochs, train_acc, val_acc)\n",
        "                    )\n",
        "\n",
        "                # Keep track of the best model\n",
        "                if val_acc > self.best_val_acc:\n",
        "                    self.best_val_acc = val_acc\n",
        "                    self.best_params = {}\n",
        "                    for k, v in self.model.params.items():\n",
        "                        self.best_params[k] = v.copy()\n",
        "\n",
        "        # At the end of training swap the best params into the model\n",
        "        self.model.params = self.best_params\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}